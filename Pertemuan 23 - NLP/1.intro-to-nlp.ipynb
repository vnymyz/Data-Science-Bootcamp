{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f00a6d97",
   "metadata": {},
   "source": [
    "# Pertemuan 23\n",
    "---\n",
    "## NLP (Natural Language Processing) For Machine Learning\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999a2baa",
   "metadata": {},
   "source": [
    "### Natural Language Processing (NLP)\n",
    "\n",
    "### Apa itu NLP?\n",
    "\n",
    "Natural Language Processing (NLP) adalah cabang dari **Artificial Intelligence (AI)** dan **Machine Learning** yang berfokus pada interaksi antara **komputer** dan **bahasa manusia**.  \n",
    "Dengan NLP, komputer dapat:\n",
    "- Membaca\n",
    "\n",
    "- Memahami\n",
    "\n",
    "- Menafsirkan\n",
    "\n",
    "- Menganalisis\n",
    "\n",
    "- Membuat teks dalam bahasa manusia\n",
    "\n",
    "Tujuan utama NLP adalah membuat mesin **\"mengerti\" bahasa manusia** sehingga bisa digunakan untuk berbagai aplikasi.\n",
    "\n",
    "### Mengapa NLP Penting?\n",
    "Bahasa adalah cara utama manusia berkomunikasi. Data berbentuk teks sangat banyak jumlahnya, misalnya:\n",
    "- Artikel, berita, blog\n",
    "\n",
    "- Postingan media sosial\n",
    "\n",
    "- Chat atau email\n",
    "\n",
    "- Ulasan produk\n",
    "\n",
    "- Dokumen perusahaan\n",
    "\n",
    "> Dengan NLP, data teks yang tidak terstruktur dapat diubah menjadi **informasi berharga** untuk analisis maupun otomatisasi.\n",
    "\n",
    "\n",
    "### Manfaat NLP\n",
    "Beberapa manfaat utama dari NLP:\n",
    "\n",
    "1. **Otomatisasi Tugas** → Membaca ribuan dokumen atau ulasan secara cepat tanpa perlu tenaga manusia.\n",
    "\n",
    "2. **Meningkatkan Customer Experience** → Chatbot & asisten virtual untuk melayani pelanggan 24/7.\n",
    "\n",
    "3. **Analisis Sentimen** → Mengetahui opini pelanggan tentang suatu produk atau layanan.\n",
    "\n",
    "4. **Pencarian Informasi Lebih Cerdas** → Mesin pencari, rekomendasi konten, dan sistem ta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665aa820",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16d446b",
   "metadata": {},
   "source": [
    "**Cara Kerja Sentiment Analysis**\n",
    "\n",
    "Prosesnya bisa dibagi jadi beberapa tahap:\n",
    "\n",
    "1. Data Collection (Mengumpulkan Data)\n",
    "\n",
    "Data bisa berasal dari Twitter, review produk (Amazon, Tokopedia), survey pelanggan, komentar YouTube, dsb.\n",
    "\n",
    "Contoh dataset populer: IMDb Movie Reviews, Twitter US Airline Sentiment, Amazon Reviews.\n",
    "\n",
    "2. Text Preprocessing (Praproses Teks)\n",
    "\n",
    "- Agar model lebih mudah belajar, teks biasanya dibersihkan dulu:\n",
    "\n",
    "- Lowercasing (ubah semua huruf jadi kecil)\n",
    "\n",
    "- Hapus HTML tags\n",
    "\n",
    "- Hapus URL\n",
    "\n",
    "- Hapus angka & karakter spesial\n",
    "\n",
    "- Tokenization (pecah teks jadi kata/kalimat)\n",
    "\n",
    "- Stopwords removal (hapus kata umum seperti is, the, and)\n",
    "\n",
    "- Lemmatization/Stemming (ubah kata ke bentuk dasar: “loved” → “love”)\n",
    "\n",
    "3. Feature Extraction (Ekstraksi Fitur)\n",
    "\n",
    "- Karena komputer tidak paham teks langsung, kata-kata perlu diubah jadi angka.\n",
    "Beberapa metode:\n",
    "\n",
    "- Bag of Words (BoW) → hitung frekuensi kata.\n",
    "\n",
    "- TF-IDF → beri bobot lebih pada kata yang jarang tapi penting.\n",
    "\n",
    "- Word Embeddings (Word2Vec, GloVe, FastText).\n",
    "\n",
    "- Contextual Embeddings (BERT, GPT).\n",
    "\n",
    "4. Modeling (Pemodelan)\n",
    "\n",
    "- Machine Learning klasik: Logistic Regression, Naive Bayes, SVM.\n",
    "\n",
    "- Deep Learning: RNN, LSTM, GRU, Transformer (BERT).\n",
    "\n",
    "5. Prediction (Klasifikasi Sentimen)\n",
    "\n",
    "Model memprediksi label sentimen dari teks.\n",
    "Contoh:\n",
    "\n",
    "- Input: “The food was delicious and the service was great.”\n",
    "\n",
    "- Output: Positif (0.92 confidence)\n",
    "\n",
    "6. Evaluation (Evaluasi)\n",
    "\n",
    "Gunakan metrik seperti accuracy, precision, recall, F1-score untuk mengukur performa model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb20ff0f",
   "metadata": {},
   "source": [
    "### 1. Intro to Sentiment Analysis\n",
    "\n",
    "- Sentiment Analysis (analisis sentimen) adalah teknik dalam Natural Language Processing (NLP) dan Machine Learning yang digunakan untuk mengidentifikasi dan mengklasifikasikan opini, emosi, atau sikap dari teks.\n",
    "\n",
    "- Sentiment Analysis = mengklasifikasikan teks (positif, negatif, netral).\n",
    "\n",
    "- Contoh aplikasi: review produk, analisis opini publik, feedback pelanggan.\n",
    "\n",
    "- Tujuan utamanya adalah mengetahui apakah suatu teks memiliki sentimen:\n",
    "\n",
    "    - Positif → contoh: “I love this product, it’s amazing!”\n",
    "\n",
    "    - Negatif → contoh: “This is the worst service I’ve ever had.”\n",
    "\n",
    "    - Netral → contoh: “The product arrived yesterday.”\n",
    "\n",
    "- Dataset: IMDb Reviews (film review positif/negatif).\n",
    "\n",
    "- Bisnis → mengetahui kepuasan pelanggan dari review, survei, atau media sosial.\n",
    "\n",
    "- Politik → menganalisis opini publik tentang kandidat/isu politik.\n",
    "\n",
    "- Media Sosial → memantau tren, reputasi brand, atau reaksi terhadap event tertentu.\n",
    "\n",
    "- Layanan Pelanggan → memfilter keluhan negatif untuk ditangani cepat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32ad7863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: I loved this movie, it was amazing! -> Sentiment: positive\n",
      "Review: This movie was terrible and boring -> Sentiment: negative\n"
     ]
    }
   ],
   "source": [
    "# Contoh data sederhana\n",
    "reviews = [\n",
    "    (\"I loved this movie, it was amazing!\", \"positive\"),\n",
    "    (\"This movie was terrible and boring\", \"negative\")\n",
    "]\n",
    "\n",
    "for review, label in reviews:\n",
    "    print(f\"Review: {review} -> Sentiment: {label}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1438967f",
   "metadata": {},
   "source": [
    "### 2. Preprocessing Text\n",
    "\n",
    "Untuk membersihkan text, karena tidak semua text dalam suatu dataset itu sama.\n",
    "\n",
    "#### Langkah :\n",
    "\n",
    "- Lowercase\n",
    "\n",
    "- Hapus HTML\n",
    "\n",
    "- Hapus URL\n",
    "\n",
    "- Hapus karakter aneh\n",
    "\n",
    "- Hapus spasi berlebih"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b3bfff",
   "metadata": {},
   "source": [
    "bs4 = beautiful soup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da01dec5",
   "metadata": {},
   "source": [
    "```bash\n",
    "pip install bs4\n",
    "\n",
    "pip install lxml\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c8a6f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in d:\\laragon\\bin\\python\\python-3.13\\lib\\site-packages (0.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in d:\\laragon\\bin\\python\\python-3.13\\lib\\site-packages (from bs4) (4.13.5)\n",
      "Requirement already satisfied: soupsieve>1.2 in d:\\laragon\\bin\\python\\python-3.13\\lib\\site-packages (from beautifulsoup4->bs4) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in d:\\laragon\\bin\\python\\python-3.13\\lib\\site-packages (from beautifulsoup4->bs4) (4.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd9de530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in d:\\laragon\\bin\\python\\python-3.13\\lib\\site-packages (6.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3508820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i loved this movie visit\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "text = \"I LOVED this movie!!! <br> Visit: https://imdb.com\"\n",
    "\n",
    "# lowercase\n",
    "text = text.lower()\n",
    "# hapus html\n",
    "text = BeautifulSoup(text, \"lxml\").get_text()\n",
    "# hapus url\n",
    "text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "# hapus karakter spesial (hanya huruf dan spasi yang tersisa)\n",
    "text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "# hapus spasi berlebih\n",
    "text = \" \".join(text.split())\n",
    "\n",
    "print(text)  # \"i loved this movie visit\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10606691",
   "metadata": {},
   "source": [
    "### 3. Tokenization\n",
    "\n",
    "- Dengan ini kita dapat memecah teks menjadi suatu kata atau sentence.\n",
    "\n",
    "- Tokenization adalah proses memecah teks menjadi unit-unit kecil yang disebut token.\n",
    "Token bisa berupa:\n",
    "\n",
    "- Kata (word-level tokenization)\n",
    "\n",
    "- Kalimat (sentence-level tokenization)\n",
    "\n",
    "- Sub-kata/karakter (character-level tokenization, sering dipakai di deep learning)\n",
    "\n",
    "- Contoh sederhana:\n",
    "```python\n",
    "Teks: \"I love this movie!\"\n",
    "Token: [\"I\", \"love\", \"this\", \"movie\"]\n",
    "```\n",
    "\n",
    "**Mengapa Tokenization Penting dalam Sentiment Analysis?**\n",
    "\n",
    "Dalam Sentiment Analysis, kita ingin mengetahui apakah sebuah teks bernada positif, negatif, atau netral.\n",
    "Supaya komputer bisa menganalisis, teks harus diubah dulu menjadi bentuk yang mudah dipahami oleh algoritma, dan itu dimulai dengan tokenization.\n",
    "\n",
    "Manfaat Tokenization:\n",
    "\n",
    "- Memudahkan Analisis → Dengan token, kita bisa menghitung frekuensi kata, mengidentifikasi kata penting, dll.\n",
    "\n",
    "- Dasar untuk Preprocessing → Stopword removal, stemming, dan lemmatization semuanya membutuhkan token.\n",
    "\n",
    "- Fitur untuk Model ML → Representasi teks (seperti Bag of Words atau TF-IDF) dibangun dari token.\n",
    "\n",
    "- Konteks Sentimen → Kata tertentu (misalnya \"great\", \"terrible\") menjadi indikator utama sentimen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "379fcbc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in d:\\laragon\\bin\\python\\python-3.13\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in d:\\laragon\\bin\\python\\python-3.13\\lib\\site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in d:\\laragon\\bin\\python\\python-3.13\\lib\\site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\laragon\\bin\\python\\python-3.13\\lib\\site-packages (from nltk) (2025.9.1)\n",
      "Requirement already satisfied: tqdm in d:\\laragon\\bin\\python\\python-3.13\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\vanya\\appdata\\roaming\\python\\python313\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bac9c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Vanya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Vanya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Vanya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Vanya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')      # for tokenization\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55b7a90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokenization: ['I', 'loved', 'this', 'movie', '.', 'It', 'was', 'amazing', '!']\n",
      "Sentence Tokenization: ['I loved this movie.', 'It was amazing!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "text = \"I loved this movie. It was amazing!\"\n",
    "\n",
    "print(\"Word Tokenization:\", word_tokenize(text))\n",
    "print(\"Sentence Tokenization:\", sent_tokenize(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c146f371",
   "metadata": {},
   "source": [
    "### 4. Stop Words\n",
    "\n",
    "Untuk memberhentikan beberapa text yg mengandung stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4c4c3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: ['This', 'is', 'a', 'really', 'good', 'movie']\n",
      "After removing stopwords: ['really', 'good', 'movie']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "text = \"This is a really good movie\"\n",
    "words = word_tokenize(text)\n",
    "filtered = [w for w in words if w.lower() not in stop_words]\n",
    "\n",
    "print(\"Before:\", words)\n",
    "print(\"After removing stopwords:\", filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1b4ee1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['إذ', 'إذا', 'إذما', 'إذن', 'أف', 'أقل', 'أكثر', 'ألا', 'إلا', 'التي', 'الذي', 'الذين', 'اللاتي', 'اللائي', 'اللتان', 'اللتيا', 'اللتين', 'اللذان', 'اللذين', 'اللواتي', 'إلى', 'إليك', 'إليكم', 'إليكما', 'إليكن', 'أم', 'أما', 'أما', 'إما', 'أن', 'إن', 'إنا', 'أنا', 'أنت', 'أنتم', 'أنتما', 'أنتن', 'إنما', 'إنه', 'أنى', 'أنى', 'آه', 'آها', 'أو', 'أولاء', 'أولئك', 'أوه', 'آي', 'أي', 'أيها', 'إي', 'أين', 'أين', 'أينما', 'إيه', 'بخ', 'بس', 'بعد', 'بعض', 'بك', 'بكم', 'بكم', 'بكما', 'بكن', 'بل', 'بلى', 'بما', 'بماذا', 'بمن', 'بنا', 'به', 'بها', 'بهم', 'بهما', 'بهن', 'بي', 'بين', 'بيد', 'تلك', 'تلكم', 'تلكما', 'ته', 'تي', 'تين', 'تينك', 'ثم', 'ثمة', 'حاشا', 'حبذا', 'حتى', 'حيث', 'حيثما', 'حين', 'خلا', 'دون', 'ذا', 'ذات', 'ذاك', 'ذان', 'ذانك', 'ذلك', 'ذلكم', 'ذلكما', 'ذلكن', 'ذه', 'ذو', 'ذوا', 'ذواتا', 'ذواتي', 'ذي', 'ذين', 'ذينك', 'ريث', 'سوف', 'سوى', 'شتان', 'عدا', 'عسى', 'عل', 'على', 'عليك', 'عليه', 'عما', 'عن', 'عند', 'غير', 'فإذا', 'فإن', 'فلا', 'فمن', 'في', 'فيم', 'فيما', 'فيه', 'فيها', 'قد', 'كأن', 'كأنما', 'كأي', 'كأين', 'كذا', 'كذلك', 'كل', 'كلا', 'كلاهما', 'كلتا', 'كلما', 'كليكما', 'كليهما', 'كم', 'كم', 'كما', 'كي', 'كيت', 'كيف', 'كيفما', 'لا', 'لاسيما', 'لدى', 'لست', 'لستم', 'لستما', 'لستن', 'لسن', 'لسنا', 'لعل', 'لك', 'لكم', 'لكما', 'لكن', 'لكنما', 'لكي', 'لكيلا', 'لم', 'لما', 'لن', 'لنا', 'له', 'لها', 'لهم', 'لهما', 'لهن', 'لو', 'لولا', 'لوما', 'لي', 'لئن', 'ليت', 'ليس', 'ليسا', 'ليست', 'ليستا', 'ليسوا', 'ما', 'ماذا', 'متى', 'مذ', 'مع', 'مما', 'ممن', 'من', 'منه', 'منها', 'منذ', 'مه', 'مهما', 'نحن', 'نحو', 'نعم', 'ها', 'هاتان', 'هاته', 'هاتي', 'هاتين', 'هاك', 'هاهنا', 'هذا', 'هذان', 'هذه', 'هذي', 'هذين', 'هكذا', 'هل', 'هلا', 'هم', 'هما', 'هن', 'هنا', 'هناك', 'هنالك', 'هو', 'هؤلاء', 'هي', 'هيا', 'هيت', 'هيهات', 'والذي', 'والذين', 'وإذ', 'وإذا', 'وإن', 'ولا', 'ولكن', 'ولو', 'وما', 'ومن', 'وهو', 'يا', 'أبٌ', 'أخٌ', 'حمٌ', 'فو', 'أنتِ', 'يناير', 'فبراير', 'مارس', 'أبريل', 'مايو', 'يونيو', 'يوليو', 'أغسطس', 'سبتمبر', 'أكتوبر', 'نوفمبر', 'ديسمبر', 'جانفي', 'فيفري', 'مارس', 'أفريل', 'ماي', 'جوان', 'جويلية', 'أوت', 'كانون', 'شباط', 'آذار', 'نيسان', 'أيار', 'حزيران', 'تموز', 'آب', 'أيلول', 'تشرين', 'دولار', 'دينار', 'ريال', 'درهم', 'ليرة', 'جنيه', 'قرش', 'مليم', 'فلس', 'هللة', 'سنتيم', 'يورو', 'ين', 'يوان', 'شيكل', 'واحد', 'اثنان', 'ثلاثة', 'أربعة', 'خمسة', 'ستة', 'سبعة', 'ثمانية', 'تسعة', 'عشرة', 'أحد', 'اثنا', 'اثني', 'إحدى', 'ثلاث', 'أربع', 'خمس', 'ست', 'سبع', 'ثماني', 'تسع', 'عشر', 'ثمان', 'سبت', 'أحد', 'اثنين', 'ثلاثاء', 'أربعاء', 'خميس', 'جمعة', 'أول', 'ثان', 'ثاني', 'ثالث', 'رابع', 'خامس', 'سادس', 'سابع', 'ثامن', 'تاسع', 'عاشر', 'حادي', 'أ', 'ب', 'ت', 'ث', 'ج', 'ح', 'خ', 'د', 'ذ', 'ر', 'ز', 'س', 'ش', 'ص', 'ض', 'ط', 'ظ', 'ع', 'غ', 'ف', 'ق', 'ك', 'ل', 'م', 'ن', 'ه', 'و', 'ي', 'ء', 'ى', 'آ', 'ؤ', 'ئ', 'أ', 'ة', 'ألف', 'باء', 'تاء', 'ثاء', 'جيم', 'حاء', 'خاء', 'دال', 'ذال', 'راء', 'زاي', 'سين', 'شين', 'صاد', 'ضاد', 'طاء', 'ظاء', 'عين', 'غين', 'فاء', 'قاف', 'كاف', 'لام', 'ميم', 'نون', 'هاء', 'واو', 'ياء', 'همزة', 'ي', 'نا', 'ك', 'كن', 'ه', 'إياه', 'إياها', 'إياهما', 'إياهم', 'إياهن', 'إياك', 'إياكما', 'إياكم', 'إياك', 'إياكن', 'إياي', 'إيانا', 'أولالك', 'تانِ', 'تانِك', 'تِه', 'تِي', 'تَيْنِ', 'ثمّ', 'ثمّة', 'ذانِ', 'ذِه', 'ذِي', 'ذَيْنِ', 'هَؤلاء', 'هَاتانِ', 'هَاتِه', 'هَاتِي', 'هَاتَيْنِ', 'هَذا', 'هَذانِ', 'هَذِه', 'هَذِي', 'هَذَيْنِ', 'الألى', 'الألاء', 'أل', 'أنّى', 'أيّ', 'ّأيّان', 'أنّى', 'أيّ', 'ّأيّان', 'ذيت', 'كأيّ', 'كأيّن', 'بضع', 'فلان', 'وا', 'آمينَ', 'آهِ', 'آهٍ', 'آهاً', 'أُفٍّ', 'أُفٍّ', 'أفٍّ', 'أمامك', 'أمامكَ', 'أوّهْ', 'إلَيْكَ', 'إلَيْكَ', 'إليكَ', 'إليكنّ', 'إيهٍ', 'بخٍ', 'بسّ', 'بَسْ', 'بطآن', 'بَلْهَ', 'حاي', 'حَذارِ', 'حيَّ', 'حيَّ', 'دونك', 'رويدك', 'سرعان', 'شتانَ', 'شَتَّانَ', 'صهْ', 'صهٍ', 'طاق', 'طَق', 'عَدَسْ', 'كِخ', 'مكانَك', 'مكانَك', 'مكانَك', 'مكانكم', 'مكانكما', 'مكانكنّ', 'نَخْ', 'هاكَ', 'هَجْ', 'هلم', 'هيّا', 'هَيْهات', 'وا', 'واهاً', 'وراءَك', 'وُشْكَانَ', 'وَيْ', 'يفعلان', 'تفعلان', 'يفعلون', 'تفعلون', 'تفعلين', 'اتخذ', 'ألفى', 'تخذ', 'ترك', 'تعلَّم', 'جعل', 'حجا', 'حبيب', 'خال', 'حسب', 'خال', 'درى', 'رأى', 'زعم', 'صبر', 'ظنَّ', 'عدَّ', 'علم', 'غادر', 'ذهب', 'وجد', 'ورد', 'وهب', 'أسكن', 'أطعم', 'أعطى', 'رزق', 'زود', 'سقى', 'كسا', 'أخبر', 'أرى', 'أعلم', 'أنبأ', 'حدَث', 'خبَّر', 'نبَّا', 'أفعل به', 'ما أفعله', 'بئس', 'ساء', 'طالما', 'قلما', 'لات', 'لكنَّ', 'ءَ', 'أجل', 'إذاً', 'أمّا', 'إمّا', 'إنَّ', 'أنًّ', 'أى', 'إى', 'أيا', 'ب', 'ثمَّ', 'جلل', 'جير', 'رُبَّ', 'س', 'علًّ', 'ف', 'كأنّ', 'كلَّا', 'كى', 'ل', 'لات', 'لعلَّ', 'لكنَّ', 'لكنَّ', 'م', 'نَّ', 'هلّا', 'وا', 'أل', 'إلّا', 'ت', 'ك', 'لمّا', 'ن', 'ه', 'و', 'ا', 'ي', 'تجاه', 'تلقاء', 'جميع', 'حسب', 'سبحان', 'شبه', 'لعمر', 'مثل', 'معاذ', 'أبو', 'أخو', 'حمو', 'فو', 'مئة', 'مئتان', 'ثلاثمئة', 'أربعمئة', 'خمسمئة', 'ستمئة', 'سبعمئة', 'ثمنمئة', 'تسعمئة', 'مائة', 'ثلاثمائة', 'أربعمائة', 'خمسمائة', 'ستمائة', 'سبعمائة', 'ثمانمئة', 'تسعمائة', 'عشرون', 'ثلاثون', 'اربعون', 'خمسون', 'ستون', 'سبعون', 'ثمانون', 'تسعون', 'عشرين', 'ثلاثين', 'اربعين', 'خمسين', 'ستين', 'سبعين', 'ثمانين', 'تسعين', 'بضع', 'نيف', 'أجمع', 'جميع', 'عامة', 'عين', 'نفس', 'لا سيما', 'أصلا', 'أهلا', 'أيضا', 'بؤسا', 'بعدا', 'بغتة', 'تعسا', 'حقا', 'حمدا', 'خلافا', 'خاصة', 'دواليك', 'سحقا', 'سرا', 'سمعا', 'صبرا', 'صدقا', 'صراحة', 'طرا', 'عجبا', 'عيانا', 'غالبا', 'فرادى', 'فضلا', 'قاطبة', 'كثيرا', 'لبيك', 'معاذ', 'أبدا', 'إزاء', 'أصلا', 'الآن', 'أمد', 'أمس', 'آنفا', 'آناء', 'أنّى', 'أول', 'أيّان', 'تارة', 'ثمّ', 'ثمّة', 'حقا', 'صباح', 'مساء', 'ضحوة', 'عوض', 'غدا', 'غداة', 'قطّ', 'كلّما', 'لدن', 'لمّا', 'مرّة', 'قبل', 'خلف', 'أمام', 'فوق', 'تحت', 'يمين', 'شمال', 'ارتدّ', 'استحال', 'أصبح', 'أضحى', 'آض', 'أمسى', 'انقلب', 'بات', 'تبدّل', 'تحوّل', 'حار', 'رجع', 'راح', 'صار', 'ظلّ', 'عاد', 'غدا', 'كان', 'ما انفك', 'ما برح', 'مادام', 'مازال', 'مافتئ', 'ابتدأ', 'أخذ', 'اخلولق', 'أقبل', 'انبرى', 'أنشأ', 'أوشك', 'جعل', 'حرى', 'شرع', 'طفق', 'علق', 'قام', 'كرب', 'كاد', 'هبّ']\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('arabic'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865564ba",
   "metadata": {},
   "source": [
    "### 5. Stemming & Lemmatization\n",
    "\n",
    "**Stemming**\n",
    "\n",
    "Stemming adalah proses memotong kata menjadi bentuk dasarnya (stem) tanpa memperhatikan aturan tata bahasa.\n",
    "\n",
    "Algoritma stemming biasanya bekerja dengan memotong akhiran (suffix) atau awalan (prefix) secara kasar.\n",
    "\n",
    "Hasilnya kadang bukan kata yang valid dalam kamus, tapi cukup untuk tujuan analisis.\n",
    "\n",
    "Contoh:\n",
    "\n",
    "```python\n",
    "\"playing\", \"played\", \"plays\" → \"play\"\n",
    "\n",
    "\"studies\", \"studying\" → \"studi\" (bukan kata baku, tapi stem-nya dipakai komputer)\n",
    "```\n",
    "\n",
    "- Kelebihan:\n",
    "\n",
    "    - Cepat dan sederhana.\n",
    "\n",
    "    - Cocok kalau kita hanya butuh representasi kata dasar secara kasar.\n",
    "\n",
    "- Kekurangan:\n",
    "\n",
    "    - Bisa menghasilkan kata yang aneh (tidak ada di kamus).\n",
    "\n",
    "    - Kurang akurat untuk bahasa yang kompleks.\n",
    "\n",
    "**Lemmatization**\n",
    "\n",
    "Lemmatization adalah proses mengubah kata menjadi lemma atau bentuk dasarnya, berdasarkan kamus dan aturan tata bahasa.\n",
    "\n",
    "Memerlukan informasi part-of-speech (POS), misalnya kata kerja, kata benda, dll.\n",
    "\n",
    "Hasilnya adalah kata yang valid dalam kamus.\n",
    "\n",
    "Contoh:\n",
    "\n",
    "```pyhton\n",
    "\"playing\", \"played\", \"plays\" → \"play\"\n",
    "\n",
    "\"better\" → \"good\"\n",
    "\n",
    "\"studies\" → \"study\"\n",
    "```\n",
    "\n",
    "Kelebihan:\n",
    "\n",
    "- Lebih akurat karena mempertimbangkan konteks bahasa.\n",
    "\n",
    "- Hasilnya kata baku sesuai kamus.\n",
    "\n",
    "Kekurangan:\n",
    "\n",
    "- Lebih lambat daripada stemming.\n",
    "\n",
    "- Membutuhkan resource tambahan (kamus linguistik).\n",
    "\n",
    "| Aspek     | Stemming                             | Lemmatization               |\n",
    "| --------- | ------------------------------------ | --------------------------- |\n",
    "| Metode    | Memotong awalan/akhiran (rule-based) | Berdasarkan kamus + grammar |\n",
    "| Hasil     | Bisa berupa kata yang tidak valid    | Kata valid dalam kamus      |\n",
    "| Kecepatan | Cepat                                | Lebih lambat                |\n",
    "| Akurasi   | Lebih rendah                         | Lebih tinggi                |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d43e337c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming: ['run', 'better', 'studi']\n",
      "Lemmatization: ['running', 'better', 'study']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words = [\"running\", \"better\", \"studies\"]\n",
    "\n",
    "print(\"Stemming:\", [stemmer.stem(w) for w in words])\n",
    "print(\"Lemmatization:\", [lemmatizer.lemmatize(w) for w in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e7b50f",
   "metadata": {},
   "source": [
    "### 6. Bag of Words Model\n",
    "\n",
    "Bagaimana teks diubah jadi angka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0baf834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: ['amazing' 'film' 'hate' 'is' 'love' 'movie' 'this']\n",
      "Bag of Words matrix:\n",
      " [[0 0 0 0 1 1 1]\n",
      " [1 0 0 1 0 1 1]\n",
      " [0 1 1 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\n",
    "    \"I love this movie\",\n",
    "    \"This movie is amazing\",\n",
    "    \"I hate this film\"\n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(\"Feature names:\", vectorizer.get_feature_names_out())\n",
    "print(\"Bag of Words matrix:\\n\", X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a86c20e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26371432",
   "metadata": {},
   "source": [
    "### Latihan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "82c7610d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            review sentiment\n",
      "0       I love this phone, the battery lasts long!  positive\n",
      "1            The screen is too dark and not clear.  negative\n",
      "2    Amazing camera quality, really happy with it!  positive\n",
      "3             Worst purchase ever, waste of money.  negative\n",
      "4  The design is nice but the performance is slow.   neutral\n",
      "5             Super fast delivery, very satisfied!  positive\n",
      "6                      Not worth the price at all.  negative\n",
      "7                        Good phone for the price.   neutral\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"review\": [\n",
    "        \"I love this phone, the battery lasts long!\",\n",
    "        \"The screen is too dark and not clear.\",\n",
    "        \"Amazing camera quality, really happy with it!\",\n",
    "        \"Worst purchase ever, waste of money.\",\n",
    "        \"The design is nice but the performance is slow.\",\n",
    "        \"Super fast delivery, very satisfied!\",\n",
    "        \"Not worth the price at all.\",\n",
    "        \"Good phone for the price.\"\n",
    "    ],\n",
    "    \"sentiment\": [\n",
    "        \"positive\",   # 1\n",
    "        \"negative\",   # 2\n",
    "        \"positive\",   # 3\n",
    "        \"negative\",   # 4\n",
    "        \"neutral\",    # 5\n",
    "        \"positive\",   # 6\n",
    "        \"negative\",   # 7\n",
    "        \"neutral\"     # 8\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db974c4",
   "metadata": {},
   "source": [
    "📝 Soal Latihan\n",
    "1. Intro Sentiment Analysis (15 menit)\n",
    "\n",
    "Pertanyaan: Dari dataset di atas, berapa banyak review positif, negatif, dan netral?\n",
    "(Hint: gunakan value_counts() di kolom sentiment)\n",
    "\n",
    "2. Preprocessing Text (20 menit)\n",
    "\n",
    "Lakukan preprocessing sederhana:\n",
    "\n",
    "- ubah teks jadi lowercase\n",
    "\n",
    "- hapus tanda baca dan angka\n",
    "\n",
    "- hapus spasi berlebih\n",
    "\n",
    "3. Tokenization (15 menit)\n",
    "\n",
    "Tokenisasi setiap review menjadi kata-kata menggunakan nltk.word_tokenize.\n",
    "Tuliskan hasil tokenisasi untuk 2 review pertama.\n",
    "\n",
    "4. Stop Words (15 menit)\n",
    "\n",
    "Hapus stopwords bahasa Inggris dari review ke-1 dan review ke-2.\n",
    "(Hint: gunakan stopwords.words(\"english\"))\n",
    "\n",
    "5. Stemming & Lemmatization (20 menit)\n",
    "\n",
    "Lakukan stemming dan lemmatization pada review ke-3:\n",
    "\"Amazing camera quality, really happy with it!\"\n",
    "Gunakan:\n",
    "\n",
    "- PorterStemmer untuk stemming\n",
    "\n",
    "- WordNetLemmatizer untuk lemmatization\n",
    "\n",
    "6. Bag of Words Model (30 menit)\n",
    "\n",
    "- Buat representasi Bag of Words dari semua review dengan CountVectorizer (scikit-learn).\n",
    "\n",
    "- Tampilkan hasil matrix (fitur kata → jumlah kemunculan).\n",
    "\n",
    "- Berapa banyak fitur (kata unik) yang terbentuk?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbceeef",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830c76ba",
   "metadata": {},
   "source": [
    "### Code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1977bfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a974a9a3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdeadb5f",
   "metadata": {},
   "source": [
    "### Jawaban"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2247e181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment\n",
      "positive    3\n",
      "negative    3\n",
      "neutral     2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 1. Intro Sentiment Analysis\n",
    "print(df[\"sentiment\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "50674fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            review  \\\n",
      "0       I love this phone, the battery lasts long!   \n",
      "1            The screen is too dark and not clear.   \n",
      "2    Amazing camera quality, really happy with it!   \n",
      "3             Worst purchase ever, waste of money.   \n",
      "4  The design is nice but the performance is slow.   \n",
      "5             Super fast delivery, very satisfied!   \n",
      "6                      Not worth the price at all.   \n",
      "7                        Good phone for the price.   \n",
      "\n",
      "                                          cleaned  \n",
      "0        i love this phone the battery lasts long  \n",
      "1            the screen is too dark and not clear  \n",
      "2     amazing camera quality really happy with it  \n",
      "3              worst purchase ever waste of money  \n",
      "4  the design is nice but the performance is slow  \n",
      "5              super fast delivery very satisfied  \n",
      "6                      not worth the price at all  \n",
      "7                        good phone for the price  \n"
     ]
    }
   ],
   "source": [
    "# 2. Preprocessing Text\n",
    "import re\n",
    "\n",
    "df[\"cleaned\"] = df[\"review\"].apply(lambda x: re.sub(r\"[^a-zA-Z\\s]\", \"\", x.lower()).strip())\n",
    "print(df[[\"review\",\"cleaned\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "33a8ddd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'love', 'this', 'phone', 'the', 'battery', 'lasts', 'long']\n",
      "['the', 'screen', 'is', 'too', 'dark', 'and', 'not', 'clear']\n"
     ]
    }
   ],
   "source": [
    "# 3. Tokenization\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "print(word_tokenize(df[\"cleaned\"][0]))  # review 1\n",
    "print(word_tokenize(df[\"cleaned\"][1]))  # review 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4c8b9f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'phone', 'battery', 'lasts', 'long']\n",
      "['screen', 'dark', 'clear']\n"
     ]
    }
   ],
   "source": [
    "# 4. Stopword Removal\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "remove_sw = lambda x: [w for w in word_tokenize(x) if w not in stop_words]\n",
    "\n",
    "print(remove_sw(df[\"cleaned\"][0]))  # review 1\n",
    "print(remove_sw(df[\"cleaned\"][1]))  # review 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e74749f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: ['amazing', 'camera', 'quality', 'really', 'happy', 'with', 'it']\n",
      "Stemming: ['amaz', 'camera', 'qualiti', 'realli', 'happi', 'with', 'it']\n",
      "Lemmatization: ['amazing', 'camera', 'quality', 'really', 'happy', 'with', 'it']\n"
     ]
    }
   ],
   "source": [
    "# 5. Stemming & Lemmatization\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "text = df[\"cleaned\"][2]  # review 3\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "print(\"Original:\", tokens)\n",
    "print(\"Stemming:\", [stemmer.stem(w) for w in tokens])\n",
    "print(\"Lemmatization:\", [lemmatizer.lemmatize(w) for w in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d8fda24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['all' 'amazing' 'and' 'at' 'battery' 'but' 'camera' 'clear' 'dark'\n",
      " 'delivery' 'design' 'ever' 'fast' 'for' 'good' 'happy' 'is' 'it' 'lasts'\n",
      " 'long' 'love' 'money' 'nice' 'not' 'of' 'performance' 'phone' 'price'\n",
      " 'purchase' 'quality' 'really' 'satisfied' 'screen' 'slow' 'super' 'the'\n",
      " 'this' 'too' 'very' 'waste' 'with' 'worst' 'worth']\n",
      "[[0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1\n",
      "  1 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1\n",
      "  0 1 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0\n",
      "  0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0\n",
      "  0 0 0 1 0 1 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 2 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 2\n",
      "  0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0\n",
      "  0 0 1 0 0 0 0]\n",
      " [1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1\n",
      "  0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1\n",
      "  0 0 0 0 0 0 0]]\n",
      "Jumlah fitur unik: 43\n"
     ]
    }
   ],
   "source": [
    "# 6. Bag of Words Model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df[\"cleaned\"])\n",
    "\n",
    "print(vectorizer.get_feature_names_out())  # daftar fitur\n",
    "print(X.toarray())  # matrix\n",
    "print(\"Jumlah fitur unik:\", len(vectorizer.get_feature_names_out()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

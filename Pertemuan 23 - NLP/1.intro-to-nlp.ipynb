{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f00a6d97",
   "metadata": {},
   "source": [
    "# Pertemuan 23\n",
    "---\n",
    "## NLP (Natural Language Processing) For Machine Learning\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999a2baa",
   "metadata": {},
   "source": [
    "### Natural Language Processing (NLP)\n",
    "\n",
    "### Apa itu NLP?\n",
    "\n",
    "Natural Language Processing (NLP) adalah cabang dari **Artificial Intelligence (AI)** dan **Machine Learning** yang berfokus pada interaksi antara **komputer** dan **bahasa manusia**.  \n",
    "Dengan NLP, komputer dapat:\n",
    "- Membaca\n",
    "\n",
    "- Memahami\n",
    "\n",
    "- Menafsirkan\n",
    "\n",
    "- Menganalisis\n",
    "\n",
    "- Membuat teks dalam bahasa manusia\n",
    "\n",
    "Tujuan utama NLP adalah membuat mesin **\"mengerti\" bahasa manusia** sehingga bisa digunakan untuk berbagai aplikasi.\n",
    "\n",
    "### Mengapa NLP Penting?\n",
    "Bahasa adalah cara utama manusia berkomunikasi. Data berbentuk teks sangat banyak jumlahnya, misalnya:\n",
    "- Artikel, berita, blog\n",
    "\n",
    "- Postingan media sosial\n",
    "\n",
    "- Chat atau email\n",
    "\n",
    "- Ulasan produk\n",
    "\n",
    "- Dokumen perusahaan\n",
    "\n",
    "> Dengan NLP, data teks yang tidak terstruktur dapat diubah menjadi **informasi berharga** untuk analisis maupun otomatisasi.\n",
    "\n",
    "\n",
    "### Manfaat NLP\n",
    "Beberapa manfaat utama dari NLP:\n",
    "\n",
    "1. **Otomatisasi Tugas** â†’ Membaca ribuan dokumen atau ulasan secara cepat tanpa perlu tenaga manusia.\n",
    "\n",
    "2. **Meningkatkan Customer Experience** â†’ Chatbot & asisten virtual untuk melayani pelanggan 24/7.\n",
    "\n",
    "3. **Analisis Sentimen** â†’ Mengetahui opini pelanggan tentang suatu produk atau layanan.\n",
    "\n",
    "4. **Pencarian Informasi Lebih Cerdas** â†’ Mesin pencari, rekomendasi konten, dan sistem ta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665aa820",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16d446b",
   "metadata": {},
   "source": [
    "**Cara Kerja Sentiment Analysis**\n",
    "\n",
    "Prosesnya bisa dibagi jadi beberapa tahap:\n",
    "\n",
    "1. Data Collection (Mengumpulkan Data)\n",
    "\n",
    "Data bisa berasal dari Twitter, review produk (Amazon, Tokopedia), survey pelanggan, komentar YouTube, dsb.\n",
    "\n",
    "Contoh dataset populer: IMDb Movie Reviews, Twitter US Airline Sentiment, Amazon Reviews.\n",
    "\n",
    "2. Text Preprocessing (Praproses Teks)\n",
    "\n",
    "- Agar model lebih mudah belajar, teks biasanya dibersihkan dulu:\n",
    "\n",
    "- Lowercasing (ubah semua huruf jadi kecil)\n",
    "\n",
    "- Hapus HTML tags\n",
    "\n",
    "- Hapus URL\n",
    "\n",
    "- Hapus angka & karakter spesial\n",
    "\n",
    "- Tokenization (pecah teks jadi kata/kalimat)\n",
    "\n",
    "- Stopwords removal (hapus kata umum seperti is, the, and)\n",
    "\n",
    "- Lemmatization/Stemming (ubah kata ke bentuk dasar: â€œlovedâ€ â†’ â€œloveâ€)\n",
    "\n",
    "3. Feature Extraction (Ekstraksi Fitur)\n",
    "\n",
    "- Karena komputer tidak paham teks langsung, kata-kata perlu diubah jadi angka.\n",
    "Beberapa metode:\n",
    "\n",
    "- Bag of Words (BoW) â†’ hitung frekuensi kata.\n",
    "\n",
    "- TF-IDF â†’ beri bobot lebih pada kata yang jarang tapi penting.\n",
    "\n",
    "- Word Embeddings (Word2Vec, GloVe, FastText).\n",
    "\n",
    "- Contextual Embeddings (BERT, GPT).\n",
    "\n",
    "4. Modeling (Pemodelan)\n",
    "\n",
    "- Machine Learning klasik: Logistic Regression, Naive Bayes, SVM.\n",
    "\n",
    "- Deep Learning: RNN, LSTM, GRU, Transformer (BERT).\n",
    "\n",
    "5. Prediction (Klasifikasi Sentimen)\n",
    "\n",
    "Model memprediksi label sentimen dari teks.\n",
    "Contoh:\n",
    "\n",
    "- Input: â€œThe food was delicious and the service was great.â€\n",
    "\n",
    "- Output: Positif (0.92 confidence)\n",
    "\n",
    "6. Evaluation (Evaluasi)\n",
    "\n",
    "Gunakan metrik seperti accuracy, precision, recall, F1-score untuk mengukur performa model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb20ff0f",
   "metadata": {},
   "source": [
    "### 1. Intro to Sentiment Analysis\n",
    "\n",
    "- Sentiment Analysis (analisis sentimen) adalah teknik dalam Natural Language Processing (NLP) dan Machine Learning yang digunakan untuk mengidentifikasi dan mengklasifikasikan opini, emosi, atau sikap dari teks.\n",
    "\n",
    "- Sentiment Analysis = mengklasifikasikan teks (positif, negatif, netral).\n",
    "\n",
    "- Contoh aplikasi: review produk, analisis opini publik, feedback pelanggan.\n",
    "\n",
    "- Tujuan utamanya adalah mengetahui apakah suatu teks memiliki sentimen:\n",
    "\n",
    "    - Positif â†’ contoh: â€œI love this product, itâ€™s amazing!â€\n",
    "\n",
    "    - Negatif â†’ contoh: â€œThis is the worst service Iâ€™ve ever had.â€\n",
    "\n",
    "    - Netral â†’ contoh: â€œThe product arrived yesterday.â€\n",
    "\n",
    "- Dataset: IMDb Reviews (film review positif/negatif).\n",
    "\n",
    "- Bisnis â†’ mengetahui kepuasan pelanggan dari review, survei, atau media sosial.\n",
    "\n",
    "- Politik â†’ menganalisis opini publik tentang kandidat/isu politik.\n",
    "\n",
    "- Media Sosial â†’ memantau tren, reputasi brand, atau reaksi terhadap event tertentu.\n",
    "\n",
    "- Layanan Pelanggan â†’ memfilter keluhan negatif untuk ditangani cepat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32ad7863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: I loved this movie, it was amazing! -> Sentiment: positive\n",
      "Review: This movie was terrible and boring -> Sentiment: negative\n"
     ]
    }
   ],
   "source": [
    "# Contoh data sederhana\n",
    "reviews = [\n",
    "    (\"I loved this movie, it was amazing!\", \"positive\"),\n",
    "    (\"This movie was terrible and boring\", \"negative\")\n",
    "]\n",
    "\n",
    "for review, label in reviews:\n",
    "    print(f\"Review: {review} -> Sentiment: {label}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1438967f",
   "metadata": {},
   "source": [
    "### 2. Preprocessing Text\n",
    "\n",
    "Untuk membersihkan text, karena tidak semua text dalam suatu dataset itu sama.\n",
    "\n",
    "#### Langkah :\n",
    "\n",
    "- Lowercase\n",
    "\n",
    "- Hapus HTML\n",
    "\n",
    "- Hapus URL\n",
    "\n",
    "- Hapus karakter aneh\n",
    "\n",
    "- Hapus spasi berlebih"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b3bfff",
   "metadata": {},
   "source": [
    "bs4 = beautiful soup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da01dec5",
   "metadata": {},
   "source": [
    "```bash\n",
    "pip install bs4\n",
    "\n",
    "pip install lxml\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c8a6f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in d:\\laragon\\bin\\python\\python-3.13\\lib\\site-packages (0.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in d:\\laragon\\bin\\python\\python-3.13\\lib\\site-packages (from bs4) (4.13.5)\n",
      "Requirement already satisfied: soupsieve>1.2 in d:\\laragon\\bin\\python\\python-3.13\\lib\\site-packages (from beautifulsoup4->bs4) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in d:\\laragon\\bin\\python\\python-3.13\\lib\\site-packages (from beautifulsoup4->bs4) (4.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd9de530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in d:\\laragon\\bin\\python\\python-3.13\\lib\\site-packages (6.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3508820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i loved this movie visit\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "text = \"I LOVED this movie!!! <br> Visit: https://imdb.com\"\n",
    "\n",
    "# lowercase\n",
    "text = text.lower()\n",
    "# hapus html\n",
    "text = BeautifulSoup(text, \"lxml\").get_text()\n",
    "# hapus url\n",
    "text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "# hapus karakter spesial (hanya huruf dan spasi yang tersisa)\n",
    "text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "# hapus spasi berlebih\n",
    "text = \" \".join(text.split())\n",
    "\n",
    "print(text)  # \"i loved this movie visit\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10606691",
   "metadata": {},
   "source": [
    "### 3. Tokenization\n",
    "\n",
    "- Dengan ini kita dapat memecah teks menjadi suatu kata atau sentence.\n",
    "\n",
    "- Tokenization adalah proses memecah teks menjadi unit-unit kecil yang disebut token.\n",
    "Token bisa berupa:\n",
    "\n",
    "- Kata (word-level tokenization)\n",
    "\n",
    "- Kalimat (sentence-level tokenization)\n",
    "\n",
    "- Sub-kata/karakter (character-level tokenization, sering dipakai di deep learning)\n",
    "\n",
    "- Contoh sederhana:\n",
    "```python\n",
    "Teks: \"I love this movie!\"\n",
    "Token: [\"I\", \"love\", \"this\", \"movie\"]\n",
    "```\n",
    "\n",
    "**Mengapa Tokenization Penting dalam Sentiment Analysis?**\n",
    "\n",
    "Dalam Sentiment Analysis, kita ingin mengetahui apakah sebuah teks bernada positif, negatif, atau netral.\n",
    "Supaya komputer bisa menganalisis, teks harus diubah dulu menjadi bentuk yang mudah dipahami oleh algoritma, dan itu dimulai dengan tokenization.\n",
    "\n",
    "Manfaat Tokenization:\n",
    "\n",
    "- Memudahkan Analisis â†’ Dengan token, kita bisa menghitung frekuensi kata, mengidentifikasi kata penting, dll.\n",
    "\n",
    "- Dasar untuk Preprocessing â†’ Stopword removal, stemming, dan lemmatization semuanya membutuhkan token.\n",
    "\n",
    "- Fitur untuk Model ML â†’ Representasi teks (seperti Bag of Words atau TF-IDF) dibangun dari token.\n",
    "\n",
    "- Konteks Sentimen â†’ Kata tertentu (misalnya \"great\", \"terrible\") menjadi indikator utama sentimen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "379fcbc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in d:\\laragon\\bin\\python\\python-3.13\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in d:\\laragon\\bin\\python\\python-3.13\\lib\\site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in d:\\laragon\\bin\\python\\python-3.13\\lib\\site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\laragon\\bin\\python\\python-3.13\\lib\\site-packages (from nltk) (2025.9.1)\n",
      "Requirement already satisfied: tqdm in d:\\laragon\\bin\\python\\python-3.13\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\vanya\\appdata\\roaming\\python\\python313\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bac9c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Vanya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Vanya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Vanya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Vanya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')      # for tokenization\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55b7a90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokenization: ['I', 'loved', 'this', 'movie', '.', 'It', 'was', 'amazing', '!']\n",
      "Sentence Tokenization: ['I loved this movie.', 'It was amazing!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "text = \"I loved this movie. It was amazing!\"\n",
    "\n",
    "print(\"Word Tokenization:\", word_tokenize(text))\n",
    "print(\"Sentence Tokenization:\", sent_tokenize(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c146f371",
   "metadata": {},
   "source": [
    "### 4. Stop Words\n",
    "\n",
    "Untuk memberhentikan beberapa text yg mengandung stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4c4c3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: ['This', 'is', 'a', 'really', 'good', 'movie']\n",
      "After removing stopwords: ['really', 'good', 'movie']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "text = \"This is a really good movie\"\n",
    "words = word_tokenize(text)\n",
    "filtered = [w for w in words if w.lower() not in stop_words]\n",
    "\n",
    "print(\"Before:\", words)\n",
    "print(\"After removing stopwords:\", filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1b4ee1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ø¥Ø°', 'Ø¥Ø°Ø§', 'Ø¥Ø°Ù…Ø§', 'Ø¥Ø°Ù†', 'Ø£Ù', 'Ø£Ù‚Ù„', 'Ø£ÙƒØ«Ø±', 'Ø£Ù„Ø§', 'Ø¥Ù„Ø§', 'Ø§Ù„ØªÙŠ', 'Ø§Ù„Ø°ÙŠ', 'Ø§Ù„Ø°ÙŠÙ†', 'Ø§Ù„Ù„Ø§ØªÙŠ', 'Ø§Ù„Ù„Ø§Ø¦ÙŠ', 'Ø§Ù„Ù„ØªØ§Ù†', 'Ø§Ù„Ù„ØªÙŠØ§', 'Ø§Ù„Ù„ØªÙŠÙ†', 'Ø§Ù„Ù„Ø°Ø§Ù†', 'Ø§Ù„Ù„Ø°ÙŠÙ†', 'Ø§Ù„Ù„ÙˆØ§ØªÙŠ', 'Ø¥Ù„Ù‰', 'Ø¥Ù„ÙŠÙƒ', 'Ø¥Ù„ÙŠÙƒÙ…', 'Ø¥Ù„ÙŠÙƒÙ…Ø§', 'Ø¥Ù„ÙŠÙƒÙ†', 'Ø£Ù…', 'Ø£Ù…Ø§', 'Ø£Ù…Ø§', 'Ø¥Ù…Ø§', 'Ø£Ù†', 'Ø¥Ù†', 'Ø¥Ù†Ø§', 'Ø£Ù†Ø§', 'Ø£Ù†Øª', 'Ø£Ù†ØªÙ…', 'Ø£Ù†ØªÙ…Ø§', 'Ø£Ù†ØªÙ†', 'Ø¥Ù†Ù…Ø§', 'Ø¥Ù†Ù‡', 'Ø£Ù†Ù‰', 'Ø£Ù†Ù‰', 'Ø¢Ù‡', 'Ø¢Ù‡Ø§', 'Ø£Ùˆ', 'Ø£ÙˆÙ„Ø§Ø¡', 'Ø£ÙˆÙ„Ø¦Ùƒ', 'Ø£ÙˆÙ‡', 'Ø¢ÙŠ', 'Ø£ÙŠ', 'Ø£ÙŠÙ‡Ø§', 'Ø¥ÙŠ', 'Ø£ÙŠÙ†', 'Ø£ÙŠÙ†', 'Ø£ÙŠÙ†Ù…Ø§', 'Ø¥ÙŠÙ‡', 'Ø¨Ø®', 'Ø¨Ø³', 'Ø¨Ø¹Ø¯', 'Ø¨Ø¹Ø¶', 'Ø¨Ùƒ', 'Ø¨ÙƒÙ…', 'Ø¨ÙƒÙ…', 'Ø¨ÙƒÙ…Ø§', 'Ø¨ÙƒÙ†', 'Ø¨Ù„', 'Ø¨Ù„Ù‰', 'Ø¨Ù…Ø§', 'Ø¨Ù…Ø§Ø°Ø§', 'Ø¨Ù…Ù†', 'Ø¨Ù†Ø§', 'Ø¨Ù‡', 'Ø¨Ù‡Ø§', 'Ø¨Ù‡Ù…', 'Ø¨Ù‡Ù…Ø§', 'Ø¨Ù‡Ù†', 'Ø¨ÙŠ', 'Ø¨ÙŠÙ†', 'Ø¨ÙŠØ¯', 'ØªÙ„Ùƒ', 'ØªÙ„ÙƒÙ…', 'ØªÙ„ÙƒÙ…Ø§', 'ØªÙ‡', 'ØªÙŠ', 'ØªÙŠÙ†', 'ØªÙŠÙ†Ùƒ', 'Ø«Ù…', 'Ø«Ù…Ø©', 'Ø­Ø§Ø´Ø§', 'Ø­Ø¨Ø°Ø§', 'Ø­ØªÙ‰', 'Ø­ÙŠØ«', 'Ø­ÙŠØ«Ù…Ø§', 'Ø­ÙŠÙ†', 'Ø®Ù„Ø§', 'Ø¯ÙˆÙ†', 'Ø°Ø§', 'Ø°Ø§Øª', 'Ø°Ø§Ùƒ', 'Ø°Ø§Ù†', 'Ø°Ø§Ù†Ùƒ', 'Ø°Ù„Ùƒ', 'Ø°Ù„ÙƒÙ…', 'Ø°Ù„ÙƒÙ…Ø§', 'Ø°Ù„ÙƒÙ†', 'Ø°Ù‡', 'Ø°Ùˆ', 'Ø°ÙˆØ§', 'Ø°ÙˆØ§ØªØ§', 'Ø°ÙˆØ§ØªÙŠ', 'Ø°ÙŠ', 'Ø°ÙŠÙ†', 'Ø°ÙŠÙ†Ùƒ', 'Ø±ÙŠØ«', 'Ø³ÙˆÙ', 'Ø³ÙˆÙ‰', 'Ø´ØªØ§Ù†', 'Ø¹Ø¯Ø§', 'Ø¹Ø³Ù‰', 'Ø¹Ù„', 'Ø¹Ù„Ù‰', 'Ø¹Ù„ÙŠÙƒ', 'Ø¹Ù„ÙŠÙ‡', 'Ø¹Ù…Ø§', 'Ø¹Ù†', 'Ø¹Ù†Ø¯', 'ØºÙŠØ±', 'ÙØ¥Ø°Ø§', 'ÙØ¥Ù†', 'ÙÙ„Ø§', 'ÙÙ…Ù†', 'ÙÙŠ', 'ÙÙŠÙ…', 'ÙÙŠÙ…Ø§', 'ÙÙŠÙ‡', 'ÙÙŠÙ‡Ø§', 'Ù‚Ø¯', 'ÙƒØ£Ù†', 'ÙƒØ£Ù†Ù…Ø§', 'ÙƒØ£ÙŠ', 'ÙƒØ£ÙŠÙ†', 'ÙƒØ°Ø§', 'ÙƒØ°Ù„Ùƒ', 'ÙƒÙ„', 'ÙƒÙ„Ø§', 'ÙƒÙ„Ø§Ù‡Ù…Ø§', 'ÙƒÙ„ØªØ§', 'ÙƒÙ„Ù…Ø§', 'ÙƒÙ„ÙŠÙƒÙ…Ø§', 'ÙƒÙ„ÙŠÙ‡Ù…Ø§', 'ÙƒÙ…', 'ÙƒÙ…', 'ÙƒÙ…Ø§', 'ÙƒÙŠ', 'ÙƒÙŠØª', 'ÙƒÙŠÙ', 'ÙƒÙŠÙÙ…Ø§', 'Ù„Ø§', 'Ù„Ø§Ø³ÙŠÙ…Ø§', 'Ù„Ø¯Ù‰', 'Ù„Ø³Øª', 'Ù„Ø³ØªÙ…', 'Ù„Ø³ØªÙ…Ø§', 'Ù„Ø³ØªÙ†', 'Ù„Ø³Ù†', 'Ù„Ø³Ù†Ø§', 'Ù„Ø¹Ù„', 'Ù„Ùƒ', 'Ù„ÙƒÙ…', 'Ù„ÙƒÙ…Ø§', 'Ù„ÙƒÙ†', 'Ù„ÙƒÙ†Ù…Ø§', 'Ù„ÙƒÙŠ', 'Ù„ÙƒÙŠÙ„Ø§', 'Ù„Ù…', 'Ù„Ù…Ø§', 'Ù„Ù†', 'Ù„Ù†Ø§', 'Ù„Ù‡', 'Ù„Ù‡Ø§', 'Ù„Ù‡Ù…', 'Ù„Ù‡Ù…Ø§', 'Ù„Ù‡Ù†', 'Ù„Ùˆ', 'Ù„ÙˆÙ„Ø§', 'Ù„ÙˆÙ…Ø§', 'Ù„ÙŠ', 'Ù„Ø¦Ù†', 'Ù„ÙŠØª', 'Ù„ÙŠØ³', 'Ù„ÙŠØ³Ø§', 'Ù„ÙŠØ³Øª', 'Ù„ÙŠØ³ØªØ§', 'Ù„ÙŠØ³ÙˆØ§', 'Ù…Ø§', 'Ù…Ø§Ø°Ø§', 'Ù…ØªÙ‰', 'Ù…Ø°', 'Ù…Ø¹', 'Ù…Ù…Ø§', 'Ù…Ù…Ù†', 'Ù…Ù†', 'Ù…Ù†Ù‡', 'Ù…Ù†Ù‡Ø§', 'Ù…Ù†Ø°', 'Ù…Ù‡', 'Ù…Ù‡Ù…Ø§', 'Ù†Ø­Ù†', 'Ù†Ø­Ùˆ', 'Ù†Ø¹Ù…', 'Ù‡Ø§', 'Ù‡Ø§ØªØ§Ù†', 'Ù‡Ø§ØªÙ‡', 'Ù‡Ø§ØªÙŠ', 'Ù‡Ø§ØªÙŠÙ†', 'Ù‡Ø§Ùƒ', 'Ù‡Ø§Ù‡Ù†Ø§', 'Ù‡Ø°Ø§', 'Ù‡Ø°Ø§Ù†', 'Ù‡Ø°Ù‡', 'Ù‡Ø°ÙŠ', 'Ù‡Ø°ÙŠÙ†', 'Ù‡ÙƒØ°Ø§', 'Ù‡Ù„', 'Ù‡Ù„Ø§', 'Ù‡Ù…', 'Ù‡Ù…Ø§', 'Ù‡Ù†', 'Ù‡Ù†Ø§', 'Ù‡Ù†Ø§Ùƒ', 'Ù‡Ù†Ø§Ù„Ùƒ', 'Ù‡Ùˆ', 'Ù‡Ø¤Ù„Ø§Ø¡', 'Ù‡ÙŠ', 'Ù‡ÙŠØ§', 'Ù‡ÙŠØª', 'Ù‡ÙŠÙ‡Ø§Øª', 'ÙˆØ§Ù„Ø°ÙŠ', 'ÙˆØ§Ù„Ø°ÙŠÙ†', 'ÙˆØ¥Ø°', 'ÙˆØ¥Ø°Ø§', 'ÙˆØ¥Ù†', 'ÙˆÙ„Ø§', 'ÙˆÙ„ÙƒÙ†', 'ÙˆÙ„Ùˆ', 'ÙˆÙ…Ø§', 'ÙˆÙ…Ù†', 'ÙˆÙ‡Ùˆ', 'ÙŠØ§', 'Ø£Ø¨ÙŒ', 'Ø£Ø®ÙŒ', 'Ø­Ù…ÙŒ', 'ÙÙˆ', 'Ø£Ù†ØªÙ', 'ÙŠÙ†Ø§ÙŠØ±', 'ÙØ¨Ø±Ø§ÙŠØ±', 'Ù…Ø§Ø±Ø³', 'Ø£Ø¨Ø±ÙŠÙ„', 'Ù…Ø§ÙŠÙˆ', 'ÙŠÙˆÙ†ÙŠÙˆ', 'ÙŠÙˆÙ„ÙŠÙˆ', 'Ø£ØºØ³Ø·Ø³', 'Ø³Ø¨ØªÙ…Ø¨Ø±', 'Ø£ÙƒØªÙˆØ¨Ø±', 'Ù†ÙˆÙÙ…Ø¨Ø±', 'Ø¯ÙŠØ³Ù…Ø¨Ø±', 'Ø¬Ø§Ù†ÙÙŠ', 'ÙÙŠÙØ±ÙŠ', 'Ù…Ø§Ø±Ø³', 'Ø£ÙØ±ÙŠÙ„', 'Ù…Ø§ÙŠ', 'Ø¬ÙˆØ§Ù†', 'Ø¬ÙˆÙŠÙ„ÙŠØ©', 'Ø£ÙˆØª', 'ÙƒØ§Ù†ÙˆÙ†', 'Ø´Ø¨Ø§Ø·', 'Ø¢Ø°Ø§Ø±', 'Ù†ÙŠØ³Ø§Ù†', 'Ø£ÙŠØ§Ø±', 'Ø­Ø²ÙŠØ±Ø§Ù†', 'ØªÙ…ÙˆØ²', 'Ø¢Ø¨', 'Ø£ÙŠÙ„ÙˆÙ„', 'ØªØ´Ø±ÙŠÙ†', 'Ø¯ÙˆÙ„Ø§Ø±', 'Ø¯ÙŠÙ†Ø§Ø±', 'Ø±ÙŠØ§Ù„', 'Ø¯Ø±Ù‡Ù…', 'Ù„ÙŠØ±Ø©', 'Ø¬Ù†ÙŠÙ‡', 'Ù‚Ø±Ø´', 'Ù…Ù„ÙŠÙ…', 'ÙÙ„Ø³', 'Ù‡Ù„Ù„Ø©', 'Ø³Ù†ØªÙŠÙ…', 'ÙŠÙˆØ±Ùˆ', 'ÙŠÙ†', 'ÙŠÙˆØ§Ù†', 'Ø´ÙŠÙƒÙ„', 'ÙˆØ§Ø­Ø¯', 'Ø§Ø«Ù†Ø§Ù†', 'Ø«Ù„Ø§Ø«Ø©', 'Ø£Ø±Ø¨Ø¹Ø©', 'Ø®Ù…Ø³Ø©', 'Ø³ØªØ©', 'Ø³Ø¨Ø¹Ø©', 'Ø«Ù…Ø§Ù†ÙŠØ©', 'ØªØ³Ø¹Ø©', 'Ø¹Ø´Ø±Ø©', 'Ø£Ø­Ø¯', 'Ø§Ø«Ù†Ø§', 'Ø§Ø«Ù†ÙŠ', 'Ø¥Ø­Ø¯Ù‰', 'Ø«Ù„Ø§Ø«', 'Ø£Ø±Ø¨Ø¹', 'Ø®Ù…Ø³', 'Ø³Øª', 'Ø³Ø¨Ø¹', 'Ø«Ù…Ø§Ù†ÙŠ', 'ØªØ³Ø¹', 'Ø¹Ø´Ø±', 'Ø«Ù…Ø§Ù†', 'Ø³Ø¨Øª', 'Ø£Ø­Ø¯', 'Ø§Ø«Ù†ÙŠÙ†', 'Ø«Ù„Ø§Ø«Ø§Ø¡', 'Ø£Ø±Ø¨Ø¹Ø§Ø¡', 'Ø®Ù…ÙŠØ³', 'Ø¬Ù…Ø¹Ø©', 'Ø£ÙˆÙ„', 'Ø«Ø§Ù†', 'Ø«Ø§Ù†ÙŠ', 'Ø«Ø§Ù„Ø«', 'Ø±Ø§Ø¨Ø¹', 'Ø®Ø§Ù…Ø³', 'Ø³Ø§Ø¯Ø³', 'Ø³Ø§Ø¨Ø¹', 'Ø«Ø§Ù…Ù†', 'ØªØ§Ø³Ø¹', 'Ø¹Ø§Ø´Ø±', 'Ø­Ø§Ø¯ÙŠ', 'Ø£', 'Ø¨', 'Øª', 'Ø«', 'Ø¬', 'Ø­', 'Ø®', 'Ø¯', 'Ø°', 'Ø±', 'Ø²', 'Ø³', 'Ø´', 'Øµ', 'Ø¶', 'Ø·', 'Ø¸', 'Ø¹', 'Øº', 'Ù', 'Ù‚', 'Ùƒ', 'Ù„', 'Ù…', 'Ù†', 'Ù‡', 'Ùˆ', 'ÙŠ', 'Ø¡', 'Ù‰', 'Ø¢', 'Ø¤', 'Ø¦', 'Ø£', 'Ø©', 'Ø£Ù„Ù', 'Ø¨Ø§Ø¡', 'ØªØ§Ø¡', 'Ø«Ø§Ø¡', 'Ø¬ÙŠÙ…', 'Ø­Ø§Ø¡', 'Ø®Ø§Ø¡', 'Ø¯Ø§Ù„', 'Ø°Ø§Ù„', 'Ø±Ø§Ø¡', 'Ø²Ø§ÙŠ', 'Ø³ÙŠÙ†', 'Ø´ÙŠÙ†', 'ØµØ§Ø¯', 'Ø¶Ø§Ø¯', 'Ø·Ø§Ø¡', 'Ø¸Ø§Ø¡', 'Ø¹ÙŠÙ†', 'ØºÙŠÙ†', 'ÙØ§Ø¡', 'Ù‚Ø§Ù', 'ÙƒØ§Ù', 'Ù„Ø§Ù…', 'Ù…ÙŠÙ…', 'Ù†ÙˆÙ†', 'Ù‡Ø§Ø¡', 'ÙˆØ§Ùˆ', 'ÙŠØ§Ø¡', 'Ù‡Ù…Ø²Ø©', 'ÙŠ', 'Ù†Ø§', 'Ùƒ', 'ÙƒÙ†', 'Ù‡', 'Ø¥ÙŠØ§Ù‡', 'Ø¥ÙŠØ§Ù‡Ø§', 'Ø¥ÙŠØ§Ù‡Ù…Ø§', 'Ø¥ÙŠØ§Ù‡Ù…', 'Ø¥ÙŠØ§Ù‡Ù†', 'Ø¥ÙŠØ§Ùƒ', 'Ø¥ÙŠØ§ÙƒÙ…Ø§', 'Ø¥ÙŠØ§ÙƒÙ…', 'Ø¥ÙŠØ§Ùƒ', 'Ø¥ÙŠØ§ÙƒÙ†', 'Ø¥ÙŠØ§ÙŠ', 'Ø¥ÙŠØ§Ù†Ø§', 'Ø£ÙˆÙ„Ø§Ù„Ùƒ', 'ØªØ§Ù†Ù', 'ØªØ§Ù†ÙÙƒ', 'ØªÙÙ‡', 'ØªÙÙŠ', 'ØªÙÙŠÙ’Ù†Ù', 'Ø«Ù…Ù‘', 'Ø«Ù…Ù‘Ø©', 'Ø°Ø§Ù†Ù', 'Ø°ÙÙ‡', 'Ø°ÙÙŠ', 'Ø°ÙÙŠÙ’Ù†Ù', 'Ù‡ÙØ¤Ù„Ø§Ø¡', 'Ù‡ÙØ§ØªØ§Ù†Ù', 'Ù‡ÙØ§ØªÙÙ‡', 'Ù‡ÙØ§ØªÙÙŠ', 'Ù‡ÙØ§ØªÙÙŠÙ’Ù†Ù', 'Ù‡ÙØ°Ø§', 'Ù‡ÙØ°Ø§Ù†Ù', 'Ù‡ÙØ°ÙÙ‡', 'Ù‡ÙØ°ÙÙŠ', 'Ù‡ÙØ°ÙÙŠÙ’Ù†Ù', 'Ø§Ù„Ø£Ù„Ù‰', 'Ø§Ù„Ø£Ù„Ø§Ø¡', 'Ø£Ù„', 'Ø£Ù†Ù‘Ù‰', 'Ø£ÙŠÙ‘', 'Ù‘Ø£ÙŠÙ‘Ø§Ù†', 'Ø£Ù†Ù‘Ù‰', 'Ø£ÙŠÙ‘', 'Ù‘Ø£ÙŠÙ‘Ø§Ù†', 'Ø°ÙŠØª', 'ÙƒØ£ÙŠÙ‘', 'ÙƒØ£ÙŠÙ‘Ù†', 'Ø¨Ø¶Ø¹', 'ÙÙ„Ø§Ù†', 'ÙˆØ§', 'Ø¢Ù…ÙŠÙ†Ù', 'Ø¢Ù‡Ù', 'Ø¢Ù‡Ù', 'Ø¢Ù‡Ø§Ù‹', 'Ø£ÙÙÙÙ‘', 'Ø£ÙÙÙÙ‘', 'Ø£ÙÙÙ‘', 'Ø£Ù…Ø§Ù…Ùƒ', 'Ø£Ù…Ø§Ù…ÙƒÙ', 'Ø£ÙˆÙ‘Ù‡Ù’', 'Ø¥Ù„ÙÙŠÙ’ÙƒÙ', 'Ø¥Ù„ÙÙŠÙ’ÙƒÙ', 'Ø¥Ù„ÙŠÙƒÙ', 'Ø¥Ù„ÙŠÙƒÙ†Ù‘', 'Ø¥ÙŠÙ‡Ù', 'Ø¨Ø®Ù', 'Ø¨Ø³Ù‘', 'Ø¨ÙØ³Ù’', 'Ø¨Ø·Ø¢Ù†', 'Ø¨ÙÙ„Ù’Ù‡Ù', 'Ø­Ø§ÙŠ', 'Ø­ÙØ°Ø§Ø±Ù', 'Ø­ÙŠÙÙ‘', 'Ø­ÙŠÙÙ‘', 'Ø¯ÙˆÙ†Ùƒ', 'Ø±ÙˆÙŠØ¯Ùƒ', 'Ø³Ø±Ø¹Ø§Ù†', 'Ø´ØªØ§Ù†Ù', 'Ø´ÙØªÙÙ‘Ø§Ù†Ù', 'ØµÙ‡Ù’', 'ØµÙ‡Ù', 'Ø·Ø§Ù‚', 'Ø·ÙÙ‚', 'Ø¹ÙØ¯ÙØ³Ù’', 'ÙƒÙØ®', 'Ù…ÙƒØ§Ù†ÙÙƒ', 'Ù…ÙƒØ§Ù†ÙÙƒ', 'Ù…ÙƒØ§Ù†ÙÙƒ', 'Ù…ÙƒØ§Ù†ÙƒÙ…', 'Ù…ÙƒØ§Ù†ÙƒÙ…Ø§', 'Ù…ÙƒØ§Ù†ÙƒÙ†Ù‘', 'Ù†ÙØ®Ù’', 'Ù‡Ø§ÙƒÙ', 'Ù‡ÙØ¬Ù’', 'Ù‡Ù„Ù…', 'Ù‡ÙŠÙ‘Ø§', 'Ù‡ÙÙŠÙ’Ù‡Ø§Øª', 'ÙˆØ§', 'ÙˆØ§Ù‡Ø§Ù‹', 'ÙˆØ±Ø§Ø¡ÙÙƒ', 'ÙˆÙØ´Ù’ÙƒÙØ§Ù†Ù', 'ÙˆÙÙŠÙ’', 'ÙŠÙØ¹Ù„Ø§Ù†', 'ØªÙØ¹Ù„Ø§Ù†', 'ÙŠÙØ¹Ù„ÙˆÙ†', 'ØªÙØ¹Ù„ÙˆÙ†', 'ØªÙØ¹Ù„ÙŠÙ†', 'Ø§ØªØ®Ø°', 'Ø£Ù„ÙÙ‰', 'ØªØ®Ø°', 'ØªØ±Ùƒ', 'ØªØ¹Ù„ÙÙ‘Ù…', 'Ø¬Ø¹Ù„', 'Ø­Ø¬Ø§', 'Ø­Ø¨ÙŠØ¨', 'Ø®Ø§Ù„', 'Ø­Ø³Ø¨', 'Ø®Ø§Ù„', 'Ø¯Ø±Ù‰', 'Ø±Ø£Ù‰', 'Ø²Ø¹Ù…', 'ØµØ¨Ø±', 'Ø¸Ù†ÙÙ‘', 'Ø¹Ø¯ÙÙ‘', 'Ø¹Ù„Ù…', 'ØºØ§Ø¯Ø±', 'Ø°Ù‡Ø¨', 'ÙˆØ¬Ø¯', 'ÙˆØ±Ø¯', 'ÙˆÙ‡Ø¨', 'Ø£Ø³ÙƒÙ†', 'Ø£Ø·Ø¹Ù…', 'Ø£Ø¹Ø·Ù‰', 'Ø±Ø²Ù‚', 'Ø²ÙˆØ¯', 'Ø³Ù‚Ù‰', 'ÙƒØ³Ø§', 'Ø£Ø®Ø¨Ø±', 'Ø£Ø±Ù‰', 'Ø£Ø¹Ù„Ù…', 'Ø£Ù†Ø¨Ø£', 'Ø­Ø¯ÙØ«', 'Ø®Ø¨ÙÙ‘Ø±', 'Ù†Ø¨ÙÙ‘Ø§', 'Ø£ÙØ¹Ù„ Ø¨Ù‡', 'Ù…Ø§ Ø£ÙØ¹Ù„Ù‡', 'Ø¨Ø¦Ø³', 'Ø³Ø§Ø¡', 'Ø·Ø§Ù„Ù…Ø§', 'Ù‚Ù„Ù…Ø§', 'Ù„Ø§Øª', 'Ù„ÙƒÙ†ÙÙ‘', 'Ø¡Ù', 'Ø£Ø¬Ù„', 'Ø¥Ø°Ø§Ù‹', 'Ø£Ù…Ù‘Ø§', 'Ø¥Ù…Ù‘Ø§', 'Ø¥Ù†ÙÙ‘', 'Ø£Ù†Ù‹Ù‘', 'Ø£Ù‰', 'Ø¥Ù‰', 'Ø£ÙŠØ§', 'Ø¨', 'Ø«Ù…ÙÙ‘', 'Ø¬Ù„Ù„', 'Ø¬ÙŠØ±', 'Ø±ÙØ¨ÙÙ‘', 'Ø³', 'Ø¹Ù„Ù‹Ù‘', 'Ù', 'ÙƒØ£Ù†Ù‘', 'ÙƒÙ„ÙÙ‘Ø§', 'ÙƒÙ‰', 'Ù„', 'Ù„Ø§Øª', 'Ù„Ø¹Ù„ÙÙ‘', 'Ù„ÙƒÙ†ÙÙ‘', 'Ù„ÙƒÙ†ÙÙ‘', 'Ù…', 'Ù†ÙÙ‘', 'Ù‡Ù„Ù‘Ø§', 'ÙˆØ§', 'Ø£Ù„', 'Ø¥Ù„Ù‘Ø§', 'Øª', 'Ùƒ', 'Ù„Ù…Ù‘Ø§', 'Ù†', 'Ù‡', 'Ùˆ', 'Ø§', 'ÙŠ', 'ØªØ¬Ø§Ù‡', 'ØªÙ„Ù‚Ø§Ø¡', 'Ø¬Ù…ÙŠØ¹', 'Ø­Ø³Ø¨', 'Ø³Ø¨Ø­Ø§Ù†', 'Ø´Ø¨Ù‡', 'Ù„Ø¹Ù…Ø±', 'Ù…Ø«Ù„', 'Ù…Ø¹Ø§Ø°', 'Ø£Ø¨Ùˆ', 'Ø£Ø®Ùˆ', 'Ø­Ù…Ùˆ', 'ÙÙˆ', 'Ù…Ø¦Ø©', 'Ù…Ø¦ØªØ§Ù†', 'Ø«Ù„Ø§Ø«Ù…Ø¦Ø©', 'Ø£Ø±Ø¨Ø¹Ù…Ø¦Ø©', 'Ø®Ù…Ø³Ù…Ø¦Ø©', 'Ø³ØªÙ…Ø¦Ø©', 'Ø³Ø¨Ø¹Ù…Ø¦Ø©', 'Ø«Ù…Ù†Ù…Ø¦Ø©', 'ØªØ³Ø¹Ù…Ø¦Ø©', 'Ù…Ø§Ø¦Ø©', 'Ø«Ù„Ø§Ø«Ù…Ø§Ø¦Ø©', 'Ø£Ø±Ø¨Ø¹Ù…Ø§Ø¦Ø©', 'Ø®Ù…Ø³Ù…Ø§Ø¦Ø©', 'Ø³ØªÙ…Ø§Ø¦Ø©', 'Ø³Ø¨Ø¹Ù…Ø§Ø¦Ø©', 'Ø«Ù…Ø§Ù†Ù…Ø¦Ø©', 'ØªØ³Ø¹Ù…Ø§Ø¦Ø©', 'Ø¹Ø´Ø±ÙˆÙ†', 'Ø«Ù„Ø§Ø«ÙˆÙ†', 'Ø§Ø±Ø¨Ø¹ÙˆÙ†', 'Ø®Ù…Ø³ÙˆÙ†', 'Ø³ØªÙˆÙ†', 'Ø³Ø¨Ø¹ÙˆÙ†', 'Ø«Ù…Ø§Ù†ÙˆÙ†', 'ØªØ³Ø¹ÙˆÙ†', 'Ø¹Ø´Ø±ÙŠÙ†', 'Ø«Ù„Ø§Ø«ÙŠÙ†', 'Ø§Ø±Ø¨Ø¹ÙŠÙ†', 'Ø®Ù…Ø³ÙŠÙ†', 'Ø³ØªÙŠÙ†', 'Ø³Ø¨Ø¹ÙŠÙ†', 'Ø«Ù…Ø§Ù†ÙŠÙ†', 'ØªØ³Ø¹ÙŠÙ†', 'Ø¨Ø¶Ø¹', 'Ù†ÙŠÙ', 'Ø£Ø¬Ù…Ø¹', 'Ø¬Ù…ÙŠØ¹', 'Ø¹Ø§Ù…Ø©', 'Ø¹ÙŠÙ†', 'Ù†ÙØ³', 'Ù„Ø§ Ø³ÙŠÙ…Ø§', 'Ø£ØµÙ„Ø§', 'Ø£Ù‡Ù„Ø§', 'Ø£ÙŠØ¶Ø§', 'Ø¨Ø¤Ø³Ø§', 'Ø¨Ø¹Ø¯Ø§', 'Ø¨ØºØªØ©', 'ØªØ¹Ø³Ø§', 'Ø­Ù‚Ø§', 'Ø­Ù…Ø¯Ø§', 'Ø®Ù„Ø§ÙØ§', 'Ø®Ø§ØµØ©', 'Ø¯ÙˆØ§Ù„ÙŠÙƒ', 'Ø³Ø­Ù‚Ø§', 'Ø³Ø±Ø§', 'Ø³Ù…Ø¹Ø§', 'ØµØ¨Ø±Ø§', 'ØµØ¯Ù‚Ø§', 'ØµØ±Ø§Ø­Ø©', 'Ø·Ø±Ø§', 'Ø¹Ø¬Ø¨Ø§', 'Ø¹ÙŠØ§Ù†Ø§', 'ØºØ§Ù„Ø¨Ø§', 'ÙØ±Ø§Ø¯Ù‰', 'ÙØ¶Ù„Ø§', 'Ù‚Ø§Ø·Ø¨Ø©', 'ÙƒØ«ÙŠØ±Ø§', 'Ù„Ø¨ÙŠÙƒ', 'Ù…Ø¹Ø§Ø°', 'Ø£Ø¨Ø¯Ø§', 'Ø¥Ø²Ø§Ø¡', 'Ø£ØµÙ„Ø§', 'Ø§Ù„Ø¢Ù†', 'Ø£Ù…Ø¯', 'Ø£Ù…Ø³', 'Ø¢Ù†ÙØ§', 'Ø¢Ù†Ø§Ø¡', 'Ø£Ù†Ù‘Ù‰', 'Ø£ÙˆÙ„', 'Ø£ÙŠÙ‘Ø§Ù†', 'ØªØ§Ø±Ø©', 'Ø«Ù…Ù‘', 'Ø«Ù…Ù‘Ø©', 'Ø­Ù‚Ø§', 'ØµØ¨Ø§Ø­', 'Ù…Ø³Ø§Ø¡', 'Ø¶Ø­ÙˆØ©', 'Ø¹ÙˆØ¶', 'ØºØ¯Ø§', 'ØºØ¯Ø§Ø©', 'Ù‚Ø·Ù‘', 'ÙƒÙ„Ù‘Ù…Ø§', 'Ù„Ø¯Ù†', 'Ù„Ù…Ù‘Ø§', 'Ù…Ø±Ù‘Ø©', 'Ù‚Ø¨Ù„', 'Ø®Ù„Ù', 'Ø£Ù…Ø§Ù…', 'ÙÙˆÙ‚', 'ØªØ­Øª', 'ÙŠÙ…ÙŠÙ†', 'Ø´Ù…Ø§Ù„', 'Ø§Ø±ØªØ¯Ù‘', 'Ø§Ø³ØªØ­Ø§Ù„', 'Ø£ØµØ¨Ø­', 'Ø£Ø¶Ø­Ù‰', 'Ø¢Ø¶', 'Ø£Ù…Ø³Ù‰', 'Ø§Ù†Ù‚Ù„Ø¨', 'Ø¨Ø§Øª', 'ØªØ¨Ø¯Ù‘Ù„', 'ØªØ­ÙˆÙ‘Ù„', 'Ø­Ø§Ø±', 'Ø±Ø¬Ø¹', 'Ø±Ø§Ø­', 'ØµØ§Ø±', 'Ø¸Ù„Ù‘', 'Ø¹Ø§Ø¯', 'ØºØ¯Ø§', 'ÙƒØ§Ù†', 'Ù…Ø§ Ø§Ù†ÙÙƒ', 'Ù…Ø§ Ø¨Ø±Ø­', 'Ù…Ø§Ø¯Ø§Ù…', 'Ù…Ø§Ø²Ø§Ù„', 'Ù…Ø§ÙØªØ¦', 'Ø§Ø¨ØªØ¯Ø£', 'Ø£Ø®Ø°', 'Ø§Ø®Ù„ÙˆÙ„Ù‚', 'Ø£Ù‚Ø¨Ù„', 'Ø§Ù†Ø¨Ø±Ù‰', 'Ø£Ù†Ø´Ø£', 'Ø£ÙˆØ´Ùƒ', 'Ø¬Ø¹Ù„', 'Ø­Ø±Ù‰', 'Ø´Ø±Ø¹', 'Ø·ÙÙ‚', 'Ø¹Ù„Ù‚', 'Ù‚Ø§Ù…', 'ÙƒØ±Ø¨', 'ÙƒØ§Ø¯', 'Ù‡Ø¨Ù‘']\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('arabic'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865564ba",
   "metadata": {},
   "source": [
    "### 5. Stemming & Lemmatization\n",
    "\n",
    "**Stemming**\n",
    "\n",
    "Stemming adalah proses memotong kata menjadi bentuk dasarnya (stem) tanpa memperhatikan aturan tata bahasa.\n",
    "\n",
    "Algoritma stemming biasanya bekerja dengan memotong akhiran (suffix) atau awalan (prefix) secara kasar.\n",
    "\n",
    "Hasilnya kadang bukan kata yang valid dalam kamus, tapi cukup untuk tujuan analisis.\n",
    "\n",
    "Contoh:\n",
    "\n",
    "```python\n",
    "\"playing\", \"played\", \"plays\" â†’ \"play\"\n",
    "\n",
    "\"studies\", \"studying\" â†’ \"studi\" (bukan kata baku, tapi stem-nya dipakai komputer)\n",
    "```\n",
    "\n",
    "- Kelebihan:\n",
    "\n",
    "    - Cepat dan sederhana.\n",
    "\n",
    "    - Cocok kalau kita hanya butuh representasi kata dasar secara kasar.\n",
    "\n",
    "- Kekurangan:\n",
    "\n",
    "    - Bisa menghasilkan kata yang aneh (tidak ada di kamus).\n",
    "\n",
    "    - Kurang akurat untuk bahasa yang kompleks.\n",
    "\n",
    "**Lemmatization**\n",
    "\n",
    "Lemmatization adalah proses mengubah kata menjadi lemma atau bentuk dasarnya, berdasarkan kamus dan aturan tata bahasa.\n",
    "\n",
    "Memerlukan informasi part-of-speech (POS), misalnya kata kerja, kata benda, dll.\n",
    "\n",
    "Hasilnya adalah kata yang valid dalam kamus.\n",
    "\n",
    "Contoh:\n",
    "\n",
    "```pyhton\n",
    "\"playing\", \"played\", \"plays\" â†’ \"play\"\n",
    "\n",
    "\"better\" â†’ \"good\"\n",
    "\n",
    "\"studies\" â†’ \"study\"\n",
    "```\n",
    "\n",
    "Kelebihan:\n",
    "\n",
    "- Lebih akurat karena mempertimbangkan konteks bahasa.\n",
    "\n",
    "- Hasilnya kata baku sesuai kamus.\n",
    "\n",
    "Kekurangan:\n",
    "\n",
    "- Lebih lambat daripada stemming.\n",
    "\n",
    "- Membutuhkan resource tambahan (kamus linguistik).\n",
    "\n",
    "| Aspek     | Stemming                             | Lemmatization               |\n",
    "| --------- | ------------------------------------ | --------------------------- |\n",
    "| Metode    | Memotong awalan/akhiran (rule-based) | Berdasarkan kamus + grammar |\n",
    "| Hasil     | Bisa berupa kata yang tidak valid    | Kata valid dalam kamus      |\n",
    "| Kecepatan | Cepat                                | Lebih lambat                |\n",
    "| Akurasi   | Lebih rendah                         | Lebih tinggi                |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d43e337c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming: ['run', 'better', 'studi']\n",
      "Lemmatization: ['running', 'better', 'study']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words = [\"running\", \"better\", \"studies\"]\n",
    "\n",
    "print(\"Stemming:\", [stemmer.stem(w) for w in words])\n",
    "print(\"Lemmatization:\", [lemmatizer.lemmatize(w) for w in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e7b50f",
   "metadata": {},
   "source": [
    "### 6. Bag of Words Model\n",
    "\n",
    "Bagaimana teks diubah jadi angka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0baf834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: ['amazing' 'film' 'hate' 'is' 'love' 'movie' 'this']\n",
      "Bag of Words matrix:\n",
      " [[0 0 0 0 1 1 1]\n",
      " [1 0 0 1 0 1 1]\n",
      " [0 1 1 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\n",
    "    \"I love this movie\",\n",
    "    \"This movie is amazing\",\n",
    "    \"I hate this film\"\n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(\"Feature names:\", vectorizer.get_feature_names_out())\n",
    "print(\"Bag of Words matrix:\\n\", X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a86c20e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26371432",
   "metadata": {},
   "source": [
    "### Latihan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "82c7610d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            review sentiment\n",
      "0       I love this phone, the battery lasts long!  positive\n",
      "1            The screen is too dark and not clear.  negative\n",
      "2    Amazing camera quality, really happy with it!  positive\n",
      "3             Worst purchase ever, waste of money.  negative\n",
      "4  The design is nice but the performance is slow.   neutral\n",
      "5             Super fast delivery, very satisfied!  positive\n",
      "6                      Not worth the price at all.  negative\n",
      "7                        Good phone for the price.   neutral\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"review\": [\n",
    "        \"I love this phone, the battery lasts long!\",\n",
    "        \"The screen is too dark and not clear.\",\n",
    "        \"Amazing camera quality, really happy with it!\",\n",
    "        \"Worst purchase ever, waste of money.\",\n",
    "        \"The design is nice but the performance is slow.\",\n",
    "        \"Super fast delivery, very satisfied!\",\n",
    "        \"Not worth the price at all.\",\n",
    "        \"Good phone for the price.\"\n",
    "    ],\n",
    "    \"sentiment\": [\n",
    "        \"positive\",   # 1\n",
    "        \"negative\",   # 2\n",
    "        \"positive\",   # 3\n",
    "        \"negative\",   # 4\n",
    "        \"neutral\",    # 5\n",
    "        \"positive\",   # 6\n",
    "        \"negative\",   # 7\n",
    "        \"neutral\"     # 8\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db974c4",
   "metadata": {},
   "source": [
    "ğŸ“ Soal Latihan\n",
    "1. Intro Sentiment Analysis (15 menit)\n",
    "\n",
    "Pertanyaan: Dari dataset di atas, berapa banyak review positif, negatif, dan netral?\n",
    "(Hint: gunakan value_counts() di kolom sentiment)\n",
    "\n",
    "2. Preprocessing Text (20 menit)\n",
    "\n",
    "Lakukan preprocessing sederhana:\n",
    "\n",
    "- ubah teks jadi lowercase\n",
    "\n",
    "- hapus tanda baca dan angka\n",
    "\n",
    "- hapus spasi berlebih\n",
    "\n",
    "3. Tokenization (15 menit)\n",
    "\n",
    "Tokenisasi setiap review menjadi kata-kata menggunakan nltk.word_tokenize.\n",
    "Tuliskan hasil tokenisasi untuk 2 review pertama.\n",
    "\n",
    "4. Stop Words (15 menit)\n",
    "\n",
    "Hapus stopwords bahasa Inggris dari review ke-1 dan review ke-2.\n",
    "(Hint: gunakan stopwords.words(\"english\"))\n",
    "\n",
    "5. Stemming & Lemmatization (20 menit)\n",
    "\n",
    "Lakukan stemming dan lemmatization pada review ke-3:\n",
    "\"Amazing camera quality, really happy with it!\"\n",
    "Gunakan:\n",
    "\n",
    "- PorterStemmer untuk stemming\n",
    "\n",
    "- WordNetLemmatizer untuk lemmatization\n",
    "\n",
    "6. Bag of Words Model (30 menit)\n",
    "\n",
    "- Buat representasi Bag of Words dari semua review dengan CountVectorizer (scikit-learn).\n",
    "\n",
    "- Tampilkan hasil matrix (fitur kata â†’ jumlah kemunculan).\n",
    "\n",
    "- Berapa banyak fitur (kata unik) yang terbentuk?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbceeef",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830c76ba",
   "metadata": {},
   "source": [
    "### Code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1977bfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a974a9a3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdeadb5f",
   "metadata": {},
   "source": [
    "### Jawaban"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2247e181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment\n",
      "positive    3\n",
      "negative    3\n",
      "neutral     2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 1. Intro Sentiment Analysis\n",
    "print(df[\"sentiment\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "50674fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            review  \\\n",
      "0       I love this phone, the battery lasts long!   \n",
      "1            The screen is too dark and not clear.   \n",
      "2    Amazing camera quality, really happy with it!   \n",
      "3             Worst purchase ever, waste of money.   \n",
      "4  The design is nice but the performance is slow.   \n",
      "5             Super fast delivery, very satisfied!   \n",
      "6                      Not worth the price at all.   \n",
      "7                        Good phone for the price.   \n",
      "\n",
      "                                          cleaned  \n",
      "0        i love this phone the battery lasts long  \n",
      "1            the screen is too dark and not clear  \n",
      "2     amazing camera quality really happy with it  \n",
      "3              worst purchase ever waste of money  \n",
      "4  the design is nice but the performance is slow  \n",
      "5              super fast delivery very satisfied  \n",
      "6                      not worth the price at all  \n",
      "7                        good phone for the price  \n"
     ]
    }
   ],
   "source": [
    "# 2. Preprocessing Text\n",
    "import re\n",
    "\n",
    "df[\"cleaned\"] = df[\"review\"].apply(lambda x: re.sub(r\"[^a-zA-Z\\s]\", \"\", x.lower()).strip())\n",
    "print(df[[\"review\",\"cleaned\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "33a8ddd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'love', 'this', 'phone', 'the', 'battery', 'lasts', 'long']\n",
      "['the', 'screen', 'is', 'too', 'dark', 'and', 'not', 'clear']\n"
     ]
    }
   ],
   "source": [
    "# 3. Tokenization\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "print(word_tokenize(df[\"cleaned\"][0]))  # review 1\n",
    "print(word_tokenize(df[\"cleaned\"][1]))  # review 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4c8b9f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'phone', 'battery', 'lasts', 'long']\n",
      "['screen', 'dark', 'clear']\n"
     ]
    }
   ],
   "source": [
    "# 4. Stopword Removal\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "remove_sw = lambda x: [w for w in word_tokenize(x) if w not in stop_words]\n",
    "\n",
    "print(remove_sw(df[\"cleaned\"][0]))  # review 1\n",
    "print(remove_sw(df[\"cleaned\"][1]))  # review 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e74749f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: ['amazing', 'camera', 'quality', 'really', 'happy', 'with', 'it']\n",
      "Stemming: ['amaz', 'camera', 'qualiti', 'realli', 'happi', 'with', 'it']\n",
      "Lemmatization: ['amazing', 'camera', 'quality', 'really', 'happy', 'with', 'it']\n"
     ]
    }
   ],
   "source": [
    "# 5. Stemming & Lemmatization\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "text = df[\"cleaned\"][2]  # review 3\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "print(\"Original:\", tokens)\n",
    "print(\"Stemming:\", [stemmer.stem(w) for w in tokens])\n",
    "print(\"Lemmatization:\", [lemmatizer.lemmatize(w) for w in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d8fda24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['all' 'amazing' 'and' 'at' 'battery' 'but' 'camera' 'clear' 'dark'\n",
      " 'delivery' 'design' 'ever' 'fast' 'for' 'good' 'happy' 'is' 'it' 'lasts'\n",
      " 'long' 'love' 'money' 'nice' 'not' 'of' 'performance' 'phone' 'price'\n",
      " 'purchase' 'quality' 'really' 'satisfied' 'screen' 'slow' 'super' 'the'\n",
      " 'this' 'too' 'very' 'waste' 'with' 'worst' 'worth']\n",
      "[[0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1\n",
      "  1 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1\n",
      "  0 1 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0\n",
      "  0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0\n",
      "  0 0 0 1 0 1 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 2 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 2\n",
      "  0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0\n",
      "  0 0 1 0 0 0 0]\n",
      " [1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1\n",
      "  0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1\n",
      "  0 0 0 0 0 0 0]]\n",
      "Jumlah fitur unik: 43\n"
     ]
    }
   ],
   "source": [
    "# 6. Bag of Words Model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df[\"cleaned\"])\n",
    "\n",
    "print(vectorizer.get_feature_names_out())  # daftar fitur\n",
    "print(X.toarray())  # matrix\n",
    "print(\"Jumlah fitur unik:\", len(vectorizer.get_feature_names_out()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

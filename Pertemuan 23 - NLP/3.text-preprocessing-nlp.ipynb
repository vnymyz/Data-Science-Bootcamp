{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "974acb97",
   "metadata": {},
   "source": [
    "## What is Tokenization and How does it work ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ca06b7",
   "metadata": {},
   "source": [
    "Tokenization adalah proses mendasar dalam Natural Language Processing (NLP) yang bertujuan untuk memecah teks menjadi unit-unit yang lebih kecil, disebut token. Token dapat berupa kata, frasa, atau bahkan karakter, tergantung pada kebutuhan analisis.\n",
    "\n",
    "Secara umum, tokenization dilakukan dalam dua bentuk utama:\n",
    "- **Tokenisasi Kalimat**: Memecah paragraf menjadi kalimat-kalimat terpisah. Setiap kalimat dianggap sebagai satu token.\n",
    "\n",
    "- **Tokenisasi Kata**: Memecah kalimat atau paragraf menjadi kata-kata individual. Setiap kata menjadi token yang dapat dianalisis lebih lanjut.\n",
    "\n",
    "Manfaat tokenization:\n",
    "- Memudahkan proses analisis teks seperti pencarian kata kunci, stemming, lemmatization, dan ekstraksi fitur.\n",
    "\n",
    "- Membantu dalam membangun model machine learning untuk tugas-tugas seperti klasifikasi teks, sentiment analysis, dan chatbot.\n",
    "\n",
    "Tokenization juga penting untuk menghilangkan ambiguitas dalam teks, misalnya membedakan antara kata \"NLP\" dan \"NLP.\" (dengan tanda titik). Proses ini biasanya dilakukan menggunakan library seperti NLTK atau Spacy yang menyediakan berbagai metode tokenisasi sesuai kebutuhan.\n",
    "\n",
    "Dengan tokenization, data teks yang awalnya tidak terstruktur menjadi lebih terorganisir dan siap untuk tahap pemrosesan selanjutnya dalam NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a4ee12",
   "metadata": {},
   "source": [
    "## Tokenization Practical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151f5312",
   "metadata": {},
   "source": [
    "Using Library NLTK : https://www.nltk.org/\n",
    "\n",
    "Using Library Spacy : https://spacy.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd30f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49a0f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=\"\"\"Hello Welcome, to Vanya's NLP Tutorials.\n",
    "Please do learn and watch the entire course on today's class ! to have a better understanding about NLP.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b594d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0101bf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dace9676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization\n",
    "# mengubah paragraf menjadi kalimat\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31daf89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ini harus di download kalau semisal mau memberi token ke dalam corpus\n",
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fdf138",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9d27a7",
   "metadata": {},
   "source": [
    "https://www.nltk.org/api/nltk.tokenize.sent_tokenize.html\n",
    "\n",
    "https://www.nltk.org/api/nltk.tokenize.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b152129f",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8a16fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d019fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d1065d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in documents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886cf163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization 2\n",
    "# convert paragraf ke words\n",
    "# convert kalimat (sentence) ke words\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960695e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145bb8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in documents:\n",
    "    print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3881ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fef5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tapi kalau ini dia dipisah \"text.\" jadinya 'text', '.'\n",
    "# word tokenize belum tentu misahin \"text.\" \"kata-\", \n",
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44468a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc123bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a346ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dia itu kek misal \"text.\" dia digabung\n",
    "# tapi dikata terakhir dia dipisah \"text\" dan \".\"\n",
    "tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8786e41b",
   "metadata": {},
   "source": [
    "### Perbandingan Tokenizer NLTK\n",
    "Berikut contoh sederhana untuk membandingkan hasil dari beberapa tokenizer di NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedef27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contoh kalimat sederhana untuk perbandingan tokenizer\n",
    "text = \"Hello world! NLP is fun. Let's tokenize this text: NLP, AI & ML.\"\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, wordpunct_tokenize, TreebankWordTokenizer\n",
    "\n",
    "print('sent_tokenize:')\n",
    "print(sent_tokenize(text))\n",
    "\n",
    "print('\\nword_tokenize:')\n",
    "print(word_tokenize(text))\n",
    "\n",
    "print('\\nwordpunct_tokenize:')\n",
    "print(wordpunct_tokenize(text))\n",
    "\n",
    "print('\\nTreebankWordTokenizer:')\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "print(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd94c6e2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a2daff",
   "metadata": {},
   "source": [
    "## Stemming dan tipe nya"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e1954e",
   "metadata": {},
   "source": [
    "Stemming adalah proses dalam Natural Language Processing (NLP) untuk mengubah kata ke bentuk dasarnya (stem/root) dengan cara menghapus akhiran atau awalan tertentu. \n",
    "\n",
    "Tujuan stemming adalah menyederhanakan kata sehingga analisis teks menjadi lebih efisien, misalnya kata \"berlari\", \"lari\", dan \"pelari\" akan diubah menjadi \"lari\". \n",
    "\n",
    "Stemming sering digunakan dalam pencarian informasi, text mining, dan analisis sentimen agar kata-kata yang memiliki makna serupa dapat dikenali sebagai satu entitas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d726a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifiation Problem\n",
    "# Apakah komentar dari suatu produk itu positif atau negatif review\n",
    "# Reviews ------> eatin, eat, eaten, [going, gone, goes]---> go\n",
    "# apakah email spam atau tidak\n",
    "# tidak rekomend untuk membuat chatbot\n",
    "\n",
    "words=[\"eating\",\"eats\",\"eaten\",\"writing\",\"writes\",\"programming\",\"programs\",\"history\",\"finally\",\"finalized\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ff18fc",
   "metadata": {},
   "source": [
    "## Porter Stemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c10e2c",
   "metadata": {},
   "source": [
    "Porter Stemmer adalah salah satu algoritma stemming paling populer dalam Natural Language Processing (NLP). Algoritma ini dikembangkan oleh Martin Porter pada tahun 1980 dan bertujuan untuk mengubah kata-kata berimbuhan ke bentuk dasarnya (stem/root) dengan cara menghapus akhiran tertentu.\n",
    "\n",
    "Porter Stemmer bekerja berdasarkan serangkaian aturan yang secara bertahap menghilangkan akhiran kata dalam bahasa Inggris. Meskipun hasil stemming tidak selalu menghasilkan kata yang valid secara tata bahasa, metode ini sangat efektif untuk mengurangi variasi kata sehingga memudahkan analisis teks, seperti pencarian informasi dan klasifikasi dokumen.\n",
    "\n",
    "Contoh:\n",
    "- \"running\", \"runs\", \"runner\" → \"run\"\n",
    "- \"eating\", \"eaten\", \"eats\" → \"eat\"\n",
    "\n",
    "Porter Stemmer banyak digunakan karena sederhana, cepat, dan cukup akurat untuk berbagai aplikasi NLP dasar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d9b203",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dde9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemming = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef21f90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in words:\n",
    "    print(word+\"------>\"+stemming.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e03c5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemming.stem('congratulations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f562b1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemming.stem('duration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90b2c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemming.stem(\"running\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc24ec88",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemming.stem(\"sitting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad7c51c",
   "metadata": {},
   "source": [
    "## Regexp Stemmer Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5851677",
   "metadata": {},
   "source": [
    "Regexp Stemmer adalah salah satu metode stemming yang menggunakan pola regular expression (regex) untuk menghapus akhiran atau bagian tertentu dari kata. Dengan Regexp Stemmer, kita dapat menentukan aturan sendiri dalam bentuk pola regex, misalnya menghapus akhiran seperti \"ing\", \"s\", \"e\", atau \"able\" dari kata-kata yang memiliki panjang minimal tertentu.\n",
    "\n",
    "Keunggulan Regexp Stemmer:\n",
    "\n",
    "- Fleksibel karena dapat menyesuaikan pola penghapusan sesuai kebutuhan.\n",
    "\n",
    "- Cocok untuk bahasa atau kasus khusus yang tidak didukung stemmer standar.\n",
    "\n",
    "Contoh penggunaan:\n",
    "Jika kita ingin menghapus akhiran \"ing\", \"s\", \"e\", atau \"able\" dari kata-kata yang panjangnya minimal 4 karakter, kita bisa menggunakan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33b9bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reguler Expression Stemmer\n",
    "from nltk.stem import RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ed5f515e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'able$' artinya menghilangkan akhiran 'able'\n",
    "# 'ing$' artinya menghilangkan akhiran 'ing'\n",
    "# 's$' artinya menghilangkan akhiran 's'\n",
    "# 'e$' artinya menghilangkan akhiran 'e'\n",
    "# min=4 artinya hanya kata yang memiliki panjang minimal 4 karakter yang akan di stem\n",
    "# jadi kalau dollar nya diilangin dia akan hapus kata yang mengandung ing \n",
    "reg_stemmer = RegexpStemmer('ing|s$|e$|able$', min=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d1ae8104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ridient'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stemmer.stem('ingridients')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "335bec82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stemmer.stem(\"eating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e7f41894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stemmer.stem(\"ingeingat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "72fe50c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kan'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stemmer.stem(\"ingkan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc083fa",
   "metadata": {},
   "source": [
    "## Snowball Stemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba249d2",
   "metadata": {},
   "source": [
    "Snowball Stemmer adalah algoritma stemming yang dikembangkan sebagai penerus Porter Stemmer oleh Martin Porter. Algoritma ini juga dikenal sebagai \"Porter2 Stemmer\". Snowball Stemmer dirancang untuk lebih fleksibel dan mendukung berbagai bahasa, tidak hanya bahasa Inggris.\n",
    "\n",
    "#### Keunggulan Snowball Stemmer dibandingkan Porter Stemmer:\n",
    "\n",
    "- **Lebih Akurat dan Konsisten**: Snowball Stemmer memperbaiki beberapa kelemahan dan inkonsistensi yang ada pada Porter Stemmer, sehingga hasil stemming lebih stabil dan relevan.\n",
    "\n",
    "- **Mendukung Banyak Bahasa**: Snowball Stemmer dapat digunakan untuk berbagai bahasa seperti Inggris, Jerman, Prancis, Spanyol, dan lain-lain, sedangkan Porter Stemmer hanya untuk bahasa Inggris.\n",
    "\n",
    "- **Aturan Lebih Modern dan Fleksibel**: Algoritma Snowball menggunakan aturan yang lebih modern dan dapat disesuaikan, sehingga lebih baik dalam menangani variasi kata.\n",
    "\n",
    "- **Lebih Mudah Dikembangkan**: Snowball Stemmer dibuat dengan bahasa pemrograman khusus (Snowball) yang memudahkan pengembangan stemmer untuk bahasa lain.\n",
    "\n",
    "Snowball Stemmer adalah metode stemming yang mengubah kata ke bentuk dasarnya dengan menghapus akhiran atau imbuhan tertentu berdasarkan aturan yang telah ditentukan. Snowball Stemmer sangat berguna dalam analisis teks, pencarian informasi, dan aplikasi NLP lainnya karena mampu mengurangi variasi kata tanpa mengubah makna inti.\n",
    "\n",
    "Contoh:\n",
    "- \"running\", \"runs\", \"runner\" → \"run\"\n",
    "\n",
    "- \"fairly\", \"sportingly\" → \"fair\"\n",
    "\n",
    "Secara umum, Snowball Stemmer dianggap lebih baik daripada Porter Stemmer karena hasilnya lebih konsisten, mendukung banyak bahasa, dan lebih fleksibel dalam penggunaannya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ca05abb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1d6d67f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowballstem = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "27ea786c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating----->eat\n",
      "eats----->eat\n",
      "eaten----->eaten\n",
      "writing----->write\n",
      "writes----->write\n",
      "programming----->program\n",
      "programs----->program\n",
      "history----->histori\n",
      "finally----->final\n",
      "finalized----->final\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+\"----->\"+snowballstem.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7cba389e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fairli', 'sportingli')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Porter Stemmer\n",
    "stemming.stem(\"fairly\"),stemming.stem(\"sportingly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ffdca747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fair', 'sport')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Snowball Stemmer\n",
    "snowballstem.stem(\"fairly\"),snowballstem.stem(\"sportingly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "18576bd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'goe'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowballstem.stem(\"goes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f4686632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'goe'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming.stem(\"goes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e284e655",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15487ba1",
   "metadata": {},
   "source": [
    "lebih cepet--> stemming --> ketemu kata --> langsung di stemming --> 1 modul\n",
    "\n",
    "agak lama ---> lemmatization ---> ketemu kata ---> nge cek di kamus wordnet ---> baru di lemmatization --- 2 modul : wornet terus baru lemma\n",
    "\n",
    "Wordnet = kamusnya si lemma\n",
    "\n",
    "lemmatization modul perintah tapi kalau wordnet itu kamus nya"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1186dbcf",
   "metadata": {},
   "source": [
    "## Wordnet Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1303bb",
   "metadata": {},
   "source": [
    "Wordnet Lemmatization adalah proses mengubah kata ke bentuk dasar (lemma) yang valid secara tata bahasa dan memiliki makna yang jelas, menggunakan database WordNet. Berbeda dengan stemming yang hanya memotong akhiran kata tanpa memperhatikan makna, lemmatization memastikan hasilnya adalah kata yang benar dan bermakna dalam bahasa.\n",
    "\n",
    "WordNet Lemmatizer bekerja dengan mempertimbangkan part-of-speech (POS) seperti noun, verb, adjective, dan adverb, sehingga hasil lemmatization lebih akurat dan sesuai konteks.\n",
    "\n",
    "**Contoh hasil lemmatization pada daftar kata:**\n",
    "\n",
    "| Kata Asli     | Lemma (Noun) | Lemma (Verb) |\n",
    "|---------------|--------------|--------------|\n",
    "| eating        | eating       | eat          |\n",
    "| eats          | eat          | eat          |\n",
    "| eaten         | eaten        | eat          |\n",
    "| writing       | writing      | write        |\n",
    "| writes        | write        | write        |\n",
    "| programming   | programming  | program      |\n",
    "| programs      | program      | program      |\n",
    "| history       | history      | history      |\n",
    "| finally       | finally      | finally      |\n",
    "| finalized     | finalized    | finalize     |\n",
    "\n",
    "**Penjelasan:**\n",
    "- Lemmatization menghasilkan kata dasar yang valid, misal \"eating\" menjadi \"eat\" (verb).\n",
    "\n",
    "- Untuk kata benda, hasilnya tetap jika sudah dalam bentuk dasar.\n",
    "\n",
    "- Lemmatization lebih cocok untuk aplikasi seperti Q&A, chatbot, dan text summarization karena menjaga makna kata.\n",
    "\n",
    "Dengan WordNet Lemmatizer, analisis teks menjadi lebih akurat dan relevan dibandingkan dengan stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f5e0766c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "629a267c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ba2b6d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Vanya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0157d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cycle'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# '''\n",
    "# POS- Noun-n kata benda\n",
    "# verb-v kata kerja\n",
    "# adjective-a kata sifat\n",
    "# adverb-r kata keterangan\n",
    "# '''\n",
    "lemmatizer.lemmatize(\"cycling\",pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ae0938ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating------>eat\n",
      "eats------>eat\n",
      "eaten------>eat\n",
      "writing------>write\n",
      "writes------>write\n",
      "programming------>program\n",
      "programs------>program\n",
      "history------>history\n",
      "finally------>finally\n",
      "finalized------>finalize\n"
     ]
    }
   ],
   "source": [
    "# dia default nya noun atau n kalau enggak dikasih pos\n",
    "for word in words:\n",
    "    print(word+\"------>\"+lemmatizer.lemmatize(word ,pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f9d3cda0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating------>eat\n",
      "eats------>eat\n",
      "eaten------>eat\n",
      "writing------>write\n",
      "writes------>write\n",
      "programming------>program\n",
      "programs------>program\n",
      "history------>history\n",
      "finally------>finally\n",
      "finalized------>finalize\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+\"------>\"+lemmatizer.lemmatize(word ,pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ea635473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating------>eating\n",
      "eats------>eats\n",
      "eaten------>eaten\n",
      "writing------>writing\n",
      "writes------>writes\n",
      "programming------>programming\n",
      "programs------>programs\n",
      "history------>history\n",
      "finally------>finally\n",
      "finalized------>finalized\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+\"------>\"+lemmatizer.lemmatize(word ,pos='a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "127a1810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating------>eating\n",
      "eats------>eats\n",
      "eaten------>eaten\n",
      "writing------>writing\n",
      "writes------>writes\n",
      "programming------>programming\n",
      "programs------>programs\n",
      "history------>history\n",
      "finally------>finally\n",
      "finalized------>finalized\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+\"------>\"+lemmatizer.lemmatize(word ,pos='r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4d184b29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"goes\", pos='n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e5c6696d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fairly', 'sportingly')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"fairly\"),lemmatizer.lemmatize(\"sportingly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "14849a1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fairly', 'sportingly')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"fairly\", pos='v'),lemmatizer.lemmatize(\"sportingly\", pos='v')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d457052",
   "metadata": {},
   "source": [
    "Dari berbagai metode yang telah dibahas, WordNet Lemmatizer adalah yang paling memakan waktu dalam prosesnya.\n",
    "\n",
    "**Alasan utama:**\n",
    "\n",
    "WordNet Lemmatizer membutuhkan pencarian ke dalam database WordNet untuk menentukan bentuk dasar (lemma) dari sebuah kata berdasarkan part-of-speech (POS). Proses ini melibatkan lookup dan validasi linguistik agar hasil lemmatization benar secara tata bahasa. Berbeda dengan stemmer seperti Porter, Snowball, atau Regexp Stemmer yang hanya memotong akhiran kata berdasarkan aturan sederhana dan langsung, lemmatizer harus memastikan kata hasil lemmatization adalah kata yang valid dan bermakna.\n",
    "\n",
    "Karena kompleksitas dan kebutuhan akses ke resource eksternal (WordNet), proses lemmatization dengan WordNet Lemmatizer cenderung lebih lambat dibandingkan metode stemming lainnya. Namun, hasil yang didapat biasanya lebih akurat dan sesuai konteks.\n",
    "\n",
    "**Kesimpulan**\n",
    "\n",
    "Setiap metode memiliki kelebihan dan kekurangan masing-masing. Stemming cocok digunakan untuk proses yang membutuhkan kecepatan dan tidak terlalu mementingkan akurasi kata dasar, seperti pencarian informasi atau analisis statistik sederhana. Sementara itu, lemmatization lebih tepat digunakan ketika hasil kata dasar harus valid secara tata bahasa dan bermakna, misalnya pada aplikasi chatbot, Q&A, atau text summarization. Pemilihan metode tergantung pada kebutuhan dan tujuan analisis teks yang dilakukan."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb85307f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01289ee3",
   "metadata": {},
   "source": [
    "## Latihan Text Preprocessing dengan Dataset Baru\n",
    "Gunakan dataset berikut untuk mengerjakan soal-soal di bawah ini:\n",
    "\n",
    "```python\n",
    "dataset = [\n",
    "    \"Natural Language Processing is amazing!\",\n",
    "    \"Text preprocessing includes tokenization, stemming, and lemmatization.\",\n",
    "    \"Python and NLTK make NLP tasks easier.\",\n",
    "    \"Let's learn NLP together and build cool projects!\"\n",
    " ]\n",
    "```\n",
    "\n",
    "### Soal Latihan\n",
    "1. **Tokenisasi Kalimat**\n",
    "   - Gabungkan seluruh dataset menjadi satu paragraf, lalu pecah menjadi kalimat-kalimat menggunakan `sent_tokenize` dari NLTK.\n",
    "\n",
    "2. **Tokenisasi Kata**\n",
    "   - Pilih satu kalimat dari hasil tokenisasi sebelumnya, lalu pecah menjadi kata-kata menggunakan `word_tokenize`.\n",
    "\n",
    "3. **Perbandingan Tokenizer**\n",
    "   - Gunakan kalimat yang sama untuk membandingkan hasil dari `word_tokenize`, `wordpunct_tokenize`, dan `TreebankWordTokenizer`. Tampilkan hasilnya dan jelaskan perbedaannya.\n",
    "\n",
    "4. **Stemming**\n",
    "   - Lakukan stemming pada seluruh kata dari dataset menggunakan Porter Stemmer, Snowball Stemmer, dan Regexp Stemmer. Bandingkan hasilnya.\n",
    "\n",
    "5. **Lemmatization**\n",
    "   - Lakukan lemmatization pada seluruh kata dari dataset menggunakan WordNet Lemmatizer untuk POS noun dan verb. Tampilkan hasilnya dan jelaskan perbedaannya dengan stemming.\n",
    "\n",
    "6. **Analisis Waktu Eksekusi**\n",
    "   - Ukur waktu eksekusi untuk stemming dan lemmatization pada seluruh kata dari dataset. Bandingkan dan simpulkan mengapa lemmatization lebih lambat.\n",
    "\n",
    "7. **Penjelasan Konsep**\n",
    "   - Jelaskan dengan singkat perbedaan antara stemming dan lemmatization, serta kapan sebaiknya menggunakan masing-masing metode dalam aplikasi NLP.\n",
    "\n",
    "---\n",
    "Kerjakan setiap soal di bawah ini pada cell kode baru dan tuliskan penjelasan atau hasil analisis Anda di cell markdown setelahnya.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "65593229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "dataset = [\n",
    "    \"Natural Language Processing is amazing!\",\n",
    "    \"Text preprocessing includes tokenization, stemming, and lemmatization.\",\n",
    "    \"Python and NLTK make NLP tasks easier.\",\n",
    "    \"Let's learn NLP together and build cool projects!\"\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6de6b3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenisasi Kalimat: ['Natural Language Processing is amazing!', 'Text preprocessing includes tokenization, stemming, and lemmatization.', 'Python and NLTK make NLP tasks easier.', \"Let's learn NLP together and build cool projects!\"]\n"
     ]
    }
   ],
   "source": [
    "# 1. Tokenization menjadi kalimat\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, wordpunct_tokenize, TreebankWordTokenizer\n",
    "\n",
    "paragraf = \" \".join(dataset)\n",
    "kalimat = sent_tokenize(paragraf)\n",
    "print('Tokenisasi Kalimat:', kalimat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2dfc40d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenisasi Kata: ['Text', 'preprocessing', 'includes', 'tokenization', ',', 'stemming', ',', 'and', 'lemmatization', '.']\n"
     ]
    }
   ],
   "source": [
    "# 2. Tokenization menjadi words\n",
    "kata = word_tokenize(kalimat[1])\n",
    "print('\\nTokenisasi Kata:', kata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "92cbe76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "word_tokenize: ['Text', 'preprocessing', 'includes', 'tokenization', ',', 'stemming', ',', 'and', 'lemmatization', '.']\n",
      "wordpunct_tokenize: ['Text', 'preprocessing', 'includes', 'tokenization', ',', 'stemming', ',', 'and', 'lemmatization', '.']\n",
      "TreebankWordTokenizer: ['Text', 'preprocessing', 'includes', 'tokenization', ',', 'stemming', ',', 'and', 'lemmatization', '.']\n"
     ]
    }
   ],
   "source": [
    "# 3. Perbandingan Tokenizer\n",
    "print('\\nword_tokenize:', word_tokenize(kalimat[1]))\n",
    "print('wordpunct_tokenize:', wordpunct_tokenize(kalimat[1]))\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "print('TreebankWordTokenizer:', tokenizer.tokenize(kalimat[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7ba3edb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Porter Stemmer: ['text', 'preprocess', 'includ', 'token', ',', 'stem', ',', 'and', 'lemmat', '.']\n",
      "Snowball Stemmer: ['text', 'preprocess', 'includ', 'token', ',', 'stem', ',', 'and', 'lemmat', '.']\n",
      "Regexp Stemmer: ['Text', 'preprocess', 'include', 'tokenization', ',', 'stemm', ',', 'and', 'lemmatization', '.']\n"
     ]
    }
   ],
   "source": [
    "# 4. Stemming\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer, RegexpStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer('english')\n",
    "regexp = RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
    "\n",
    "print('\\nPorter Stemmer:', [porter.stem(w) for w in kata])\n",
    "print('Snowball Stemmer:', [snowball.stem(w) for w in kata])\n",
    "print('Regexp Stemmer:', [regexp.stem(w) for w in kata])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8ce0caa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WordNet Lemmatizer (noun): ['Text', 'preprocessing', 'includes', 'tokenization', ',', 'stemming', ',', 'and', 'lemmatization', '.']\n",
      "WordNet Lemmatizer (verb): ['Text', 'preprocessing', 'include', 'tokenization', ',', 'stem', ',', 'and', 'lemmatization', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Vanya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# 5. Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print('\\nWordNet Lemmatizer (noun):', [lemmatizer.lemmatize(w, pos='n') for w in kata])\n",
    "print('WordNet Lemmatizer (verb):', [lemmatizer.lemmatize(w, pos='v') for w in kata])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "09dab2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Waktu Stemming: 0.000372 detik\n",
      "Waktu Lemmatization: 0.000331 detik\n"
     ]
    }
   ],
   "source": [
    "# 6. Analisis waktu execution\n",
    "import time\n",
    "\n",
    "start_stem = time.time()\n",
    "[porter.stem(w) for w in kata]\n",
    "end_stem = time.time()\n",
    "\n",
    "start_lem = time.time()\n",
    "[lemmatizer.lemmatize(w, pos='v') for w in kata]\n",
    "end_lem = time.time()\n",
    "\n",
    "print(f'\\nWaktu Stemming: {end_stem - start_stem:.6f} detik')\n",
    "print(f'Waktu Lemmatization: {end_lem - start_lem:.6f} detik')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee30a847",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2188d05",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed9dd36",
   "metadata": {},
   "source": [
    "**Stopwords** adalah kata-kata umum dalam suatu bahasa yang sering muncul dalam teks, namun biasanya tidak memiliki makna penting untuk analisis atau pemrosesan lebih lanjut. Contoh stopwords dalam bahasa Inggris antara lain: \"the\", \"is\", \"in\", \"and\", \"of\", \"to\", \"he\", \"she\", \"ours\", dll.\n",
    "\n",
    "### Manfaat Stopwords\n",
    "\n",
    "- **Mengurangi Noise:** Stopwords dihapus agar analisis fokus pada kata-kata yang lebih bermakna dan relevan.\n",
    "\n",
    "- **Efisiensi Proses:** Mengurangi jumlah kata yang diproses sehingga mempercepat komputasi dan menghemat memori.\n",
    "\n",
    "- **Meningkatkan Akurasi:** Dengan menghilangkan kata-kata yang tidak penting, model NLP seperti klasifikasi, sentiment analysis, atau pencarian informasi menjadi lebih akurat.\n",
    "\n",
    "### Kapan Digunakan Stopwords?\n",
    "\n",
    "- **Text Preprocessing:** Sebelum melakukan tokenisasi, stemming, atau lemmatization, stopwords biasanya dihapus agar hasil analisis lebih bersih.\n",
    "\n",
    "- **Information Retrieval:** Dalam pencarian dokumen atau data, stopwords dihilangkan agar pencarian lebih relevan.\n",
    "\n",
    "- **Machine Learning:** Untuk membangun model yang lebih baik, stopwords dihapus agar fitur yang digunakan benar-benar mewakili informasi penting.\n",
    "\n",
    "### Konteks pada Notebook Ini\n",
    "\n",
    "Pada notebook ini, stopwords digunakan untuk membersihkan teks sebelum proses stemming dengan Porter Stemmer dan Snowball Stemmer. Contohnya pada variabel `paragraph`, kalimat-kalimat dipecah, lalu kata-kata yang termasuk stopwords dihapus sebelum dilakukan stemming. Hal ini bertujuan agar hasil stemming lebih fokus pada kata-kata inti yang membawa makna, bukan kata-kata umum yang tidak berkontribusi pada analisis.\n",
    "\n",
    "Stopwords juga tersedia untuk berbagai bahasa seperti Inggris, Jerman, Indonesia, dan Arab, sehingga dapat digunakan sesuai kebutuhan analisis teks multibahasa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "634f9f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"I have three visions for India. In 3000 years of our history, people from all over \n",
    "               the world have come and invaded us, captured our lands, conquered our minds. \n",
    "               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
    "               the French, the Dutch, all of them came and looted us, took over what was ours. \n",
    "               Yet we have not done this to any other nation. We have not conquered anyone. \n",
    "               We have not grabbed their land, their culture, \n",
    "               their history and tried to enforce our way of life on them. \n",
    "               Why? Because we respect the freedom of others.That is why my \n",
    "               first vision is that of freedom. I believe that India got its first vision of \n",
    "               this in 1857, when we started the War of Independence. It is this freedom that\n",
    "               we must protect and nurture and build on. If we are not free, no one will respect us.\n",
    "               My second vision for India’s development. For fifty years we have been a developing nation.\n",
    "               It is time we see ourselves as a developed nation. We are among the top 5 nations of the world\n",
    "               in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.\n",
    "               Our achievements are being globally recognised today. Yet we lack the self-confidence to\n",
    "               see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect?\n",
    "               I have a third vision. India must stand up to the world. Because I believe that unless India \n",
    "               stands up to the world, no one will respect us. Only strength respects strength. We must be \n",
    "               strong not only as a military power but also as an economic power. Both must go hand-in-hand. \n",
    "               My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of \n",
    "               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n",
    "               I was lucky to have worked with all three of them closely and consider this the great opportunity of my life. \n",
    "               I see four milestones in my career\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f1aafb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1b69fc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3213c721",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Vanya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9f13c395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he's\",\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " 'if',\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " \"i've\",\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " \"should've\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " \"we've\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " 'your',\n",
       " \"you're\",\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " \"you've\"]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# untuk menentukan suatu statement itu positif atau negatif\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7acb487f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aber',\n",
       " 'alle',\n",
       " 'allem',\n",
       " 'allen',\n",
       " 'aller',\n",
       " 'alles',\n",
       " 'als',\n",
       " 'also',\n",
       " 'am',\n",
       " 'an',\n",
       " 'ander',\n",
       " 'andere',\n",
       " 'anderem',\n",
       " 'anderen',\n",
       " 'anderer',\n",
       " 'anderes',\n",
       " 'anderm',\n",
       " 'andern',\n",
       " 'anderr',\n",
       " 'anders',\n",
       " 'auch',\n",
       " 'auf',\n",
       " 'aus',\n",
       " 'bei',\n",
       " 'bin',\n",
       " 'bis',\n",
       " 'bist',\n",
       " 'da',\n",
       " 'damit',\n",
       " 'dann',\n",
       " 'der',\n",
       " 'den',\n",
       " 'des',\n",
       " 'dem',\n",
       " 'die',\n",
       " 'das',\n",
       " 'dass',\n",
       " 'daß',\n",
       " 'derselbe',\n",
       " 'derselben',\n",
       " 'denselben',\n",
       " 'desselben',\n",
       " 'demselben',\n",
       " 'dieselbe',\n",
       " 'dieselben',\n",
       " 'dasselbe',\n",
       " 'dazu',\n",
       " 'dein',\n",
       " 'deine',\n",
       " 'deinem',\n",
       " 'deinen',\n",
       " 'deiner',\n",
       " 'deines',\n",
       " 'denn',\n",
       " 'derer',\n",
       " 'dessen',\n",
       " 'dich',\n",
       " 'dir',\n",
       " 'du',\n",
       " 'dies',\n",
       " 'diese',\n",
       " 'diesem',\n",
       " 'diesen',\n",
       " 'dieser',\n",
       " 'dieses',\n",
       " 'doch',\n",
       " 'dort',\n",
       " 'durch',\n",
       " 'ein',\n",
       " 'eine',\n",
       " 'einem',\n",
       " 'einen',\n",
       " 'einer',\n",
       " 'eines',\n",
       " 'einig',\n",
       " 'einige',\n",
       " 'einigem',\n",
       " 'einigen',\n",
       " 'einiger',\n",
       " 'einiges',\n",
       " 'einmal',\n",
       " 'er',\n",
       " 'ihn',\n",
       " 'ihm',\n",
       " 'es',\n",
       " 'etwas',\n",
       " 'euer',\n",
       " 'eure',\n",
       " 'eurem',\n",
       " 'euren',\n",
       " 'eurer',\n",
       " 'eures',\n",
       " 'für',\n",
       " 'gegen',\n",
       " 'gewesen',\n",
       " 'hab',\n",
       " 'habe',\n",
       " 'haben',\n",
       " 'hat',\n",
       " 'hatte',\n",
       " 'hatten',\n",
       " 'hier',\n",
       " 'hin',\n",
       " 'hinter',\n",
       " 'ich',\n",
       " 'mich',\n",
       " 'mir',\n",
       " 'ihr',\n",
       " 'ihre',\n",
       " 'ihrem',\n",
       " 'ihren',\n",
       " 'ihrer',\n",
       " 'ihres',\n",
       " 'euch',\n",
       " 'im',\n",
       " 'in',\n",
       " 'indem',\n",
       " 'ins',\n",
       " 'ist',\n",
       " 'jede',\n",
       " 'jedem',\n",
       " 'jeden',\n",
       " 'jeder',\n",
       " 'jedes',\n",
       " 'jene',\n",
       " 'jenem',\n",
       " 'jenen',\n",
       " 'jener',\n",
       " 'jenes',\n",
       " 'jetzt',\n",
       " 'kann',\n",
       " 'kein',\n",
       " 'keine',\n",
       " 'keinem',\n",
       " 'keinen',\n",
       " 'keiner',\n",
       " 'keines',\n",
       " 'können',\n",
       " 'könnte',\n",
       " 'machen',\n",
       " 'man',\n",
       " 'manche',\n",
       " 'manchem',\n",
       " 'manchen',\n",
       " 'mancher',\n",
       " 'manches',\n",
       " 'mein',\n",
       " 'meine',\n",
       " 'meinem',\n",
       " 'meinen',\n",
       " 'meiner',\n",
       " 'meines',\n",
       " 'mit',\n",
       " 'muss',\n",
       " 'musste',\n",
       " 'nach',\n",
       " 'nicht',\n",
       " 'nichts',\n",
       " 'noch',\n",
       " 'nun',\n",
       " 'nur',\n",
       " 'ob',\n",
       " 'oder',\n",
       " 'ohne',\n",
       " 'sehr',\n",
       " 'sein',\n",
       " 'seine',\n",
       " 'seinem',\n",
       " 'seinen',\n",
       " 'seiner',\n",
       " 'seines',\n",
       " 'selbst',\n",
       " 'sich',\n",
       " 'sie',\n",
       " 'ihnen',\n",
       " 'sind',\n",
       " 'so',\n",
       " 'solche',\n",
       " 'solchem',\n",
       " 'solchen',\n",
       " 'solcher',\n",
       " 'solches',\n",
       " 'soll',\n",
       " 'sollte',\n",
       " 'sondern',\n",
       " 'sonst',\n",
       " 'über',\n",
       " 'um',\n",
       " 'und',\n",
       " 'uns',\n",
       " 'unsere',\n",
       " 'unserem',\n",
       " 'unseren',\n",
       " 'unser',\n",
       " 'unseres',\n",
       " 'unter',\n",
       " 'viel',\n",
       " 'vom',\n",
       " 'von',\n",
       " 'vor',\n",
       " 'während',\n",
       " 'war',\n",
       " 'waren',\n",
       " 'warst',\n",
       " 'was',\n",
       " 'weg',\n",
       " 'weil',\n",
       " 'weiter',\n",
       " 'welche',\n",
       " 'welchem',\n",
       " 'welchen',\n",
       " 'welcher',\n",
       " 'welches',\n",
       " 'wenn',\n",
       " 'werde',\n",
       " 'werden',\n",
       " 'wie',\n",
       " 'wieder',\n",
       " 'will',\n",
       " 'wir',\n",
       " 'wird',\n",
       " 'wirst',\n",
       " 'wo',\n",
       " 'wollen',\n",
       " 'wollte',\n",
       " 'würde',\n",
       " 'würden',\n",
       " 'zu',\n",
       " 'zum',\n",
       " 'zur',\n",
       " 'zwar',\n",
       " 'zwischen']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('german')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "00b2a3bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ada',\n",
       " 'adalah',\n",
       " 'adanya',\n",
       " 'adapun',\n",
       " 'agak',\n",
       " 'agaknya',\n",
       " 'agar',\n",
       " 'akan',\n",
       " 'akankah',\n",
       " 'akhir',\n",
       " 'akhiri',\n",
       " 'akhirnya',\n",
       " 'aku',\n",
       " 'akulah',\n",
       " 'amat',\n",
       " 'amatlah',\n",
       " 'anda',\n",
       " 'andalah',\n",
       " 'antar',\n",
       " 'antara',\n",
       " 'antaranya',\n",
       " 'apa',\n",
       " 'apaan',\n",
       " 'apabila',\n",
       " 'apakah',\n",
       " 'apalagi',\n",
       " 'apatah',\n",
       " 'artinya',\n",
       " 'asal',\n",
       " 'asalkan',\n",
       " 'atas',\n",
       " 'atau',\n",
       " 'ataukah',\n",
       " 'ataupun',\n",
       " 'awal',\n",
       " 'awalnya',\n",
       " 'bagai',\n",
       " 'bagaikan',\n",
       " 'bagaimana',\n",
       " 'bagaimanakah',\n",
       " 'bagaimanapun',\n",
       " 'bagi',\n",
       " 'bagian',\n",
       " 'bahkan',\n",
       " 'bahwa',\n",
       " 'bahwasanya',\n",
       " 'baik',\n",
       " 'bakal',\n",
       " 'bakalan',\n",
       " 'balik',\n",
       " 'banyak',\n",
       " 'bapak',\n",
       " 'baru',\n",
       " 'bawah',\n",
       " 'beberapa',\n",
       " 'begini',\n",
       " 'beginian',\n",
       " 'beginikah',\n",
       " 'beginilah',\n",
       " 'begitu',\n",
       " 'begitukah',\n",
       " 'begitulah',\n",
       " 'begitupun',\n",
       " 'bekerja',\n",
       " 'belakang',\n",
       " 'belakangan',\n",
       " 'belum',\n",
       " 'belumlah',\n",
       " 'benar',\n",
       " 'benarkah',\n",
       " 'benarlah',\n",
       " 'berada',\n",
       " 'berakhir',\n",
       " 'berakhirlah',\n",
       " 'berakhirnya',\n",
       " 'berapa',\n",
       " 'berapakah',\n",
       " 'berapalah',\n",
       " 'berapapun',\n",
       " 'berarti',\n",
       " 'berawal',\n",
       " 'berbagai',\n",
       " 'berdatangan',\n",
       " 'beri',\n",
       " 'berikan',\n",
       " 'berikut',\n",
       " 'berikutnya',\n",
       " 'berjumlah',\n",
       " 'berkali-kali',\n",
       " 'berkata',\n",
       " 'berkehendak',\n",
       " 'berkeinginan',\n",
       " 'berkenaan',\n",
       " 'berlainan',\n",
       " 'berlalu',\n",
       " 'berlangsung',\n",
       " 'berlebihan',\n",
       " 'bermacam',\n",
       " 'bermacam-macam',\n",
       " 'bermaksud',\n",
       " 'bermula',\n",
       " 'bersama',\n",
       " 'bersama-sama',\n",
       " 'bersiap',\n",
       " 'bersiap-siap',\n",
       " 'bertanya',\n",
       " 'bertanya-tanya',\n",
       " 'berturut',\n",
       " 'berturut-turut',\n",
       " 'bertutur',\n",
       " 'berujar',\n",
       " 'berupa',\n",
       " 'besar',\n",
       " 'betul',\n",
       " 'betulkah',\n",
       " 'biasa',\n",
       " 'biasanya',\n",
       " 'bila',\n",
       " 'bilakah',\n",
       " 'bisa',\n",
       " 'bisakah',\n",
       " 'boleh',\n",
       " 'bolehkah',\n",
       " 'bolehlah',\n",
       " 'buat',\n",
       " 'bukan',\n",
       " 'bukankah',\n",
       " 'bukanlah',\n",
       " 'bukannya',\n",
       " 'bulan',\n",
       " 'bung',\n",
       " 'cara',\n",
       " 'caranya',\n",
       " 'cukup',\n",
       " 'cukupkah',\n",
       " 'cukuplah',\n",
       " 'cuma',\n",
       " 'dahulu',\n",
       " 'dalam',\n",
       " 'dan',\n",
       " 'dapat',\n",
       " 'dari',\n",
       " 'daripada',\n",
       " 'datang',\n",
       " 'dekat',\n",
       " 'demi',\n",
       " 'demikian',\n",
       " 'demikianlah',\n",
       " 'dengan',\n",
       " 'depan',\n",
       " 'di',\n",
       " 'dia',\n",
       " 'diakhiri',\n",
       " 'diakhirinya',\n",
       " 'dialah',\n",
       " 'diantara',\n",
       " 'diantaranya',\n",
       " 'diberi',\n",
       " 'diberikan',\n",
       " 'diberikannya',\n",
       " 'dibuat',\n",
       " 'dibuatnya',\n",
       " 'didapat',\n",
       " 'didatangkan',\n",
       " 'digunakan',\n",
       " 'diibaratkan',\n",
       " 'diibaratkannya',\n",
       " 'diingat',\n",
       " 'diingatkan',\n",
       " 'diinginkan',\n",
       " 'dijawab',\n",
       " 'dijelaskan',\n",
       " 'dijelaskannya',\n",
       " 'dikarenakan',\n",
       " 'dikatakan',\n",
       " 'dikatakannya',\n",
       " 'dikerjakan',\n",
       " 'diketahui',\n",
       " 'diketahuinya',\n",
       " 'dikira',\n",
       " 'dilakukan',\n",
       " 'dilalui',\n",
       " 'dilihat',\n",
       " 'dimaksud',\n",
       " 'dimaksudkan',\n",
       " 'dimaksudkannya',\n",
       " 'dimaksudnya',\n",
       " 'diminta',\n",
       " 'dimintai',\n",
       " 'dimisalkan',\n",
       " 'dimulai',\n",
       " 'dimulailah',\n",
       " 'dimulainya',\n",
       " 'dimungkinkan',\n",
       " 'dini',\n",
       " 'dipastikan',\n",
       " 'diperbuat',\n",
       " 'diperbuatnya',\n",
       " 'dipergunakan',\n",
       " 'diperkirakan',\n",
       " 'diperlihatkan',\n",
       " 'diperlukan',\n",
       " 'diperlukannya',\n",
       " 'dipersoalkan',\n",
       " 'dipertanyakan',\n",
       " 'dipunyai',\n",
       " 'diri',\n",
       " 'dirinya',\n",
       " 'disampaikan',\n",
       " 'disebut',\n",
       " 'disebutkan',\n",
       " 'disebutkannya',\n",
       " 'disini',\n",
       " 'disinilah',\n",
       " 'ditambahkan',\n",
       " 'ditandaskan',\n",
       " 'ditanya',\n",
       " 'ditanyai',\n",
       " 'ditanyakan',\n",
       " 'ditegaskan',\n",
       " 'ditujukan',\n",
       " 'ditunjuk',\n",
       " 'ditunjuki',\n",
       " 'ditunjukkan',\n",
       " 'ditunjukkannya',\n",
       " 'ditunjuknya',\n",
       " 'dituturkan',\n",
       " 'dituturkannya',\n",
       " 'diucapkan',\n",
       " 'diucapkannya',\n",
       " 'diungkapkan',\n",
       " 'dong',\n",
       " 'dua',\n",
       " 'dulu',\n",
       " 'empat',\n",
       " 'enggak',\n",
       " 'enggaknya',\n",
       " 'entah',\n",
       " 'entahlah',\n",
       " 'guna',\n",
       " 'gunakan',\n",
       " 'hal',\n",
       " 'hampir',\n",
       " 'hanya',\n",
       " 'hanyalah',\n",
       " 'hari',\n",
       " 'harus',\n",
       " 'haruslah',\n",
       " 'harusnya',\n",
       " 'hendak',\n",
       " 'hendaklah',\n",
       " 'hendaknya',\n",
       " 'hingga',\n",
       " 'ia',\n",
       " 'ialah',\n",
       " 'ibarat',\n",
       " 'ibaratkan',\n",
       " 'ibaratnya',\n",
       " 'ibu',\n",
       " 'ikut',\n",
       " 'ingat',\n",
       " 'ingat-ingat',\n",
       " 'ingin',\n",
       " 'inginkah',\n",
       " 'inginkan',\n",
       " 'ini',\n",
       " 'inikah',\n",
       " 'inilah',\n",
       " 'itu',\n",
       " 'itukah',\n",
       " 'itulah',\n",
       " 'jadi',\n",
       " 'jadilah',\n",
       " 'jadinya',\n",
       " 'jangan',\n",
       " 'jangankan',\n",
       " 'janganlah',\n",
       " 'jauh',\n",
       " 'jawab',\n",
       " 'jawaban',\n",
       " 'jawabnya',\n",
       " 'jelas',\n",
       " 'jelaskan',\n",
       " 'jelaslah',\n",
       " 'jelasnya',\n",
       " 'jika',\n",
       " 'jikalau',\n",
       " 'juga',\n",
       " 'jumlah',\n",
       " 'jumlahnya',\n",
       " 'justru',\n",
       " 'kala',\n",
       " 'kalau',\n",
       " 'kalaulah',\n",
       " 'kalaupun',\n",
       " 'kalian',\n",
       " 'kami',\n",
       " 'kamilah',\n",
       " 'kamu',\n",
       " 'kamulah',\n",
       " 'kan',\n",
       " 'kapan',\n",
       " 'kapankah',\n",
       " 'kapanpun',\n",
       " 'karena',\n",
       " 'karenanya',\n",
       " 'kasus',\n",
       " 'kata',\n",
       " 'katakan',\n",
       " 'katakanlah',\n",
       " 'katanya',\n",
       " 'ke',\n",
       " 'keadaan',\n",
       " 'kebetulan',\n",
       " 'kecil',\n",
       " 'kedua',\n",
       " 'keduanya',\n",
       " 'keinginan',\n",
       " 'kelamaan',\n",
       " 'kelihatan',\n",
       " 'kelihatannya',\n",
       " 'kelima',\n",
       " 'keluar',\n",
       " 'kembali',\n",
       " 'kemudian',\n",
       " 'kemungkinan',\n",
       " 'kemungkinannya',\n",
       " 'kenapa',\n",
       " 'kepada',\n",
       " 'kepadanya',\n",
       " 'kesampaian',\n",
       " 'keseluruhan',\n",
       " 'keseluruhannya',\n",
       " 'keterlaluan',\n",
       " 'ketika',\n",
       " 'khususnya',\n",
       " 'kini',\n",
       " 'kinilah',\n",
       " 'kira',\n",
       " 'kira-kira',\n",
       " 'kiranya',\n",
       " 'kita',\n",
       " 'kitalah',\n",
       " 'kok',\n",
       " 'kurang',\n",
       " 'lagi',\n",
       " 'lagian',\n",
       " 'lah',\n",
       " 'lain',\n",
       " 'lainnya',\n",
       " 'lalu',\n",
       " 'lama',\n",
       " 'lamanya',\n",
       " 'lanjut',\n",
       " 'lanjutnya',\n",
       " 'lebih',\n",
       " 'lewat',\n",
       " 'lima',\n",
       " 'luar',\n",
       " 'macam',\n",
       " 'maka',\n",
       " 'makanya',\n",
       " 'makin',\n",
       " 'malah',\n",
       " 'malahan',\n",
       " 'mampu',\n",
       " 'mampukah',\n",
       " 'mana',\n",
       " 'manakala',\n",
       " 'manalagi',\n",
       " 'masa',\n",
       " 'masalah',\n",
       " 'masalahnya',\n",
       " 'masih',\n",
       " 'masihkah',\n",
       " 'masing',\n",
       " 'masing-masing',\n",
       " 'mau',\n",
       " 'maupun',\n",
       " 'melainkan',\n",
       " 'melakukan',\n",
       " 'melalui',\n",
       " 'melihat',\n",
       " 'melihatnya',\n",
       " 'memang',\n",
       " 'memastikan',\n",
       " 'memberi',\n",
       " 'memberikan',\n",
       " 'membuat',\n",
       " 'memerlukan',\n",
       " 'memihak',\n",
       " 'meminta',\n",
       " 'memintakan',\n",
       " 'memisalkan',\n",
       " 'memperbuat',\n",
       " 'mempergunakan',\n",
       " 'memperkirakan',\n",
       " 'memperlihatkan',\n",
       " 'mempersiapkan',\n",
       " 'mempersoalkan',\n",
       " 'mempertanyakan',\n",
       " 'mempunyai',\n",
       " 'memulai',\n",
       " 'memungkinkan',\n",
       " 'menaiki',\n",
       " 'menambahkan',\n",
       " 'menandaskan',\n",
       " 'menanti',\n",
       " 'menanti-nanti',\n",
       " 'menantikan',\n",
       " 'menanya',\n",
       " 'menanyai',\n",
       " 'menanyakan',\n",
       " 'mendapat',\n",
       " 'mendapatkan',\n",
       " 'mendatang',\n",
       " 'mendatangi',\n",
       " 'mendatangkan',\n",
       " 'menegaskan',\n",
       " 'mengakhiri',\n",
       " 'mengapa',\n",
       " 'mengatakan',\n",
       " 'mengatakannya',\n",
       " 'mengenai',\n",
       " 'mengerjakan',\n",
       " 'mengetahui',\n",
       " 'menggunakan',\n",
       " 'menghendaki',\n",
       " 'mengibaratkan',\n",
       " 'mengibaratkannya',\n",
       " 'mengingat',\n",
       " 'mengingatkan',\n",
       " 'menginginkan',\n",
       " 'mengira',\n",
       " 'mengucapkan',\n",
       " 'mengucapkannya',\n",
       " 'mengungkapkan',\n",
       " 'menjadi',\n",
       " 'menjawab',\n",
       " 'menjelaskan',\n",
       " 'menuju',\n",
       " 'menunjuk',\n",
       " 'menunjuki',\n",
       " 'menunjukkan',\n",
       " 'menunjuknya',\n",
       " 'menurut',\n",
       " 'menuturkan',\n",
       " 'menyampaikan',\n",
       " 'menyangkut',\n",
       " 'menyatakan',\n",
       " 'menyebutkan',\n",
       " 'menyeluruh',\n",
       " 'menyiapkan',\n",
       " 'merasa',\n",
       " 'mereka',\n",
       " 'merekalah',\n",
       " 'merupakan',\n",
       " 'meski',\n",
       " 'meskipun',\n",
       " 'meyakini',\n",
       " 'meyakinkan',\n",
       " 'minta',\n",
       " 'mirip',\n",
       " 'misal',\n",
       " 'misalkan',\n",
       " 'misalnya',\n",
       " 'mula',\n",
       " 'mulai',\n",
       " 'mulailah',\n",
       " 'mulanya',\n",
       " 'mungkin',\n",
       " 'mungkinkah',\n",
       " 'nah',\n",
       " 'naik',\n",
       " 'namun',\n",
       " 'nanti',\n",
       " 'nantinya',\n",
       " 'nyaris',\n",
       " 'nyatanya',\n",
       " 'oleh',\n",
       " 'olehnya',\n",
       " 'pada',\n",
       " 'padahal',\n",
       " 'padanya',\n",
       " 'pak',\n",
       " 'paling',\n",
       " 'panjang',\n",
       " 'pantas',\n",
       " 'para',\n",
       " 'pasti',\n",
       " 'pastilah',\n",
       " 'penting',\n",
       " 'pentingnya',\n",
       " 'per',\n",
       " 'percuma',\n",
       " 'perlu',\n",
       " 'perlukah',\n",
       " 'perlunya',\n",
       " 'pernah',\n",
       " 'persoalan',\n",
       " 'pertama',\n",
       " 'pertama-tama',\n",
       " 'pertanyaan',\n",
       " 'pertanyakan',\n",
       " 'pihak',\n",
       " 'pihaknya',\n",
       " 'pukul',\n",
       " 'pula',\n",
       " 'pun',\n",
       " 'punya',\n",
       " 'rasa',\n",
       " 'rasanya',\n",
       " 'rata',\n",
       " 'rupanya',\n",
       " 'saat',\n",
       " 'saatnya',\n",
       " 'saja',\n",
       " 'sajalah',\n",
       " 'saling',\n",
       " 'sama',\n",
       " 'sama-sama',\n",
       " 'sambil',\n",
       " 'sampai',\n",
       " 'sampai-sampai',\n",
       " 'sampaikan',\n",
       " 'sana',\n",
       " 'sangat',\n",
       " 'sangatlah',\n",
       " 'satu',\n",
       " 'saya',\n",
       " 'sayalah',\n",
       " 'se',\n",
       " 'sebab',\n",
       " 'sebabnya',\n",
       " 'sebagai',\n",
       " 'sebagaimana',\n",
       " 'sebagainya',\n",
       " 'sebagian',\n",
       " 'sebaik',\n",
       " 'sebaik-baiknya',\n",
       " 'sebaiknya',\n",
       " 'sebaliknya',\n",
       " 'sebanyak',\n",
       " 'sebegini',\n",
       " 'sebegitu',\n",
       " 'sebelum',\n",
       " 'sebelumnya',\n",
       " 'sebenarnya',\n",
       " 'seberapa',\n",
       " 'sebesar',\n",
       " 'sebetulnya',\n",
       " 'sebisanya',\n",
       " 'sebuah',\n",
       " 'sebut',\n",
       " 'sebutlah',\n",
       " 'sebutnya',\n",
       " 'secara',\n",
       " 'secukupnya',\n",
       " 'sedang',\n",
       " 'sedangkan',\n",
       " 'sedemikian',\n",
       " 'sedikit',\n",
       " 'sedikitnya',\n",
       " 'seenaknya',\n",
       " 'segala',\n",
       " 'segalanya',\n",
       " 'segera',\n",
       " 'seharusnya',\n",
       " 'sehingga',\n",
       " 'seingat',\n",
       " 'sejak',\n",
       " 'sejauh',\n",
       " 'sejenak',\n",
       " 'sejumlah',\n",
       " 'sekadar',\n",
       " 'sekadarnya',\n",
       " 'sekali',\n",
       " 'sekali-kali',\n",
       " 'sekalian',\n",
       " 'sekaligus',\n",
       " 'sekalipun',\n",
       " 'sekarang',\n",
       " 'sekarang',\n",
       " 'sekecil',\n",
       " 'seketika',\n",
       " 'sekiranya',\n",
       " 'sekitar',\n",
       " 'sekitarnya',\n",
       " 'sekurang-kurangnya',\n",
       " 'sekurangnya',\n",
       " 'sela',\n",
       " 'selain',\n",
       " 'selaku',\n",
       " 'selalu',\n",
       " 'selama',\n",
       " 'selama-lamanya',\n",
       " 'selamanya',\n",
       " 'selanjutnya',\n",
       " 'seluruh',\n",
       " 'seluruhnya',\n",
       " 'semacam',\n",
       " 'semakin',\n",
       " 'semampu',\n",
       " 'semampunya',\n",
       " 'semasa',\n",
       " 'semasih',\n",
       " 'semata',\n",
       " 'semata-mata',\n",
       " 'semaunya',\n",
       " 'sementara',\n",
       " 'semisal',\n",
       " 'semisalnya',\n",
       " 'sempat',\n",
       " 'semua',\n",
       " 'semuanya',\n",
       " 'semula',\n",
       " 'sendiri',\n",
       " 'sendirian',\n",
       " 'sendirinya',\n",
       " 'seolah',\n",
       " 'seolah-olah',\n",
       " 'seorang',\n",
       " 'sepanjang',\n",
       " 'sepantasnya',\n",
       " 'sepantasnyalah',\n",
       " 'seperlunya',\n",
       " 'seperti',\n",
       " 'sepertinya',\n",
       " 'sepihak',\n",
       " 'sering',\n",
       " 'seringnya',\n",
       " 'serta',\n",
       " 'serupa',\n",
       " 'sesaat',\n",
       " 'sesama',\n",
       " 'sesampai',\n",
       " 'sesegera',\n",
       " 'sesekali',\n",
       " 'seseorang',\n",
       " 'sesuatu',\n",
       " 'sesuatunya',\n",
       " 'sesudah',\n",
       " 'sesudahnya',\n",
       " 'setelah',\n",
       " 'setempat',\n",
       " 'setengah',\n",
       " 'seterusnya',\n",
       " 'setiap',\n",
       " 'setiba',\n",
       " 'setibanya',\n",
       " 'setidak-tidaknya',\n",
       " 'setidaknya',\n",
       " 'setinggi',\n",
       " 'seusai',\n",
       " 'sewaktu',\n",
       " 'siap',\n",
       " 'siapa',\n",
       " 'siapakah',\n",
       " 'siapapun',\n",
       " 'sini',\n",
       " 'sinilah',\n",
       " 'soal',\n",
       " 'soalnya',\n",
       " 'suatu',\n",
       " 'sudah',\n",
       " 'sudahkah',\n",
       " 'sudahlah',\n",
       " 'supaya',\n",
       " 'tadi',\n",
       " 'tadinya',\n",
       " 'tahu',\n",
       " 'tahun',\n",
       " 'tak',\n",
       " 'tambah',\n",
       " 'tambahnya',\n",
       " 'tampak',\n",
       " 'tampaknya',\n",
       " 'tandas',\n",
       " 'tandasnya',\n",
       " 'tanpa',\n",
       " 'tanya',\n",
       " 'tanyakan',\n",
       " 'tanyanya',\n",
       " 'tapi',\n",
       " 'tegas',\n",
       " 'tegasnya',\n",
       " 'telah',\n",
       " 'tempat',\n",
       " 'tengah',\n",
       " 'tentang',\n",
       " 'tentu',\n",
       " 'tentulah',\n",
       " 'tentunya',\n",
       " 'tepat',\n",
       " 'terakhir',\n",
       " 'terasa',\n",
       " 'terbanyak',\n",
       " 'terdahulu',\n",
       " 'terdapat',\n",
       " 'terdiri',\n",
       " 'terhadap',\n",
       " 'terhadapnya',\n",
       " 'teringat',\n",
       " 'teringat-ingat',\n",
       " 'terjadi',\n",
       " 'terjadilah',\n",
       " 'terjadinya',\n",
       " 'terkira',\n",
       " 'terlalu',\n",
       " 'terlebih',\n",
       " 'terlihat',\n",
       " 'termasuk',\n",
       " 'ternyata',\n",
       " 'tersampaikan',\n",
       " 'tersebut',\n",
       " 'tersebutlah',\n",
       " 'tertentu',\n",
       " 'tertuju',\n",
       " 'terus',\n",
       " 'terutama',\n",
       " 'tetap',\n",
       " 'tetapi',\n",
       " 'tiap',\n",
       " 'tiba',\n",
       " 'tiba-tiba',\n",
       " 'tidak',\n",
       " 'tidakkah',\n",
       " 'tidaklah',\n",
       " 'tiga',\n",
       " 'tinggi',\n",
       " 'toh',\n",
       " 'tunjuk',\n",
       " 'turut',\n",
       " 'tutur',\n",
       " 'tuturnya',\n",
       " 'ucap',\n",
       " 'ucapnya',\n",
       " 'ujar',\n",
       " 'ujarnya',\n",
       " 'umum',\n",
       " 'umumnya',\n",
       " 'ungkap',\n",
       " 'ungkapnya',\n",
       " 'untuk',\n",
       " 'usah',\n",
       " 'usai',\n",
       " 'waduh',\n",
       " 'wah',\n",
       " 'wahai',\n",
       " 'waktu',\n",
       " 'waktunya',\n",
       " 'walau',\n",
       " 'walaupun',\n",
       " 'wong',\n",
       " 'yaitu',\n",
       " 'yakin',\n",
       " 'yakni',\n",
       " 'yang']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('indonesian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2092ba6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['إذ',\n",
       " 'إذا',\n",
       " 'إذما',\n",
       " 'إذن',\n",
       " 'أف',\n",
       " 'أقل',\n",
       " 'أكثر',\n",
       " 'ألا',\n",
       " 'إلا',\n",
       " 'التي',\n",
       " 'الذي',\n",
       " 'الذين',\n",
       " 'اللاتي',\n",
       " 'اللائي',\n",
       " 'اللتان',\n",
       " 'اللتيا',\n",
       " 'اللتين',\n",
       " 'اللذان',\n",
       " 'اللذين',\n",
       " 'اللواتي',\n",
       " 'إلى',\n",
       " 'إليك',\n",
       " 'إليكم',\n",
       " 'إليكما',\n",
       " 'إليكن',\n",
       " 'أم',\n",
       " 'أما',\n",
       " 'أما',\n",
       " 'إما',\n",
       " 'أن',\n",
       " 'إن',\n",
       " 'إنا',\n",
       " 'أنا',\n",
       " 'أنت',\n",
       " 'أنتم',\n",
       " 'أنتما',\n",
       " 'أنتن',\n",
       " 'إنما',\n",
       " 'إنه',\n",
       " 'أنى',\n",
       " 'أنى',\n",
       " 'آه',\n",
       " 'آها',\n",
       " 'أو',\n",
       " 'أولاء',\n",
       " 'أولئك',\n",
       " 'أوه',\n",
       " 'آي',\n",
       " 'أي',\n",
       " 'أيها',\n",
       " 'إي',\n",
       " 'أين',\n",
       " 'أين',\n",
       " 'أينما',\n",
       " 'إيه',\n",
       " 'بخ',\n",
       " 'بس',\n",
       " 'بعد',\n",
       " 'بعض',\n",
       " 'بك',\n",
       " 'بكم',\n",
       " 'بكم',\n",
       " 'بكما',\n",
       " 'بكن',\n",
       " 'بل',\n",
       " 'بلى',\n",
       " 'بما',\n",
       " 'بماذا',\n",
       " 'بمن',\n",
       " 'بنا',\n",
       " 'به',\n",
       " 'بها',\n",
       " 'بهم',\n",
       " 'بهما',\n",
       " 'بهن',\n",
       " 'بي',\n",
       " 'بين',\n",
       " 'بيد',\n",
       " 'تلك',\n",
       " 'تلكم',\n",
       " 'تلكما',\n",
       " 'ته',\n",
       " 'تي',\n",
       " 'تين',\n",
       " 'تينك',\n",
       " 'ثم',\n",
       " 'ثمة',\n",
       " 'حاشا',\n",
       " 'حبذا',\n",
       " 'حتى',\n",
       " 'حيث',\n",
       " 'حيثما',\n",
       " 'حين',\n",
       " 'خلا',\n",
       " 'دون',\n",
       " 'ذا',\n",
       " 'ذات',\n",
       " 'ذاك',\n",
       " 'ذان',\n",
       " 'ذانك',\n",
       " 'ذلك',\n",
       " 'ذلكم',\n",
       " 'ذلكما',\n",
       " 'ذلكن',\n",
       " 'ذه',\n",
       " 'ذو',\n",
       " 'ذوا',\n",
       " 'ذواتا',\n",
       " 'ذواتي',\n",
       " 'ذي',\n",
       " 'ذين',\n",
       " 'ذينك',\n",
       " 'ريث',\n",
       " 'سوف',\n",
       " 'سوى',\n",
       " 'شتان',\n",
       " 'عدا',\n",
       " 'عسى',\n",
       " 'عل',\n",
       " 'على',\n",
       " 'عليك',\n",
       " 'عليه',\n",
       " 'عما',\n",
       " 'عن',\n",
       " 'عند',\n",
       " 'غير',\n",
       " 'فإذا',\n",
       " 'فإن',\n",
       " 'فلا',\n",
       " 'فمن',\n",
       " 'في',\n",
       " 'فيم',\n",
       " 'فيما',\n",
       " 'فيه',\n",
       " 'فيها',\n",
       " 'قد',\n",
       " 'كأن',\n",
       " 'كأنما',\n",
       " 'كأي',\n",
       " 'كأين',\n",
       " 'كذا',\n",
       " 'كذلك',\n",
       " 'كل',\n",
       " 'كلا',\n",
       " 'كلاهما',\n",
       " 'كلتا',\n",
       " 'كلما',\n",
       " 'كليكما',\n",
       " 'كليهما',\n",
       " 'كم',\n",
       " 'كم',\n",
       " 'كما',\n",
       " 'كي',\n",
       " 'كيت',\n",
       " 'كيف',\n",
       " 'كيفما',\n",
       " 'لا',\n",
       " 'لاسيما',\n",
       " 'لدى',\n",
       " 'لست',\n",
       " 'لستم',\n",
       " 'لستما',\n",
       " 'لستن',\n",
       " 'لسن',\n",
       " 'لسنا',\n",
       " 'لعل',\n",
       " 'لك',\n",
       " 'لكم',\n",
       " 'لكما',\n",
       " 'لكن',\n",
       " 'لكنما',\n",
       " 'لكي',\n",
       " 'لكيلا',\n",
       " 'لم',\n",
       " 'لما',\n",
       " 'لن',\n",
       " 'لنا',\n",
       " 'له',\n",
       " 'لها',\n",
       " 'لهم',\n",
       " 'لهما',\n",
       " 'لهن',\n",
       " 'لو',\n",
       " 'لولا',\n",
       " 'لوما',\n",
       " 'لي',\n",
       " 'لئن',\n",
       " 'ليت',\n",
       " 'ليس',\n",
       " 'ليسا',\n",
       " 'ليست',\n",
       " 'ليستا',\n",
       " 'ليسوا',\n",
       " 'ما',\n",
       " 'ماذا',\n",
       " 'متى',\n",
       " 'مذ',\n",
       " 'مع',\n",
       " 'مما',\n",
       " 'ممن',\n",
       " 'من',\n",
       " 'منه',\n",
       " 'منها',\n",
       " 'منذ',\n",
       " 'مه',\n",
       " 'مهما',\n",
       " 'نحن',\n",
       " 'نحو',\n",
       " 'نعم',\n",
       " 'ها',\n",
       " 'هاتان',\n",
       " 'هاته',\n",
       " 'هاتي',\n",
       " 'هاتين',\n",
       " 'هاك',\n",
       " 'هاهنا',\n",
       " 'هذا',\n",
       " 'هذان',\n",
       " 'هذه',\n",
       " 'هذي',\n",
       " 'هذين',\n",
       " 'هكذا',\n",
       " 'هل',\n",
       " 'هلا',\n",
       " 'هم',\n",
       " 'هما',\n",
       " 'هن',\n",
       " 'هنا',\n",
       " 'هناك',\n",
       " 'هنالك',\n",
       " 'هو',\n",
       " 'هؤلاء',\n",
       " 'هي',\n",
       " 'هيا',\n",
       " 'هيت',\n",
       " 'هيهات',\n",
       " 'والذي',\n",
       " 'والذين',\n",
       " 'وإذ',\n",
       " 'وإذا',\n",
       " 'وإن',\n",
       " 'ولا',\n",
       " 'ولكن',\n",
       " 'ولو',\n",
       " 'وما',\n",
       " 'ومن',\n",
       " 'وهو',\n",
       " 'يا',\n",
       " 'أبٌ',\n",
       " 'أخٌ',\n",
       " 'حمٌ',\n",
       " 'فو',\n",
       " 'أنتِ',\n",
       " 'يناير',\n",
       " 'فبراير',\n",
       " 'مارس',\n",
       " 'أبريل',\n",
       " 'مايو',\n",
       " 'يونيو',\n",
       " 'يوليو',\n",
       " 'أغسطس',\n",
       " 'سبتمبر',\n",
       " 'أكتوبر',\n",
       " 'نوفمبر',\n",
       " 'ديسمبر',\n",
       " 'جانفي',\n",
       " 'فيفري',\n",
       " 'مارس',\n",
       " 'أفريل',\n",
       " 'ماي',\n",
       " 'جوان',\n",
       " 'جويلية',\n",
       " 'أوت',\n",
       " 'كانون',\n",
       " 'شباط',\n",
       " 'آذار',\n",
       " 'نيسان',\n",
       " 'أيار',\n",
       " 'حزيران',\n",
       " 'تموز',\n",
       " 'آب',\n",
       " 'أيلول',\n",
       " 'تشرين',\n",
       " 'دولار',\n",
       " 'دينار',\n",
       " 'ريال',\n",
       " 'درهم',\n",
       " 'ليرة',\n",
       " 'جنيه',\n",
       " 'قرش',\n",
       " 'مليم',\n",
       " 'فلس',\n",
       " 'هللة',\n",
       " 'سنتيم',\n",
       " 'يورو',\n",
       " 'ين',\n",
       " 'يوان',\n",
       " 'شيكل',\n",
       " 'واحد',\n",
       " 'اثنان',\n",
       " 'ثلاثة',\n",
       " 'أربعة',\n",
       " 'خمسة',\n",
       " 'ستة',\n",
       " 'سبعة',\n",
       " 'ثمانية',\n",
       " 'تسعة',\n",
       " 'عشرة',\n",
       " 'أحد',\n",
       " 'اثنا',\n",
       " 'اثني',\n",
       " 'إحدى',\n",
       " 'ثلاث',\n",
       " 'أربع',\n",
       " 'خمس',\n",
       " 'ست',\n",
       " 'سبع',\n",
       " 'ثماني',\n",
       " 'تسع',\n",
       " 'عشر',\n",
       " 'ثمان',\n",
       " 'سبت',\n",
       " 'أحد',\n",
       " 'اثنين',\n",
       " 'ثلاثاء',\n",
       " 'أربعاء',\n",
       " 'خميس',\n",
       " 'جمعة',\n",
       " 'أول',\n",
       " 'ثان',\n",
       " 'ثاني',\n",
       " 'ثالث',\n",
       " 'رابع',\n",
       " 'خامس',\n",
       " 'سادس',\n",
       " 'سابع',\n",
       " 'ثامن',\n",
       " 'تاسع',\n",
       " 'عاشر',\n",
       " 'حادي',\n",
       " 'أ',\n",
       " 'ب',\n",
       " 'ت',\n",
       " 'ث',\n",
       " 'ج',\n",
       " 'ح',\n",
       " 'خ',\n",
       " 'د',\n",
       " 'ذ',\n",
       " 'ر',\n",
       " 'ز',\n",
       " 'س',\n",
       " 'ش',\n",
       " 'ص',\n",
       " 'ض',\n",
       " 'ط',\n",
       " 'ظ',\n",
       " 'ع',\n",
       " 'غ',\n",
       " 'ف',\n",
       " 'ق',\n",
       " 'ك',\n",
       " 'ل',\n",
       " 'م',\n",
       " 'ن',\n",
       " 'ه',\n",
       " 'و',\n",
       " 'ي',\n",
       " 'ء',\n",
       " 'ى',\n",
       " 'آ',\n",
       " 'ؤ',\n",
       " 'ئ',\n",
       " 'أ',\n",
       " 'ة',\n",
       " 'ألف',\n",
       " 'باء',\n",
       " 'تاء',\n",
       " 'ثاء',\n",
       " 'جيم',\n",
       " 'حاء',\n",
       " 'خاء',\n",
       " 'دال',\n",
       " 'ذال',\n",
       " 'راء',\n",
       " 'زاي',\n",
       " 'سين',\n",
       " 'شين',\n",
       " 'صاد',\n",
       " 'ضاد',\n",
       " 'طاء',\n",
       " 'ظاء',\n",
       " 'عين',\n",
       " 'غين',\n",
       " 'فاء',\n",
       " 'قاف',\n",
       " 'كاف',\n",
       " 'لام',\n",
       " 'ميم',\n",
       " 'نون',\n",
       " 'هاء',\n",
       " 'واو',\n",
       " 'ياء',\n",
       " 'همزة',\n",
       " 'ي',\n",
       " 'نا',\n",
       " 'ك',\n",
       " 'كن',\n",
       " 'ه',\n",
       " 'إياه',\n",
       " 'إياها',\n",
       " 'إياهما',\n",
       " 'إياهم',\n",
       " 'إياهن',\n",
       " 'إياك',\n",
       " 'إياكما',\n",
       " 'إياكم',\n",
       " 'إياك',\n",
       " 'إياكن',\n",
       " 'إياي',\n",
       " 'إيانا',\n",
       " 'أولالك',\n",
       " 'تانِ',\n",
       " 'تانِك',\n",
       " 'تِه',\n",
       " 'تِي',\n",
       " 'تَيْنِ',\n",
       " 'ثمّ',\n",
       " 'ثمّة',\n",
       " 'ذانِ',\n",
       " 'ذِه',\n",
       " 'ذِي',\n",
       " 'ذَيْنِ',\n",
       " 'هَؤلاء',\n",
       " 'هَاتانِ',\n",
       " 'هَاتِه',\n",
       " 'هَاتِي',\n",
       " 'هَاتَيْنِ',\n",
       " 'هَذا',\n",
       " 'هَذانِ',\n",
       " 'هَذِه',\n",
       " 'هَذِي',\n",
       " 'هَذَيْنِ',\n",
       " 'الألى',\n",
       " 'الألاء',\n",
       " 'أل',\n",
       " 'أنّى',\n",
       " 'أيّ',\n",
       " 'ّأيّان',\n",
       " 'أنّى',\n",
       " 'أيّ',\n",
       " 'ّأيّان',\n",
       " 'ذيت',\n",
       " 'كأيّ',\n",
       " 'كأيّن',\n",
       " 'بضع',\n",
       " 'فلان',\n",
       " 'وا',\n",
       " 'آمينَ',\n",
       " 'آهِ',\n",
       " 'آهٍ',\n",
       " 'آهاً',\n",
       " 'أُفٍّ',\n",
       " 'أُفٍّ',\n",
       " 'أفٍّ',\n",
       " 'أمامك',\n",
       " 'أمامكَ',\n",
       " 'أوّهْ',\n",
       " 'إلَيْكَ',\n",
       " 'إلَيْكَ',\n",
       " 'إليكَ',\n",
       " 'إليكنّ',\n",
       " 'إيهٍ',\n",
       " 'بخٍ',\n",
       " 'بسّ',\n",
       " 'بَسْ',\n",
       " 'بطآن',\n",
       " 'بَلْهَ',\n",
       " 'حاي',\n",
       " 'حَذارِ',\n",
       " 'حيَّ',\n",
       " 'حيَّ',\n",
       " 'دونك',\n",
       " 'رويدك',\n",
       " 'سرعان',\n",
       " 'شتانَ',\n",
       " 'شَتَّانَ',\n",
       " 'صهْ',\n",
       " 'صهٍ',\n",
       " 'طاق',\n",
       " 'طَق',\n",
       " 'عَدَسْ',\n",
       " 'كِخ',\n",
       " 'مكانَك',\n",
       " 'مكانَك',\n",
       " 'مكانَك',\n",
       " 'مكانكم',\n",
       " 'مكانكما',\n",
       " 'مكانكنّ',\n",
       " 'نَخْ',\n",
       " 'هاكَ',\n",
       " 'هَجْ',\n",
       " 'هلم',\n",
       " 'هيّا',\n",
       " 'هَيْهات',\n",
       " 'وا',\n",
       " 'واهاً',\n",
       " 'وراءَك',\n",
       " 'وُشْكَانَ',\n",
       " 'وَيْ',\n",
       " 'يفعلان',\n",
       " 'تفعلان',\n",
       " 'يفعلون',\n",
       " 'تفعلون',\n",
       " 'تفعلين',\n",
       " 'اتخذ',\n",
       " 'ألفى',\n",
       " 'تخذ',\n",
       " 'ترك',\n",
       " 'تعلَّم',\n",
       " 'جعل',\n",
       " 'حجا',\n",
       " 'حبيب',\n",
       " 'خال',\n",
       " 'حسب',\n",
       " 'خال',\n",
       " 'درى',\n",
       " 'رأى',\n",
       " 'زعم',\n",
       " 'صبر',\n",
       " 'ظنَّ',\n",
       " 'عدَّ',\n",
       " 'علم',\n",
       " 'غادر',\n",
       " 'ذهب',\n",
       " 'وجد',\n",
       " 'ورد',\n",
       " 'وهب',\n",
       " 'أسكن',\n",
       " 'أطعم',\n",
       " 'أعطى',\n",
       " 'رزق',\n",
       " 'زود',\n",
       " 'سقى',\n",
       " 'كسا',\n",
       " 'أخبر',\n",
       " 'أرى',\n",
       " 'أعلم',\n",
       " 'أنبأ',\n",
       " 'حدَث',\n",
       " 'خبَّر',\n",
       " 'نبَّا',\n",
       " 'أفعل به',\n",
       " 'ما أفعله',\n",
       " 'بئس',\n",
       " 'ساء',\n",
       " 'طالما',\n",
       " 'قلما',\n",
       " 'لات',\n",
       " 'لكنَّ',\n",
       " 'ءَ',\n",
       " 'أجل',\n",
       " 'إذاً',\n",
       " 'أمّا',\n",
       " 'إمّا',\n",
       " 'إنَّ',\n",
       " 'أنًّ',\n",
       " 'أى',\n",
       " 'إى',\n",
       " 'أيا',\n",
       " 'ب',\n",
       " 'ثمَّ',\n",
       " 'جلل',\n",
       " 'جير',\n",
       " 'رُبَّ',\n",
       " 'س',\n",
       " 'علًّ',\n",
       " 'ف',\n",
       " 'كأنّ',\n",
       " 'كلَّا',\n",
       " 'كى',\n",
       " 'ل',\n",
       " 'لات',\n",
       " 'لعلَّ',\n",
       " 'لكنَّ',\n",
       " 'لكنَّ',\n",
       " 'م',\n",
       " 'نَّ',\n",
       " 'هلّا',\n",
       " 'وا',\n",
       " 'أل',\n",
       " 'إلّا',\n",
       " 'ت',\n",
       " 'ك',\n",
       " 'لمّا',\n",
       " 'ن',\n",
       " 'ه',\n",
       " 'و',\n",
       " 'ا',\n",
       " 'ي',\n",
       " 'تجاه',\n",
       " 'تلقاء',\n",
       " 'جميع',\n",
       " 'حسب',\n",
       " 'سبحان',\n",
       " 'شبه',\n",
       " 'لعمر',\n",
       " 'مثل',\n",
       " 'معاذ',\n",
       " 'أبو',\n",
       " 'أخو',\n",
       " 'حمو',\n",
       " 'فو',\n",
       " 'مئة',\n",
       " 'مئتان',\n",
       " 'ثلاثمئة',\n",
       " 'أربعمئة',\n",
       " 'خمسمئة',\n",
       " 'ستمئة',\n",
       " 'سبعمئة',\n",
       " 'ثمنمئة',\n",
       " 'تسعمئة',\n",
       " 'مائة',\n",
       " 'ثلاثمائة',\n",
       " 'أربعمائة',\n",
       " 'خمسمائة',\n",
       " 'ستمائة',\n",
       " 'سبعمائة',\n",
       " 'ثمانمئة',\n",
       " 'تسعمائة',\n",
       " 'عشرون',\n",
       " 'ثلاثون',\n",
       " 'اربعون',\n",
       " 'خمسون',\n",
       " 'ستون',\n",
       " 'سبعون',\n",
       " 'ثمانون',\n",
       " 'تسعون',\n",
       " 'عشرين',\n",
       " 'ثلاثين',\n",
       " 'اربعين',\n",
       " 'خمسين',\n",
       " 'ستين',\n",
       " 'سبعين',\n",
       " 'ثمانين',\n",
       " 'تسعين',\n",
       " 'بضع',\n",
       " 'نيف',\n",
       " 'أجمع',\n",
       " 'جميع',\n",
       " 'عامة',\n",
       " 'عين',\n",
       " 'نفس',\n",
       " 'لا سيما',\n",
       " 'أصلا',\n",
       " 'أهلا',\n",
       " 'أيضا',\n",
       " 'بؤسا',\n",
       " 'بعدا',\n",
       " 'بغتة',\n",
       " 'تعسا',\n",
       " 'حقا',\n",
       " 'حمدا',\n",
       " 'خلافا',\n",
       " 'خاصة',\n",
       " 'دواليك',\n",
       " 'سحقا',\n",
       " 'سرا',\n",
       " 'سمعا',\n",
       " 'صبرا',\n",
       " 'صدقا',\n",
       " 'صراحة',\n",
       " 'طرا',\n",
       " 'عجبا',\n",
       " 'عيانا',\n",
       " 'غالبا',\n",
       " 'فرادى',\n",
       " 'فضلا',\n",
       " 'قاطبة',\n",
       " 'كثيرا',\n",
       " 'لبيك',\n",
       " 'معاذ',\n",
       " 'أبدا',\n",
       " 'إزاء',\n",
       " 'أصلا',\n",
       " 'الآن',\n",
       " 'أمد',\n",
       " 'أمس',\n",
       " 'آنفا',\n",
       " 'آناء',\n",
       " 'أنّى',\n",
       " 'أول',\n",
       " 'أيّان',\n",
       " 'تارة',\n",
       " 'ثمّ',\n",
       " 'ثمّة',\n",
       " 'حقا',\n",
       " 'صباح',\n",
       " 'مساء',\n",
       " 'ضحوة',\n",
       " 'عوض',\n",
       " 'غدا',\n",
       " 'غداة',\n",
       " 'قطّ',\n",
       " 'كلّما',\n",
       " 'لدن',\n",
       " 'لمّا',\n",
       " 'مرّة',\n",
       " 'قبل',\n",
       " 'خلف',\n",
       " 'أمام',\n",
       " 'فوق',\n",
       " 'تحت',\n",
       " 'يمين',\n",
       " 'شمال',\n",
       " 'ارتدّ',\n",
       " 'استحال',\n",
       " 'أصبح',\n",
       " 'أضحى',\n",
       " 'آض',\n",
       " 'أمسى',\n",
       " 'انقلب',\n",
       " 'بات',\n",
       " 'تبدّل',\n",
       " 'تحوّل',\n",
       " 'حار',\n",
       " 'رجع',\n",
       " 'راح',\n",
       " 'صار',\n",
       " 'ظلّ',\n",
       " 'عاد',\n",
       " 'غدا',\n",
       " 'كان',\n",
       " 'ما انفك',\n",
       " 'ما برح',\n",
       " 'مادام',\n",
       " 'مازال',\n",
       " 'مافتئ',\n",
       " 'ابتدأ',\n",
       " 'أخذ',\n",
       " 'اخلولق',\n",
       " 'أقبل',\n",
       " 'انبرى',\n",
       " 'أنشأ',\n",
       " 'أوشك',\n",
       " 'جعل',\n",
       " 'حرى',\n",
       " 'شرع',\n",
       " 'طفق',\n",
       " 'علق',\n",
       " 'قام',\n",
       " 'كرب',\n",
       " 'كاد',\n",
       " 'هبّ']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('arabic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f07a45a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Porter Stemmer\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "88d5aeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "260284f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I have three visions for India.',\n",
       " 'In 3000 years of our history, people from all over \\n               the world have come and invaded us, captured our lands, conquered our minds.',\n",
       " 'From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\\n               the French, the Dutch, all of them came and looted us, took over what was ours.',\n",
       " 'Yet we have not done this to any other nation.',\n",
       " 'We have not conquered anyone.',\n",
       " 'We have not grabbed their land, their culture, \\n               their history and tried to enforce our way of life on them.',\n",
       " 'Why?',\n",
       " 'Because we respect the freedom of others.That is why my \\n               first vision is that of freedom.',\n",
       " 'I believe that India got its first vision of \\n               this in 1857, when we started the War of Independence.',\n",
       " 'It is this freedom that\\n               we must protect and nurture and build on.',\n",
       " 'If we are not free, no one will respect us.',\n",
       " 'My second vision for India’s development.',\n",
       " 'For fifty years we have been a developing nation.',\n",
       " 'It is time we see ourselves as a developed nation.',\n",
       " 'We are among the top 5 nations of the world\\n               in terms of GDP.',\n",
       " 'We have a 10 percent growth rate in most areas.',\n",
       " 'Our poverty levels are falling.',\n",
       " 'Our achievements are being globally recognised today.',\n",
       " 'Yet we lack the self-confidence to\\n               see ourselves as a developed nation, self-reliant and self-assured.',\n",
       " 'Isn’t this incorrect?',\n",
       " 'I have a third vision.',\n",
       " 'India must stand up to the world.',\n",
       " 'Because I believe that unless India \\n               stands up to the world, no one will respect us.',\n",
       " 'Only strength respects strength.',\n",
       " 'We must be \\n               strong not only as a military power but also as an economic power.',\n",
       " 'Both must go hand-in-hand.',\n",
       " 'My good fortune was to have worked with three great minds.',\n",
       " 'Dr. Vikram Sarabhai of the Dept.',\n",
       " 'of \\n               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.',\n",
       " 'I was lucky to have worked with all three of them closely and consider this the great opportunity of my life.',\n",
       " 'I see four milestones in my career']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenization semua paragraf\n",
    "# mengubah semua paragraf menjadi kalimat\n",
    "# paragraph ----> sentence\n",
    "nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0286bfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9c4895bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f0d1f4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply Stopwords And Filter And then Apply Stemming\n",
    "# itunglah semua jumlah kalimat\n",
    "for i in range(len(sentences)):\n",
    "    # lakukan tokenisasi ke dalam setiap kalimat menjadi beberapa kata\n",
    "    words=nltk.word_tokenize(sentences[i])\n",
    "    # dan lakukan stemming ke dalam setiap kata dan hilangkan stopwords pada setiap kata dengan bahasa inggris\n",
    "    words=[stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    # kita gabungin beberapa kata yang sudah di stem dan dihilangkan stopwords nya\n",
    "    sentences[i]=' '.join(words)# converting all the list of words into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1c7cddeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i three vision india .',\n",
       " 'in 3000 year histori , peopl world come invad us , captur land , conquer mind .',\n",
       " 'from alexand onward , greek , turk , mogul , portugues , british , french , dutch , came loot us , took .',\n",
       " 'yet done nation .',\n",
       " 'we conquer anyon .',\n",
       " 'we grab land , cultur , histori tri enforc way life .',\n",
       " 'whi ?',\n",
       " 'becaus respect freedom others.that first vision freedom .',\n",
       " 'i believ india got first vision 1857 , start war independ .',\n",
       " 'it freedom must protect nurtur build .',\n",
       " 'if free , one respect us .',\n",
       " 'my second vision india ’ develop .',\n",
       " 'for fifti year develop nation .',\n",
       " 'it time see develop nation .',\n",
       " 'we among top 5 nation world term gdp .',\n",
       " 'we 10 percent growth rate area .',\n",
       " 'our poverti level fall .',\n",
       " 'our achiev global recognis today .',\n",
       " 'yet lack self-confid see develop nation , self-reli self-assur .',\n",
       " 'isn ’ incorrect ?',\n",
       " 'i third vision .',\n",
       " 'india must stand world .',\n",
       " 'becaus i believ unless india stand world , one respect us .',\n",
       " 'onli strength respect strength .',\n",
       " 'we must strong militari power also econom power .',\n",
       " 'both must go hand-in-hand .',\n",
       " 'my good fortun work three great mind .',\n",
       " 'dr. vikram sarabhai dept .',\n",
       " 'space , professor satish dhawan , succeed dr. brahm prakash , father nuclear materi .',\n",
       " 'i lucki work three close consid great opportun life .',\n",
       " 'i see four mileston career']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6bfbea47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# snowball stemmer\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "8b8a3dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowstemmer=SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e0d78fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "90fbbdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply Stopwords And Filter And then Apply Snowball Stemming\n",
    "for i in range(len(sentences)):\n",
    "    words=nltk.word_tokenize(sentences[i])\n",
    "    words=[snowstemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i]=' '.join(words)# converting all the list of words into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "21a37b96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i three vision india .',\n",
       " 'in 3000 year histori , peopl world come invad us , captur land , conquer mind .',\n",
       " 'from alexand onward , greek , turk , mogul , portugues , british , french , dutch , came loot us , took .',\n",
       " 'yet done nation .',\n",
       " 'we conquer anyon .',\n",
       " 'we grab land , cultur , histori tri enforc way life .',\n",
       " 'whi ?',\n",
       " 'becaus respect freedom others.that first vision freedom .',\n",
       " 'i believ india got first vision 1857 , start war independ .',\n",
       " 'it freedom must protect nurtur build .',\n",
       " 'if free , one respect us .',\n",
       " 'my second vision india ’ develop .',\n",
       " 'for fifti year develop nation .',\n",
       " 'it time see develop nation .',\n",
       " 'we among top 5 nation world term gdp .',\n",
       " 'we 10 percent growth rate area .',\n",
       " 'our poverti level fall .',\n",
       " 'our achiev global recognis today .',\n",
       " 'yet lack self-confid see develop nation , self-reli self-assur .',\n",
       " 'isn ’ incorrect ?',\n",
       " 'i third vision .',\n",
       " 'india must stand world .',\n",
       " 'becaus i believ unless india stand world , one respect us .',\n",
       " 'onli strength respect strength .',\n",
       " 'we must strong militari power also econom power .',\n",
       " 'both must go hand-in-hand .',\n",
       " 'my good fortun work three great mind .',\n",
       " 'dr. vikram sarabhai dept .',\n",
       " 'space , professor satish dhawan , succeed dr. brahm prakash , father nuclear materi .',\n",
       " 'i lucki work three close consid great opportun life .',\n",
       " 'i see four mileston career']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "47828859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "2eea0aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e38ba935",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply Stopwords And Filter And then Apply Lemmatization\n",
    "for i in range(len(sentences)):\n",
    "    words=nltk.word_tokenize(sentences[i])\n",
    "    words=[lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i]=' '.join(words)# converting all the list of words into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b3730237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I three vision India .',\n",
       " 'In 3000 year history , people world come invaded u , captured land , conquered mind .',\n",
       " 'From Alexander onwards , Greeks , Turks , Moguls , Portuguese , British , French , Dutch , came looted u , took .',\n",
       " 'Yet done nation .',\n",
       " 'We conquered anyone .',\n",
       " 'We grabbed land , culture , history tried enforce way life .',\n",
       " 'Why ?',\n",
       " 'Because respect freedom others.That first vision freedom .',\n",
       " 'I believe India got first vision 1857 , started War Independence .',\n",
       " 'It freedom must protect nurture build .',\n",
       " 'If free , one respect u .',\n",
       " 'My second vision India ’ development .',\n",
       " 'For fifty year developing nation .',\n",
       " 'It time see developed nation .',\n",
       " 'We among top 5 nation world term GDP .',\n",
       " 'We 10 percent growth rate area .',\n",
       " 'Our poverty level falling .',\n",
       " 'Our achievement globally recognised today .',\n",
       " 'Yet lack self-confidence see developed nation , self-reliant self-assured .',\n",
       " 'Isn ’ incorrect ?',\n",
       " 'I third vision .',\n",
       " 'India must stand world .',\n",
       " 'Because I believe unless India stand world , one respect u .',\n",
       " 'Only strength respect strength .',\n",
       " 'We must strong military power also economic power .',\n",
       " 'Both must go hand-in-hand .',\n",
       " 'My good fortune worked three great mind .',\n",
       " 'Dr. Vikram Sarabhai Dept .',\n",
       " 'space , Professor Satish Dhawan , succeeded Dr. Brahm Prakash , father nuclear material .',\n",
       " 'I lucky worked three closely consider great opportunity life .',\n",
       " 'I see four milestone career']"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ddbc2c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "7a50172f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# biar huruf nya kecil semua\n",
    "## Apply Stopwords And Filter And then Apply Lemmatization\n",
    "for i in range(len(sentences)):\n",
    "    words=nltk.word_tokenize(sentences[i])\n",
    "    words=[lemmatizer.lemmatize(word.lower(),pos='v') for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i]=' '.join(words)# converting all the list of words into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4a1ed01f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i three visions india .',\n",
       " 'in 3000 years history , people world come invade us , capture land , conquer mind .',\n",
       " 'from alexander onwards , greeks , turks , moguls , portuguese , british , french , dutch , come loot us , take .',\n",
       " 'yet do nation .',\n",
       " 'we conquer anyone .',\n",
       " 'we grab land , culture , history try enforce way life .',\n",
       " 'why ?',\n",
       " 'because respect freedom others.that first vision freedom .',\n",
       " 'i believe india get first vision 1857 , start war independence .',\n",
       " 'it freedom must protect nurture build .',\n",
       " 'if free , one respect us .',\n",
       " 'my second vision india ’ development .',\n",
       " 'for fifty years develop nation .',\n",
       " 'it time see develop nation .',\n",
       " 'we among top 5 nations world term gdp .',\n",
       " 'we 10 percent growth rate areas .',\n",
       " 'our poverty level fall .',\n",
       " 'our achievements globally recognise today .',\n",
       " 'yet lack self-confidence see develop nation , self-reliant self-assured .',\n",
       " 'isn ’ incorrect ?',\n",
       " 'i third vision .',\n",
       " 'india must stand world .',\n",
       " 'because i believe unless india stand world , one respect us .',\n",
       " 'only strength respect strength .',\n",
       " 'we must strong military power also economic power .',\n",
       " 'both must go hand-in-hand .',\n",
       " 'my good fortune work three great mind .',\n",
       " 'dr. vikram sarabhai dept .',\n",
       " 'space , professor satish dhawan , succeed dr. brahm prakash , father nuclear material .',\n",
       " 'i lucky work three closely consider great opportunity life .',\n",
       " 'i see four milestones career']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e4719a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a288821",
   "metadata": {},
   "source": [
    "## Part Of Speech Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f678a1",
   "metadata": {},
   "source": [
    "**Part of Speech (POS) tags** adalah label yang diberikan pada setiap kata dalam sebuah kalimat untuk menunjukkan kategori atau kelas kata tersebut dalam tata bahasa, seperti kata benda (noun), kata kerja (verb), kata sifat (adjective), kata keterangan (adverb), dan lain-lain.\n",
    "\n",
    "#### Manfaat POS Tagging\n",
    "- **Memahami Struktur Kalimat:** Membantu analisis sintaksis dan memahami hubungan antar kata dalam kalimat.\n",
    "\n",
    "- **Pra-pemrosesan NLP:** Digunakan untuk filtering kata berdasarkan kelasnya, misalnya hanya mengambil kata benda atau kata kerja.\n",
    "\n",
    "- **Meningkatkan Akurasi Model:** Berguna dalam berbagai aplikasi NLP seperti Named Entity Recognition, sentiment analysis, dan machine translation.\n",
    "\n",
    "- **Ekstraksi Informasi:** Memudahkan pengambilan informasi spesifik, seperti subjek, objek, atau aksi dalam teks.\n",
    "\n",
    "#### Kapan Digunakan POS Tagging?\n",
    "- **Text Preprocessing:** Sebelum analisis lebih lanjut, seperti lemmatization yang membutuhkan POS untuk hasil yang akurat.\n",
    "\n",
    "- **Information Extraction:** Saat ingin mengekstrak entitas atau fakta dari teks.\n",
    "\n",
    "- **Parsing dan Syntax Analysis:** Untuk membangun pohon sintaksis atau analisis struktur kalimat.\n",
    "\n",
    "- **Aplikasi NLP Lanjutan:** Chatbot, summarization, dan question answering sering menggunakan POS tagging untuk memahami konteks dan makna kalimat.\n",
    "\n",
    "Other References : https://www.geeksforgeeks.org/python/part-speech-tagging-stop-words-using-nltk-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2222466e",
   "metadata": {},
   "source": [
    "\"Bali is a beautiful place\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765b0966",
   "metadata": {},
   "source": [
    "```text\n",
    "CC coordinating conjunction \n",
    "CD cardinal digit \n",
    "DT determiner \n",
    "EX existential there (like: \"there is\" ... think of it like \"there exists\") \n",
    "FW foreign word \n",
    "IN preposition/subordinating conjunction \n",
    "JJ adjective - 'big' \n",
    "JJR adjective, comparative - 'bigger' \n",
    "JJS adjective, superlative - 'biggest' \n",
    "LS list marker 1) \n",
    "MD modal - could, will \n",
    "NN noun, singular '- desk' \n",
    "NNS noun plural - 'desks' \n",
    "NNP proper noun, singular - 'Harrison' \n",
    "NNPS proper noun, plural - 'Americans' \n",
    "PDT predeterminer - 'all the kids' \n",
    "POS possessive ending parent's \n",
    "PRP personal pronoun -  I, he, she \n",
    "PRP$ possessive pronoun - my, his, hers \n",
    "RB adverb - very, silently, \n",
    "RBR adverb, comparative - better \n",
    "RBS adverb, superlative - best \n",
    "RP particle - give up \n",
    "TO - to go 'to' the store. \n",
    "UH interjection - errrrrrrrm \n",
    "VB verb, base form - take \n",
    "VBD verb, past tense - took \n",
    "VBG verb, gerund/present participle - taking \n",
    "VBN verb, past participle - taken \n",
    "VBP verb, sing. present, non-3d - take \n",
    "VBZ verb, 3rd person sing. present - takes \n",
    "WDT wh-determiner - which \n",
    "WP wh-pronoun - who, what \n",
    "WP$ possessive wh-pronoun, eg- whose \n",
    "WRB wh-adverb, eg- where, when\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "1806639d",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"I have three visions for India. In 3000 years of our history, people from all over \n",
    "               the world have come and invaded us, captured our lands, conquered our minds. \n",
    "               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
    "               the French, the Dutch, all of them came and looted us, took over what was ours. \n",
    "               Yet we have not done this to any other nation. We have not conquered anyone. \n",
    "               We have not grabbed their land, their culture, \n",
    "               their history and tried to enforce our way of life on them. \n",
    "               Why? Because we respect the freedom of others.That is why my \n",
    "               first vision is that of freedom. I believe that India got its first vision of \n",
    "               this in 1857, when we started the War of Independence. It is this freedom that\n",
    "               we must protect and nurture and build on. If we are not free, no one will respect us.\n",
    "               My second vision for India’s development. For fifty years we have been a developing nation.\n",
    "               It is time we see ourselves as a developed nation. We are among the top 5 nations of the world\n",
    "               in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.\n",
    "               Our achievements are being globally recognised today. Yet we lack the self-confidence to\n",
    "               see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect?\n",
    "               I have a third vision. India must stand up to the world. Because I believe that unless India \n",
    "               stands up to the world, no one will respect us. Only strength respects strength. We must be \n",
    "               strong not only as a military power but also as an economic power. Both must go hand-in-hand. \n",
    "               My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of \n",
    "               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n",
    "               I was lucky to have worked with all three of them closely and consider this the great opportunity of my life. \n",
    "               I see four milestones in my career\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "55105d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "sentences=nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "3a302cd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I have three visions for India.',\n",
       " 'In 3000 years of our history, people from all over \\n               the world have come and invaded us, captured our lands, conquered our minds.',\n",
       " 'From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\\n               the French, the Dutch, all of them came and looted us, took over what was ours.',\n",
       " 'Yet we have not done this to any other nation.',\n",
       " 'We have not conquered anyone.',\n",
       " 'We have not grabbed their land, their culture, \\n               their history and tried to enforce our way of life on them.',\n",
       " 'Why?',\n",
       " 'Because we respect the freedom of others.That is why my \\n               first vision is that of freedom.',\n",
       " 'I believe that India got its first vision of \\n               this in 1857, when we started the War of Independence.',\n",
       " 'It is this freedom that\\n               we must protect and nurture and build on.',\n",
       " 'If we are not free, no one will respect us.',\n",
       " 'My second vision for India’s development.',\n",
       " 'For fifty years we have been a developing nation.',\n",
       " 'It is time we see ourselves as a developed nation.',\n",
       " 'We are among the top 5 nations of the world\\n               in terms of GDP.',\n",
       " 'We have a 10 percent growth rate in most areas.',\n",
       " 'Our poverty levels are falling.',\n",
       " 'Our achievements are being globally recognised today.',\n",
       " 'Yet we lack the self-confidence to\\n               see ourselves as a developed nation, self-reliant and self-assured.',\n",
       " 'Isn’t this incorrect?',\n",
       " 'I have a third vision.',\n",
       " 'India must stand up to the world.',\n",
       " 'Because I believe that unless India \\n               stands up to the world, no one will respect us.',\n",
       " 'Only strength respects strength.',\n",
       " 'We must be \\n               strong not only as a military power but also as an economic power.',\n",
       " 'Both must go hand-in-hand.',\n",
       " 'My good fortune was to have worked with three great minds.',\n",
       " 'Dr. Vikram Sarabhai of the Dept.',\n",
       " 'of \\n               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.',\n",
       " 'I was lucky to have worked with all three of them closely and consider this the great opportunity of my life.',\n",
       " 'I see four milestones in my career']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1beebae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Vanya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b1b8fc1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('three', 'CD'), ('visions', 'NNS'), ('India', 'NNP'), ('.', '.')]\n",
      "[('In', 'IN'), ('3000', 'CD'), ('years', 'NNS'), ('history', 'NN'), (',', ','), ('people', 'NNS'), ('world', 'NN'), ('come', 'VBP'), ('invaded', 'VBN'), ('us', 'PRP'), (',', ','), ('captured', 'VBD'), ('lands', 'NNS'), (',', ','), ('conquered', 'VBD'), ('minds', 'NNS'), ('.', '.')]\n",
      "[('From', 'IN'), ('Alexander', 'NNP'), ('onwards', 'NNS'), (',', ','), ('Greeks', 'NNP'), (',', ','), ('Turks', 'NNP'), (',', ','), ('Moguls', 'NNP'), (',', ','), ('Portuguese', 'NNP'), (',', ','), ('British', 'NNP'), (',', ','), ('French', 'NNP'), (',', ','), ('Dutch', 'NNP'), (',', ','), ('came', 'VBD'), ('looted', 'JJ'), ('us', 'PRP'), (',', ','), ('took', 'VBD'), ('.', '.')]\n",
      "[('Yet', 'RB'), ('done', 'VBN'), ('nation', 'NN'), ('.', '.')]\n",
      "[('We', 'PRP'), ('conquered', 'VBD'), ('anyone', 'NN'), ('.', '.')]\n",
      "[('We', 'PRP'), ('grabbed', 'VBD'), ('land', 'NN'), (',', ','), ('culture', 'NN'), (',', ','), ('history', 'NN'), ('tried', 'VBD'), ('enforce', 'JJ'), ('way', 'NN'), ('life', 'NN'), ('.', '.')]\n",
      "[('Why', 'WRB'), ('?', '.')]\n",
      "[('Because', 'IN'), ('respect', 'NN'), ('freedom', 'NN'), ('others.That', 'IN'), ('first', 'JJ'), ('vision', 'NN'), ('freedom', 'NN'), ('.', '.')]\n",
      "[('I', 'PRP'), ('believe', 'VBP'), ('India', 'NNP'), ('got', 'VBD'), ('first', 'JJ'), ('vision', 'NN'), ('1857', 'CD'), (',', ','), ('started', 'VBD'), ('War', 'NNP'), ('Independence', 'NNP'), ('.', '.')]\n",
      "[('It', 'PRP'), ('freedom', 'NN'), ('must', 'MD'), ('protect', 'VB'), ('nurture', 'NN'), ('build', 'NN'), ('.', '.')]\n",
      "[('If', 'IN'), ('free', 'JJ'), (',', ','), ('one', 'CD'), ('respect', 'NN'), ('us', 'PRP'), ('.', '.')]\n",
      "[('My', 'PRP$'), ('second', 'JJ'), ('vision', 'NN'), ('India', 'NNP'), ('’', 'NNP'), ('development', 'NN'), ('.', '.')]\n",
      "[('For', 'IN'), ('fifty', 'JJ'), ('years', 'NNS'), ('developing', 'VBG'), ('nation', 'NN'), ('.', '.')]\n",
      "[('It', 'PRP'), ('time', 'NN'), ('see', 'VB'), ('developed', 'JJ'), ('nation', 'NN'), ('.', '.')]\n",
      "[('We', 'PRP'), ('among', 'IN'), ('top', 'JJ'), ('5', 'CD'), ('nations', 'NNS'), ('world', 'NN'), ('terms', 'NNS'), ('GDP', 'NNP'), ('.', '.')]\n",
      "[('We', 'PRP'), ('10', 'CD'), ('percent', 'JJ'), ('growth', 'NN'), ('rate', 'NN'), ('areas', 'NNS'), ('.', '.')]\n",
      "[('Our', 'PRP$'), ('poverty', 'NN'), ('levels', 'NNS'), ('falling', 'VBG'), ('.', '.')]\n",
      "[('Our', 'PRP$'), ('achievements', 'NNS'), ('globally', 'RB'), ('recognised', 'VBN'), ('today', 'NN'), ('.', '.')]\n",
      "[('Yet', 'RB'), ('lack', 'JJ'), ('self-confidence', 'NN'), ('see', 'NN'), ('developed', 'JJ'), ('nation', 'NN'), (',', ','), ('self-reliant', 'JJ'), ('self-assured', 'JJ'), ('.', '.')]\n",
      "[('Isn', 'NNP'), ('’', 'NNP'), ('incorrect', 'NN'), ('?', '.')]\n",
      "[('I', 'PRP'), ('third', 'JJ'), ('vision', 'NN'), ('.', '.')]\n",
      "[('India', 'NNP'), ('must', 'MD'), ('stand', 'VB'), ('world', 'NN'), ('.', '.')]\n",
      "[('Because', 'IN'), ('I', 'PRP'), ('believe', 'VBP'), ('unless', 'IN'), ('India', 'NNP'), ('stands', 'VBZ'), ('world', 'NN'), (',', ','), ('one', 'CD'), ('respect', 'NN'), ('us', 'PRP'), ('.', '.')]\n",
      "[('Only', 'RB'), ('strength', 'NN'), ('respects', 'NNS'), ('strength', 'NN'), ('.', '.')]\n",
      "[('We', 'PRP'), ('must', 'MD'), ('strong', 'JJ'), ('military', 'JJ'), ('power', 'NN'), ('also', 'RB'), ('economic', 'JJ'), ('power', 'NN'), ('.', '.')]\n",
      "[('Both', 'DT'), ('must', 'MD'), ('go', 'VB'), ('hand-in-hand', 'NN'), ('.', '.')]\n",
      "[('My', 'PRP$'), ('good', 'JJ'), ('fortune', 'NN'), ('worked', 'VBD'), ('three', 'CD'), ('great', 'JJ'), ('minds', 'NNS'), ('.', '.')]\n",
      "[('Dr.', 'NNP'), ('Vikram', 'NNP'), ('Sarabhai', 'NNP'), ('Dept', 'NNP'), ('.', '.')]\n",
      "[('space', 'NN'), (',', ','), ('Professor', 'NNP'), ('Satish', 'NNP'), ('Dhawan', 'NNP'), (',', ','), ('succeeded', 'VBD'), ('Dr.', 'NNP'), ('Brahm', 'NNP'), ('Prakash', 'NNP'), (',', ','), ('father', 'RB'), ('nuclear', 'JJ'), ('material', 'NN'), ('.', '.')]\n",
      "[('I', 'PRP'), ('lucky', 'VBP'), ('worked', 'VBD'), ('three', 'CD'), ('closely', 'RB'), ('consider', 'VBP'), ('great', 'JJ'), ('opportunity', 'NN'), ('life', 'NN'), ('.', '.')]\n",
      "[('I', 'PRP'), ('see', 'VBP'), ('four', 'CD'), ('milestones', 'NNS'), ('career', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# Mencari tau POS Tags pada paragraf\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    words=nltk.word_tokenize(sentences[i])\n",
    "    words=[word for word in words if word not in set(stopwords.words('english'))]\n",
    "    #sentences[i]=' '.join(words)# converting all the list of words into sentences\n",
    "    pos_tag=nltk.pos_tag(words)\n",
    "    print(pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a78bdb30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bali', 'is', 'a', 'beautiful', 'place']"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Bali is a beautiful place\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "31b26489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Bali', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('beautiful', 'JJ'), ('place', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "print(nltk.pos_tag(\"Bali is a beautiful place\".split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8da8e3d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d386d4",
   "metadata": {},
   "source": [
    "## Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3829a247",
   "metadata": {},
   "source": [
    "**Named Entity Recognition (NER)** adalah proses dalam Natural Language Processing (NLP) untuk mengidentifikasi dan mengklasifikasikan entitas penting dalam teks, seperti nama orang, organisasi, lokasi, tanggal, waktu, dan lain-lain. NER secara otomatis menandai kata atau frasa yang merupakan entitas tertentu sehingga memudahkan analisis dan ekstraksi informasi dari teks.\n",
    "\n",
    "#### Manfaat Named Entity Recognition\n",
    "\n",
    "- **Ekstraksi Informasi:** Memudahkan pengambilan data penting seperti nama, tempat, tanggal, dan organisasi dari dokumen atau artikel.\n",
    "\n",
    "- **Pencarian dan Kategorisasi:** Membantu dalam pencarian dokumen berdasarkan entitas tertentu dan mengelompokkan data sesuai kategori entitas.\n",
    "\n",
    "- **Analisis Data Besar:** Berguna untuk menganalisis data teks dalam jumlah besar secara otomatis, misalnya dalam berita, media sosial, atau laporan bisnis.\n",
    "\n",
    "- **Meningkatkan Akurasi Model NLP:** NER membantu model NLP memahami konteks dan makna teks dengan lebih baik.\n",
    "\n",
    "#### Kapan Digunakan Named Entity Recognition?\n",
    "\n",
    "- **Sistem Pencarian Informasi:** Untuk mengekstrak dan menampilkan entitas penting dari hasil pencarian.\n",
    "\n",
    "- **Chatbot dan Virtual Assistant:** Agar dapat mengenali nama, tempat, atau waktu dalam percakapan pengguna.\n",
    "\n",
    "- **Analisis Media Sosial:** Untuk memantau dan menganalisis topik atau entitas yang sedang tren.\n",
    "\n",
    "- **Dokumentasi dan Arsip:** Untuk mengorganisir dan mengindeks dokumen berdasarkan entitas yang terkandung di dalamnya.\n",
    "\n",
    "- **Business Intelligence:** Untuk mengekstrak insight dari laporan, email, atau dokumen bisnis.\n",
    "\n",
    "NER sangat penting dalam berbagai aplikasi NLP modern karena membantu mengubah data teks mentah menjadi informasi terstruktur yang dapat digunakan untuk analisis lebih lanjut."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4001ccb",
   "metadata": {},
   "source": [
    "```text\n",
    "sentence= \"The Eiffel Tower was built from 1887 to 1889 by Gustave Eiffel, whose company specialized in building metal frameworks and structures.\"\n",
    "\n",
    "Person Eg : Vanya Mayazura\n",
    "Place or Location Eg: Indonesia\n",
    "Date Eg: September, 24-09-1989\n",
    "Time Eg: 4:30pm\n",
    "Money Eg: 1 Million dollar\n",
    "Organization Eg: INeuron Private Limited\n",
    "Percent Eg: 20%, twenty percent\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "49b50d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence=\"The Eiffel Tower was built from 1887 to 1889 by Gustave Eiffel, whose company specialized in building metal frameworks and structures.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "6e78d0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "words=nltk.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "71766d97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('Eiffel', 'NNP'),\n",
       " ('Tower', 'NNP'),\n",
       " ('was', 'VBD'),\n",
       " ('built', 'VBN'),\n",
       " ('from', 'IN'),\n",
       " ('1887', 'CD'),\n",
       " ('to', 'TO'),\n",
       " ('1889', 'CD'),\n",
       " ('by', 'IN'),\n",
       " ('Gustave', 'NNP'),\n",
       " ('Eiffel', 'NNP'),\n",
       " (',', ','),\n",
       " ('whose', 'WP$'),\n",
       " ('company', 'NN'),\n",
       " ('specialized', 'VBD'),\n",
       " ('in', 'IN'),\n",
       " ('building', 'NN'),\n",
       " ('metal', 'NN'),\n",
       " ('frameworks', 'NNS'),\n",
       " ('and', 'CC'),\n",
       " ('structures', 'NNS'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "56959379",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_elements=nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "d31a773b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     C:\\Users\\Vanya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('maxent_ne_chunker_tab')\n",
    "# buat nentuin named entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0fdca037",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Vanya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "23d40ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kalau mau provide named entity recognition\n",
    "nltk.ne_chunk(tag_elements).draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b4df35",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e51a5e",
   "metadata": {},
   "source": [
    "## Latihan Stopwords, Part of Speech dan Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "e0787f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    \"Barack Obama was the 44th President of the United States.\",\n",
    "    \"I love learning Natural Language Processing with Python.\",\n",
    "    \"Jakarta is the capital city of Indonesia.\",\n",
    "    \"Apple is looking at buying U.K. startup for $1 billion.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42642fd1",
   "metadata": {},
   "source": [
    "### Bagian 1 – Stopwords\n",
    "\n",
    "1. Hapus stopwords dari setiap kalimat di dataset.\n",
    "\n",
    "2. Hitung berapa banyak stopwords yang ada di tiap kalimat.\n",
    "\n",
    "### Bagian 2 – Part of Speech (POS Tagging)\n",
    "\n",
    "3. Lakukan POS Tagging untuk setiap kata di dataset.\n",
    "\n",
    "4. Ambil semua kata yang merupakan kata benda (NN/NNP).\n",
    "\n",
    "### Bagian 3 – Named Entity Recognition (NER)\n",
    "\n",
    "5. Lakukan Named Entity Recognition menggunakan ne_chunk.\n",
    "\n",
    "6. Tampilkan entitas yang bertipe PERSON, GPE (Geopolitical Entity), dan ORGANIZATION."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5342d61",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ff4df4",
   "metadata": {},
   "source": [
    "### Jawab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc54dfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177d1f69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "adbe7143",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d555b030",
   "metadata": {},
   "source": [
    "### Pembahasan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c6b92317",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Vanya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Vanya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Vanya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Vanya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Vanya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "\n",
    "# Download resource\n",
    "# ini buat tokenizer\n",
    "nltk.download('punkt')\n",
    "# ini buat stopwords\n",
    "nltk.download('stopwords')\n",
    "# ini buat pos tag\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "# ini buat named entity recognition\n",
    "nltk.download('maxent_ne_chunker')\n",
    "# ini buat words atau kalimat dipecah menjadi suatu kata\n",
    "nltk.download('words')\n",
    "\n",
    "# Dataset\n",
    "dataset = [\n",
    "    \"Barack Obama was the 44th President of the United States.\",\n",
    "    \"I love learning Natural Language Processing with Python.\",\n",
    "    \"Jakarta is the capital city of Indonesia.\",\n",
    "    \"Apple is looking at buying U.K. startup for $1 billion.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "6e992014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STOPWORDS REMOVAL ===\n",
      "Kalimat asli   : Barack Obama was the 44th President of the United States.\n",
      "Setelah filter : ['Barack', 'Obama', '44th', 'President', 'United', 'States', '.']\n",
      "Jumlah stopwords dihapus: 4\n",
      "\n",
      "Kalimat asli   : I love learning Natural Language Processing with Python.\n",
      "Setelah filter : ['love', 'learning', 'Natural', 'Language', 'Processing', 'Python', '.']\n",
      "Jumlah stopwords dihapus: 2\n",
      "\n",
      "Kalimat asli   : Jakarta is the capital city of Indonesia.\n",
      "Setelah filter : ['Jakarta', 'capital', 'city', 'Indonesia', '.']\n",
      "Jumlah stopwords dihapus: 3\n",
      "\n",
      "Kalimat asli   : Apple is looking at buying U.K. startup for $1 billion.\n",
      "Setelah filter : ['Apple', 'looking', 'buying', 'U.K.', 'startup', '$', '1', 'billion', '.']\n",
      "Jumlah stopwords dihapus: 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "print(\"=== STOPWORDS REMOVAL ===\")\n",
    "# setiap kalimat yang ada di dataset\n",
    "for sentence in dataset:\n",
    "    # lakukan tokenisasi ke dalam setiap kalimat menjadi beberapa kata\n",
    "    words = word_tokenize(sentence)\n",
    "    # lakukan filter untuk menghilangkan stopwords dan tulisannya menjadi kecil\n",
    "    filtered = [w for w in words if w.lower() not in stop_words]\n",
    "    # print hasil yang udah sebelum dan sesudah di filter\n",
    "    print(f\"Kalimat asli   : {sentence}\")\n",
    "    print(f\"Setelah filter : {filtered}\")\n",
    "    # buat nampilin jumlah stopwords yang dihapus\n",
    "    print(f\"Jumlah stopwords dihapus: {len(words) - len(filtered)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "417bf1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== POS TAGGING ===\n",
      "Kalimat: Barack Obama was the 44th President of the United States.\n",
      "POS Tags: [('Barack', 'NNP'), ('Obama', 'NNP'), ('was', 'VBD'), ('the', 'DT'), ('44th', 'JJ'), ('President', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('United', 'NNP'), ('States', 'NNPS'), ('.', '.')]\n",
      "Kata benda: ['Barack', 'Obama', 'President', 'United', 'States']\n",
      "\n",
      "Kalimat: I love learning Natural Language Processing with Python.\n",
      "POS Tags: [('I', 'PRP'), ('love', 'VBP'), ('learning', 'VBG'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'VBG'), ('with', 'IN'), ('Python', 'NNP'), ('.', '.')]\n",
      "Kata benda: ['Natural', 'Language', 'Python']\n",
      "\n",
      "Kalimat: Jakarta is the capital city of Indonesia.\n",
      "POS Tags: [('Jakarta', 'NNP'), ('is', 'VBZ'), ('the', 'DT'), ('capital', 'NN'), ('city', 'NN'), ('of', 'IN'), ('Indonesia', 'NNP'), ('.', '.')]\n",
      "Kata benda: ['Jakarta', 'capital', 'city', 'Indonesia']\n",
      "\n",
      "Kalimat: Apple is looking at buying U.K. startup for $1 billion.\n",
      "POS Tags: [('Apple', 'NNP'), ('is', 'VBZ'), ('looking', 'VBG'), ('at', 'IN'), ('buying', 'VBG'), ('U.K.', 'NNP'), ('startup', 'NN'), ('for', 'IN'), ('$', '$'), ('1', 'CD'), ('billion', 'CD'), ('.', '.')]\n",
      "Kata benda: ['Apple', 'U.K.', 'startup']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# POS Tagging\n",
    "print(\"\\n=== POS TAGGING ===\")\n",
    "# dia ngitung setiap kalimat yang ada di dataset\n",
    "for sentence in dataset:\n",
    "    # dia nge tokenize setiap kalimat menjadi beberapa kata\n",
    "    words = word_tokenize(sentence)\n",
    "    # dia nge tag setiap kata yang udah di tokenize tadi\n",
    "    tagged = pos_tag(words)\n",
    "    # disuruh nampilin kata benda aja\n",
    "    nouns = [word for word, tag in tagged if tag in [\"NN\", \"NNP\", \"NNS\", \"NNPS\"]]\n",
    "    # dia nampilin hasilnya\n",
    "    print(f\"Kalimat: {sentence}\")\n",
    "    print(f\"POS Tags: {tagged}\")\n",
    "    print(f\"Kata benda: {nouns}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "c3bf2576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== NAMED ENTITY RECOGNITION ===\n",
      "Kalimat: Barack Obama was the 44th President of the United States.\n",
      "Entitas yang ditemukan:\n",
      " - Barack (PERSON)\n",
      " - Obama (PERSON)\n",
      " - United States (GPE)\n",
      "\n",
      "Kalimat: I love learning Natural Language Processing with Python.\n",
      "Entitas yang ditemukan:\n",
      " - Natural (ORGANIZATION)\n",
      " - Python (PERSON)\n",
      "\n",
      "Kalimat: Jakarta is the capital city of Indonesia.\n",
      "Entitas yang ditemukan:\n",
      " - Jakarta (GPE)\n",
      " - Indonesia (GPE)\n",
      "\n",
      "Kalimat: Apple is looking at buying U.K. startup for $1 billion.\n",
      "Entitas yang ditemukan:\n",
      " - Apple (GPE)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  Named Entity Recognition\n",
    "print(\"\\n=== NAMED ENTITY RECOGNITION ===\")\n",
    "# untuk setiap kalimat yang ada di dataset\n",
    "for sentence in dataset:\n",
    "    # lakukan tokenisasi ke dalam setiap kalimat menjadi beberapa kata\n",
    "    words = word_tokenize(sentence)\n",
    "    # lakukan pos tag ke dalam setiap kata\n",
    "    tagged = pos_tag(words)\n",
    "    # lakukan named entity recognition\n",
    "    chunks = ne_chunk(tagged)\n",
    "\n",
    "#   print hasilnya\n",
    "    print(f\"Kalimat: {sentence}\")\n",
    "    print(\"Entitas yang ditemukan:\")\n",
    "\n",
    "#   print setiap entitas yang ditemukan\n",
    "    for chunk in chunks:\n",
    "        if hasattr(chunk, 'label'):\n",
    "            if chunk.label() in [\"PERSON\", \"GPE\", \"ORGANIZATION\"]:\n",
    "                entity = \" \".join(c[0] for c in chunk.leaves())\n",
    "                print(f\" - {entity} ({chunk.label()})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "23e0e4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisasi Named Entity Recognition (NER) dengan NLTK\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "\n",
    "text = \"Barack Obama was born in Hawaii. He was elected president in 2008.\"\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "tagged = pos_tag(tokens)\n",
    "ner_tree = ne_chunk(tagged)\n",
    "\n",
    "# Menampilkan visual tree NER (jalankan di lingkungan yang mendukung GUI)\n",
    "ner_tree.draw()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

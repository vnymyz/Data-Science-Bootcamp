{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8d6e522",
   "metadata": {},
   "source": [
    "## Jawaban Codingan Latihan\n",
    "\n",
    "```python\n",
    "# Dataset baru\n",
    "dataset = [\n",
    "    \"Natural Language Processing is amazing!\",\n",
    "    \"Text preprocessing includes tokenization, stemming, and lemmatization.\",\n",
    "    \"Python and NLTK make NLP tasks easier.\",\n",
    "    \"Let's learn NLP together and build cool projects!\"\n",
    " ]\n",
    "\n",
    "# 1. Tokenisasi Kalimat\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, wordpunct_tokenize, TreebankWordTokenizer\n",
    "\n",
    "paragraf = \" \".join(dataset)\n",
    "kalimat = sent_tokenize(paragraf)\n",
    "print('Tokenisasi Kalimat:', kalimat)\n",
    "\n",
    "# 2. Tokenisasi Kata (ambil kalimat pertama)\n",
    "kata = word_tokenize(kalimat[0])\n",
    "print('\\nTokenisasi Kata:', kata)\n",
    "\n",
    "# 3. Perbandingan Tokenizer\n",
    "print('\\nword_tokenize:', word_tokenize(kalimat[0]))\n",
    "print('wordpunct_tokenize:', wordpunct_tokenize(kalimat[0]))\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "print('TreebankWordTokenizer:', tokenizer.tokenize(kalimat[0]))\n",
    "\n",
    "# 4. Stemming\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer, RegexpStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer('english')\n",
    "regexp = RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
    "\n",
    "print('\\nPorter Stemmer:', [porter.stem(w) for w in kata])\n",
    "print('Snowball Stemmer:', [snowball.stem(w) for w in kata])\n",
    "print('Regexp Stemmer:', [regexp.stem(w) for w in kata])\n",
    "\n",
    "# 5. Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print('\\nWordNet Lemmatizer (noun):', [lemmatizer.lemmatize(w, pos='n') for w in kata])\n",
    "print('WordNet Lemmatizer (verb):', [lemmatizer.lemmatize(w, pos='v') for w in kata])\n",
    "\n",
    "# 6. Analisis Waktu Eksekusi\n",
    "import time\n",
    "\n",
    "start_stem = time.time()\n",
    "[porter.stem(w) for w in kata]\n",
    "end_stem = time.time()\n",
    "\n",
    "start_lem = time.time()\n",
    "[lemmatizer.lemmatize(w, pos='v') for w in kata]\n",
    "end_lem = time.time()\n",
    "\n",
    "print(f'\\nWaktu Stemming: {end_stem - start_stem:.6f} detik')\n",
    "print(f'Waktu Lemmatization: {end_lem - start_lem:.6f} detik')\n",
    "\n",
    "# 7. Penjelasan Konsep\n",
    "print('Stemming memotong kata ke bentuk dasar dengan aturan sederhana, sedangkan lemmatization mencari bentuk dasar yang valid secara tata bahasa. Stemming lebih cepat, lemmatization lebih akurat. Pilih stemming untuk analisis cepat, lemmatization untuk aplikasi yang butuh akurasi kata dasar.')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16459675",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
